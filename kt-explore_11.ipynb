{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08be20f0-aa3d-4ce0-99c9-f8ea05c4aecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade tensorflow --quiet\n",
    "# !pip install keras_tuner --quiet\n",
    "# !pip install tensorflow-io --quiet\n",
    "# # Google colab modules\n",
    "# from google.colab import drive\n",
    "import sys, importlib\n",
    "\n",
    "# # Mount drive\n",
    "# drive.mount('/content/gdrive', force_remount=True)\n",
    "ROOT_PATH = './'\n",
    "# sys.path.append(ROOT_PATH)\n",
    "\n",
    "import coremlv2 as core\n",
    "core._init_ml()\n",
    "# core._init_models()\n",
    "# core.os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "# Reload coreml\n",
    "importlib.reload(core)\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f01d74c6-75b0-47d2-b9e6-4dce68486dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs,  1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "# Limiting GPU memory growth\n",
    "gpus = core.tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            core.tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = core.tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs, \", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed62e6c2-1987-410e-bea4-43ce639c3faf",
   "metadata": {},
   "source": [
    "### Run with revised generator\n",
    "- Use 326 as early benchmark model, with world portion variation to find the optimal point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da3a24d-1d79-4f11-95ef-4be2700975fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total constituents: 4880\n",
      "Total constituents: 422\n",
      "\n",
      "Search: Running Trial #1\n",
      "\n",
      "Hyperparameter    |Value             |Best Value So Far \n",
      "lr                |1.3843e-06        |?                 \n",
      "r_units           |64                |?                 \n",
      "d_units_1         |80                |?                 \n",
      "d_units_2         |16                |?                 \n",
      "tuner/epochs      |2                 |?                 \n",
      "tuner/initial_e...|0                 |?                 \n",
      "tuner/bracket     |3                 |?                 \n",
      "tuner/round       |0                 |?                 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RERUN WITH REVISED traintest slice code\n",
    "model_no = '326'\n",
    "constituent_limits = 0.075\n",
    "id_constituent = 1\n",
    "kt_iter = f'_model-{model_no}_wc-{constituent_limits}_ic-{id_constituent}'\n",
    "ticker_group = ['wsd']\n",
    "epochs = 40\n",
    "max_epochs = 40\n",
    "batch_size = 2048\n",
    "shuffle_buffer_size = 16\n",
    "generator = True\n",
    "\n",
    "# Train: `slice_from_beginning`=True\n",
    "train_gen = core.load_dataset_wsd(slice_from_beginning=True, ROOT_PATH='''J:\\#PROJECT\\idx''', db_ver='8', constituent_limits=constituent_limits, id_constituent=id_constituent, batch_size=batch_size, shuffle_buffer_size=shuffle_buffer_size, seed=0, generator=generator, model_no=model_no)\n",
    "# Validation: `slice_from_beginning`=False. constituent_limits in validation is always 0 (focus on idx performance progression only)\n",
    "validation_gen = core.load_dataset_wsd(slice_from_beginning=False, ROOT_PATH='''J:\\#PROJECT\\idx''', db_ver='8', constituent_limits=0, id_constituent=id_constituent, batch_size=batch_size, shuffle_buffer_size=shuffle_buffer_size, seed=0, generator=generator, model_no=model_no)\n",
    "\n",
    "tuner = kt.Hyperband(hypermodel=core.model_326_kt, objective='val_loss', max_epochs=max_epochs, hyperband_iterations=1, overwrite=True, directory=f'{ROOT_PATH}models/kt/v{kt_iter}/', project_name='_'.join(ticker_group))\n",
    "\n",
    "tuner.search(train_gen, validation_data=validation_gen, epochs=epochs, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b29feb6-0a41-459f-a445-ce8b30f77d58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebffc9f-c4c9-4054-a7a0-58c69fcb3fb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb049ba-3e8d-4b1b-a023-67968b278765",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
