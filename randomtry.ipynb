{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bf163d2-d18c-4946-9fbc-e4903249aea9",
   "metadata": {},
   "source": [
    "### Read best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16dca35f-5b5a-45d4-97e8-0b92bf20719b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'coremlv2' from 'C:\\\\Users\\\\jonat\\\\Documents\\\\#PROJECT\\\\idx\\\\coremlv2.py'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import coremlv2 as core\n",
    "import importlib\n",
    "# core._init_ml()\n",
    "importlib.reload(core)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6ee2af6-cfa5-4ff0-be9e-ca847fc235b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['superblock_final_dunits', 'lr', 'comp2', 'comp3', 'comp5', 'comp7', 'comp11', 'comp13', 'comp17', 'comp19', 'comp23', 'comp29', 'comp31', 'comp37', 'comp41', 'comp43', 'comp47', 'comp53', 'comp59', 'comp61', 'comp67', 'comp71', 'comp73', 'comp79', 'comp83', 'comp89', 'comp97', 'comp101', 'comp103', 'comp107', 'comp109', 'comp113', 'tuner/epochs', 'tuner/initial_epoch', 'tuner/bracket', 'tuner/round']\n"
     ]
    }
   ],
   "source": [
    "ticker_group = ['wsd']\n",
    "kt_iter = '24'\n",
    "print(core.read_best_model_hyperparameters_name(ticker_group, kt_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3f98af-fd1e-4cea-bb92-fad34873991b",
   "metadata": {},
   "source": [
    "### read_retrain_model\n",
    "- derived from read_best_model, but from whole data training (different folder structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91e582e6-b32d-44e6-9db1-689e0cc21c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import coremlv2 as core\n",
    "import importlib\n",
    "core._init_ml()\n",
    "importlib.reload(core)\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd61fa23-7520-4e62-839f-b11b5f76ca9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weights from epoch 2\n"
     ]
    }
   ],
   "source": [
    "ROOT_PATH = './'\n",
    "# kt_iter = '9'\n",
    "# retrain_epochs = 1\n",
    "# source_group_name = 'MERGED-shuffled'\n",
    "# source_model_path = core.os.path.join(ROOT_PATH, f'models/kt/v{kt_iter}/{source_group_name}_retrain-epochs{retrain_epochs}/model')\n",
    "\n",
    "# source_model = core.tf.keras.models.load_model(source_model_path)\n",
    "\n",
    "model_base_id = '307'\n",
    "model_source = ['BBCA']\n",
    "model_ticker_target = ['BBCA']\n",
    "shift = 0\n",
    "interval = 1\n",
    "recurrent = 120\n",
    "db_ver = '3'\n",
    "dataset_ver = '4'\n",
    "kt_iter = '9'\n",
    "split = 0.8\n",
    "\n",
    "model, data_version = core.read_best_model(model_base_id, model_source, model_ticker_target, shift, interval, recurrent, db_ver, dataset_ver, kt_iter, split, ROOT_PATH, retrain=False, backtest=False, save_model=False, generator=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68a0552-8228-45b1-9010-de579b3c5c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = './'\n",
    "# kt_iter = '9'\n",
    "# retrain_epochs = 1\n",
    "# source_group_name = 'MERGED-shuffled'\n",
    "# source_model_path = core.os.path.join(ROOT_PATH, f'models/kt/v{kt_iter}/{source_group_name}_retrain-epochs{retrain_epochs}/model')\n",
    "\n",
    "# source_model = core.tf.keras.models.load_model(source_model_path)\n",
    "\n",
    "model_base_id = '310'\n",
    "model_source = ['MERGED-shuffled']\n",
    "model_ticker_target = ['MERGED-shuffled']\n",
    "shift = 0\n",
    "interval = 1\n",
    "recurrent = 120\n",
    "db_ver = '4'\n",
    "dataset_ver = '1'\n",
    "kt_iter = '12'\n",
    "split = 0.8\n",
    "\n",
    "model, data_version = core.read_best_model(model_base_id, model_source, model_ticker_target, shift, interval, recurrent, db_ver, dataset_ver, kt_iter, split, ROOT_PATH, retrain=False, backtest=False, save_model=False, generator=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd8669ad-f1d4-4a8f-8764-5c918600e911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load model from keras_tuner\n"
     ]
    }
   ],
   "source": [
    "# Load models without retraining v2\n",
    "# def load_model()\n",
    "ROOT_PATH = './'\n",
    "kt_iter = '33.old (80 20 traintest slice)'\n",
    "model_no = '326'\n",
    "base_path = core.os.path.join(ROOT_PATH, 'models')\n",
    "base_path = core.os.path.join(base_path, 'kt')\n",
    "base_path = core.os.path.join(base_path, f'v{kt_iter}')\n",
    "\n",
    "last_epochs = core.last_epochs_from_directory(base_path, 'wsd')\n",
    "\n",
    "# Load model backbone\n",
    "model_source_path = core.os.path.join(base_path, 'wsd')\n",
    "best_model = core.best_model_metadata(model_source_path, metrics=['val_loss','val_accuracy'], primary_metrics='val_loss', ascending=True)\n",
    "modelv2 = core.model_switcher_kt_build(model_no, core.hp_params(best_model), 0, 0)\n",
    "\n",
    "if last_epochs == 0:\n",
    "    print('Load model from keras_tuner')\n",
    "    # Load weights from keras_tuner directory\n",
    "    weights_path = core.os.path.join(model_source_path, f'{best_model.id}')\n",
    "    weights_path = core.os.path.join(weights_path, 'checkpoints')\n",
    "    weights_path = core.os.path.join(weights_path, 'epoch_0')\n",
    "    weights_path = core.os.path.join(weights_path, 'checkpoint')\n",
    "    modelv2.load_weights(weights_path)\n",
    "    # If have been trained before, continue training for last epoch\n",
    "elif last_epochs > 0:\n",
    "    print(f'Load weights from epoch {last_epochs}')\n",
    "    # Load weights from previously trained model\n",
    "    weights_path = core.os.path.join(base_path, f'wsd_retrain-epochs{last_epochs}')\n",
    "    weights_path = core.os.path.join(weights_path, 'weights')\n",
    "    weights_path = core.os.path.join(weights_path, 'checkpoint')\n",
    "    modelv2.load_weights(weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9b4c7de-4c5a-4b44-aed0-48cbdbe085f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model (not weight) v2\n",
    "ROOT_PATH = './'\n",
    "kt_iter = '33.old (80 20 traintest slice)'\n",
    "base_path = core.os.path.join(ROOT_PATH, 'models')\n",
    "base_path = core.os.path.join(base_path, 'kt')\n",
    "base_path = core.os.path.join(base_path, f'v{kt_iter}')\n",
    "\n",
    "last_epochs = core.last_epochs_from_directory(base_path, 'wsd')\n",
    "\n",
    "# Load model backbone\n",
    "if last_epochs > 0:\n",
    "    model_source_path = core.os.path.join(base_path, f'wsd_retrain-epochs{last_epochs}')\n",
    "    model_source_path = core.os.path.join(model_source_path, 'model')\n",
    "    modelv3 = core.tf.keras.models.load_model(model_source_path)\n",
    "elif last_epochs == 0:\n",
    "    print('Retrain model first before use.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d029418b-7be4-4506-ba42-6162612b6f2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_group = ['BBCA']\n",
    "target_group_name = '_'.join(target_group)\n",
    "target_model_path = core.os.path.join(ROOT_PATH, f'models/kt/v{kt_iter}/{source_group_name}_epochs-multiplier{source_epochs_multiplier}/retrained/{target_group_name}')\n",
    "\n",
    "# Check if current target retrained model configurations have been tried before\n",
    "if core.os.path.exists(core.os.path.join(target_model_path, 'models/')):\n",
    "    # Directly read retrained model\n",
    "    model = core.tf.keras.models.load_model(core.os.path.join(target_model_path, 'models/'))\n",
    "else:\n",
    "    # Read whole data trained model\n",
    "\n",
    "    # Load dataset for retraining\n",
    "    shift = 0\n",
    "    interval = 1\n",
    "    recurrent = 120\n",
    "    db_ver = '3'\n",
    "    dataset_ver = '4'\n",
    "    split = 0.8\n",
    "    generator = False\n",
    "    batch_size = 32\n",
    "    train_inputs, train_labels, train_changes, test_inputs, test_labels, test_changes, data_version = core.load_dataset(target_group_name, shift, interval, recurrent, db_ver, dataset_ver, split, ROOT_PATH, generator=generator, batch_size=batch_size)\n",
    "\n",
    "    # Eliminate nan\n",
    "    train_inputs = np.nan_to_num(train_inputs, posinf=0.0, neginf=0.0)\n",
    "    test_inputs = np.nan_to_num(test_inputs, posinf=0.0, neginf=0.0)\n",
    "\n",
    "    # Model retraining configurations\n",
    "    # Freeze all layers except two final layers.\n",
    "    epochs = 1\n",
    "    # model.fit\n",
    "\n",
    "    # Save model\n",
    "    model.save(core.os.path.join(target_model_path, 'models/'))\n",
    "    model.save_weights(core.os.path.join(target_model_path, 'weights/'))\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e8930228-19f5-4e30-bec2-6b193e6777c0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_2\n",
      "conv1d_531\n",
      "conv1d_522\n",
      "conv1d_513\n",
      "conv1d_504\n",
      "conv1d_495\n",
      "conv1d_486\n",
      "conv1d_477\n",
      "conv1d_468\n",
      "conv1d_459\n",
      "conv1d_450\n",
      "conv1d_441\n",
      "conv1d_432\n",
      "conv1d_423\n",
      "conv1d_414\n",
      "conv1d_405\n",
      "conv1d_396\n",
      "conv1d_387\n",
      "conv1d_378\n",
      "conv1d_369\n",
      "conv1d_360\n",
      "conv1d_351\n",
      "conv1d_342\n",
      "conv1d_333\n",
      "conv1d_324\n",
      "conv1d_315\n",
      "conv1d_306\n",
      "conv1d_297\n",
      "conv1d_288\n",
      "conv1d_279\n",
      "conv1d_270\n",
      "batch_normalization_767\n",
      "batch_normalization_754\n",
      "batch_normalization_741\n",
      "batch_normalization_728\n",
      "batch_normalization_715\n",
      "batch_normalization_702\n",
      "batch_normalization_689\n",
      "batch_normalization_676\n",
      "batch_normalization_663\n",
      "batch_normalization_650\n",
      "batch_normalization_637\n",
      "batch_normalization_624\n",
      "batch_normalization_611\n",
      "batch_normalization_598\n",
      "batch_normalization_585\n",
      "batch_normalization_572\n",
      "batch_normalization_559\n",
      "batch_normalization_546\n",
      "batch_normalization_533\n",
      "batch_normalization_520\n",
      "batch_normalization_507\n",
      "batch_normalization_494\n",
      "batch_normalization_481\n",
      "batch_normalization_468\n",
      "batch_normalization_455\n",
      "batch_normalization_442\n",
      "batch_normalization_429\n",
      "batch_normalization_416\n",
      "batch_normalization_403\n",
      "batch_normalization_390\n",
      "activation_826\n",
      "activation_812\n",
      "activation_798\n",
      "activation_784\n",
      "activation_770\n",
      "activation_756\n",
      "activation_742\n",
      "activation_728\n",
      "activation_714\n",
      "activation_700\n",
      "activation_686\n",
      "activation_672\n",
      "activation_658\n",
      "activation_644\n",
      "activation_630\n",
      "activation_616\n",
      "activation_602\n",
      "activation_588\n",
      "activation_574\n",
      "activation_560\n",
      "activation_546\n",
      "activation_532\n",
      "activation_518\n",
      "activation_504\n",
      "activation_490\n",
      "activation_476\n",
      "activation_462\n",
      "activation_448\n",
      "activation_434\n",
      "activation_420\n",
      "conv1d_532\n",
      "conv1d_523\n",
      "conv1d_514\n",
      "conv1d_505\n",
      "conv1d_496\n",
      "conv1d_487\n",
      "conv1d_478\n",
      "conv1d_469\n",
      "conv1d_460\n",
      "conv1d_451\n",
      "conv1d_442\n",
      "conv1d_433\n",
      "conv1d_424\n",
      "conv1d_415\n",
      "conv1d_406\n",
      "conv1d_397\n",
      "conv1d_388\n",
      "conv1d_379\n",
      "conv1d_370\n",
      "conv1d_361\n",
      "conv1d_352\n",
      "conv1d_343\n",
      "conv1d_334\n",
      "conv1d_325\n",
      "conv1d_316\n",
      "conv1d_307\n",
      "conv1d_298\n",
      "conv1d_289\n",
      "conv1d_280\n",
      "conv1d_271\n",
      "batch_normalization_768\n",
      "batch_normalization_755\n",
      "batch_normalization_742\n",
      "batch_normalization_729\n",
      "batch_normalization_716\n",
      "batch_normalization_703\n",
      "batch_normalization_690\n",
      "batch_normalization_677\n",
      "batch_normalization_664\n",
      "batch_normalization_651\n",
      "batch_normalization_638\n",
      "batch_normalization_625\n",
      "batch_normalization_612\n",
      "batch_normalization_599\n",
      "batch_normalization_586\n",
      "batch_normalization_573\n",
      "batch_normalization_560\n",
      "batch_normalization_547\n",
      "batch_normalization_534\n",
      "batch_normalization_521\n",
      "batch_normalization_508\n",
      "batch_normalization_495\n",
      "batch_normalization_482\n",
      "batch_normalization_469\n",
      "batch_normalization_456\n",
      "batch_normalization_443\n",
      "batch_normalization_430\n",
      "batch_normalization_417\n",
      "batch_normalization_404\n",
      "batch_normalization_391\n",
      "activation_827\n",
      "activation_813\n",
      "activation_799\n",
      "activation_785\n",
      "activation_771\n",
      "activation_757\n",
      "activation_743\n",
      "activation_729\n",
      "activation_715\n",
      "activation_701\n",
      "activation_687\n",
      "activation_673\n",
      "activation_659\n",
      "activation_645\n",
      "activation_631\n",
      "activation_617\n",
      "activation_603\n",
      "activation_589\n",
      "activation_575\n",
      "activation_561\n",
      "activation_547\n",
      "activation_533\n",
      "activation_519\n",
      "activation_505\n",
      "activation_491\n",
      "activation_477\n",
      "activation_463\n",
      "activation_449\n",
      "activation_435\n",
      "activation_421\n",
      "conv1d_533\n",
      "conv1d_524\n",
      "conv1d_515\n",
      "conv1d_506\n",
      "conv1d_497\n",
      "conv1d_488\n",
      "conv1d_479\n",
      "conv1d_470\n",
      "conv1d_461\n",
      "conv1d_452\n",
      "conv1d_443\n",
      "conv1d_434\n",
      "conv1d_425\n",
      "conv1d_416\n",
      "conv1d_407\n",
      "conv1d_398\n",
      "conv1d_389\n",
      "conv1d_380\n",
      "conv1d_371\n",
      "conv1d_362\n",
      "conv1d_353\n",
      "conv1d_344\n",
      "conv1d_335\n",
      "conv1d_326\n",
      "conv1d_317\n",
      "conv1d_308\n",
      "conv1d_299\n",
      "conv1d_290\n",
      "conv1d_281\n",
      "conv1d_272\n",
      "batch_normalization_769\n",
      "batch_normalization_756\n",
      "batch_normalization_743\n",
      "batch_normalization_730\n",
      "batch_normalization_717\n",
      "batch_normalization_704\n",
      "batch_normalization_691\n",
      "batch_normalization_678\n",
      "batch_normalization_665\n",
      "batch_normalization_652\n",
      "batch_normalization_639\n",
      "batch_normalization_626\n",
      "batch_normalization_613\n",
      "batch_normalization_600\n",
      "batch_normalization_587\n",
      "batch_normalization_574\n",
      "batch_normalization_561\n",
      "batch_normalization_548\n",
      "batch_normalization_535\n",
      "batch_normalization_522\n",
      "batch_normalization_509\n",
      "batch_normalization_496\n",
      "batch_normalization_483\n",
      "batch_normalization_470\n",
      "batch_normalization_457\n",
      "batch_normalization_444\n",
      "batch_normalization_431\n",
      "batch_normalization_418\n",
      "batch_normalization_405\n",
      "batch_normalization_392\n",
      "activation_828\n",
      "activation_814\n",
      "activation_800\n",
      "activation_786\n",
      "activation_772\n",
      "activation_758\n",
      "activation_744\n",
      "activation_730\n",
      "activation_716\n",
      "activation_702\n",
      "activation_688\n",
      "activation_674\n",
      "activation_660\n",
      "activation_646\n",
      "activation_632\n",
      "activation_618\n",
      "activation_604\n",
      "activation_590\n",
      "activation_576\n",
      "activation_562\n",
      "activation_548\n",
      "activation_534\n",
      "activation_520\n",
      "activation_506\n",
      "activation_492\n",
      "activation_478\n",
      "activation_464\n",
      "activation_450\n",
      "activation_436\n",
      "activation_422\n",
      "gru_237\n",
      "gru_233\n",
      "gru_229\n",
      "gru_225\n",
      "gru_221\n",
      "gru_217\n",
      "gru_213\n",
      "gru_209\n",
      "gru_205\n",
      "gru_201\n",
      "gru_197\n",
      "gru_193\n",
      "gru_189\n",
      "gru_185\n",
      "gru_181\n",
      "gru_177\n",
      "gru_173\n",
      "gru_169\n",
      "gru_165\n",
      "gru_161\n",
      "gru_157\n",
      "gru_153\n",
      "gru_149\n",
      "gru_145\n",
      "gru_141\n",
      "gru_137\n",
      "gru_133\n",
      "gru_129\n",
      "gru_125\n",
      "gru_121\n",
      "batch_normalization_770\n",
      "batch_normalization_757\n",
      "batch_normalization_744\n",
      "batch_normalization_731\n",
      "batch_normalization_718\n",
      "batch_normalization_705\n",
      "batch_normalization_692\n",
      "batch_normalization_679\n",
      "batch_normalization_666\n",
      "batch_normalization_653\n",
      "batch_normalization_640\n",
      "batch_normalization_627\n",
      "batch_normalization_614\n",
      "batch_normalization_601\n",
      "batch_normalization_588\n",
      "batch_normalization_575\n",
      "batch_normalization_562\n",
      "batch_normalization_549\n",
      "batch_normalization_536\n",
      "batch_normalization_523\n",
      "batch_normalization_510\n",
      "batch_normalization_497\n",
      "batch_normalization_484\n",
      "batch_normalization_471\n",
      "batch_normalization_458\n",
      "batch_normalization_445\n",
      "batch_normalization_432\n",
      "batch_normalization_419\n",
      "batch_normalization_406\n",
      "batch_normalization_393\n",
      "activation_829\n",
      "activation_815\n",
      "activation_801\n",
      "activation_787\n",
      "activation_773\n",
      "activation_759\n",
      "activation_745\n",
      "activation_731\n",
      "activation_717\n",
      "activation_703\n",
      "activation_689\n",
      "activation_675\n",
      "activation_661\n",
      "activation_647\n",
      "activation_633\n",
      "activation_619\n",
      "activation_605\n",
      "activation_591\n",
      "activation_577\n",
      "activation_563\n",
      "activation_549\n",
      "activation_535\n",
      "activation_521\n",
      "activation_507\n",
      "activation_493\n",
      "activation_479\n",
      "activation_465\n",
      "activation_451\n",
      "activation_437\n",
      "activation_423\n",
      "conv1d_534\n",
      "conv1d_525\n",
      "conv1d_516\n",
      "conv1d_507\n",
      "conv1d_498\n",
      "conv1d_489\n",
      "conv1d_480\n",
      "conv1d_471\n",
      "conv1d_462\n",
      "conv1d_453\n",
      "conv1d_444\n",
      "conv1d_435\n",
      "conv1d_426\n",
      "conv1d_417\n",
      "conv1d_408\n",
      "conv1d_399\n",
      "conv1d_390\n",
      "conv1d_381\n",
      "conv1d_372\n",
      "conv1d_363\n",
      "conv1d_354\n",
      "conv1d_345\n",
      "conv1d_336\n",
      "conv1d_327\n",
      "conv1d_318\n",
      "conv1d_309\n",
      "conv1d_300\n",
      "conv1d_291\n",
      "conv1d_282\n",
      "conv1d_273\n",
      "batch_normalization_771\n",
      "batch_normalization_758\n",
      "batch_normalization_745\n",
      "batch_normalization_732\n",
      "batch_normalization_719\n",
      "batch_normalization_706\n",
      "batch_normalization_693\n",
      "batch_normalization_680\n",
      "batch_normalization_667\n",
      "batch_normalization_654\n",
      "batch_normalization_641\n",
      "batch_normalization_628\n",
      "batch_normalization_615\n",
      "batch_normalization_602\n",
      "batch_normalization_589\n",
      "batch_normalization_576\n",
      "batch_normalization_563\n",
      "batch_normalization_550\n",
      "batch_normalization_537\n",
      "batch_normalization_524\n",
      "batch_normalization_511\n",
      "batch_normalization_498\n",
      "batch_normalization_485\n",
      "batch_normalization_472\n",
      "batch_normalization_459\n",
      "batch_normalization_446\n",
      "batch_normalization_433\n",
      "batch_normalization_420\n",
      "batch_normalization_407\n",
      "batch_normalization_394\n",
      "activation_830\n",
      "activation_816\n",
      "activation_802\n",
      "activation_788\n",
      "activation_774\n",
      "activation_760\n",
      "activation_746\n",
      "activation_732\n",
      "activation_718\n",
      "activation_704\n",
      "activation_690\n",
      "activation_676\n",
      "activation_662\n",
      "activation_648\n",
      "activation_634\n",
      "activation_620\n",
      "activation_606\n",
      "activation_592\n",
      "activation_578\n",
      "activation_564\n",
      "activation_550\n",
      "activation_536\n",
      "activation_522\n",
      "activation_508\n",
      "activation_494\n",
      "activation_480\n",
      "activation_466\n",
      "activation_452\n",
      "activation_438\n",
      "activation_424\n",
      "conv1d_535\n",
      "conv1d_526\n",
      "conv1d_517\n",
      "conv1d_508\n",
      "conv1d_499\n",
      "conv1d_490\n",
      "conv1d_481\n",
      "conv1d_472\n",
      "conv1d_463\n",
      "conv1d_454\n",
      "conv1d_445\n",
      "conv1d_436\n",
      "conv1d_427\n",
      "conv1d_418\n",
      "conv1d_409\n",
      "conv1d_400\n",
      "conv1d_391\n",
      "conv1d_382\n",
      "conv1d_373\n",
      "conv1d_364\n",
      "conv1d_355\n",
      "conv1d_346\n",
      "conv1d_337\n",
      "conv1d_328\n",
      "conv1d_319\n",
      "conv1d_310\n",
      "conv1d_301\n",
      "conv1d_292\n",
      "conv1d_283\n",
      "conv1d_274\n",
      "batch_normalization_772\n",
      "batch_normalization_759\n",
      "batch_normalization_746\n",
      "batch_normalization_733\n",
      "batch_normalization_720\n",
      "batch_normalization_707\n",
      "batch_normalization_694\n",
      "batch_normalization_681\n",
      "batch_normalization_668\n",
      "batch_normalization_655\n",
      "batch_normalization_642\n",
      "batch_normalization_629\n",
      "batch_normalization_616\n",
      "batch_normalization_603\n",
      "batch_normalization_590\n",
      "batch_normalization_577\n",
      "batch_normalization_564\n",
      "batch_normalization_551\n",
      "batch_normalization_538\n",
      "batch_normalization_525\n",
      "batch_normalization_512\n",
      "batch_normalization_499\n",
      "batch_normalization_486\n",
      "batch_normalization_473\n",
      "batch_normalization_460\n",
      "batch_normalization_447\n",
      "batch_normalization_434\n",
      "batch_normalization_421\n",
      "batch_normalization_408\n",
      "batch_normalization_395\n",
      "activation_831\n",
      "activation_817\n",
      "activation_803\n",
      "activation_789\n",
      "activation_775\n",
      "activation_761\n",
      "activation_747\n",
      "activation_733\n",
      "activation_719\n",
      "activation_705\n",
      "activation_691\n",
      "activation_677\n",
      "activation_663\n",
      "activation_649\n",
      "activation_635\n",
      "activation_621\n",
      "activation_607\n",
      "activation_593\n",
      "activation_579\n",
      "activation_565\n",
      "activation_551\n",
      "activation_537\n",
      "activation_523\n",
      "activation_509\n",
      "activation_495\n",
      "activation_481\n",
      "activation_467\n",
      "activation_453\n",
      "activation_439\n",
      "activation_425\n",
      "conv1d_536\n",
      "conv1d_527\n",
      "conv1d_518\n",
      "conv1d_509\n",
      "conv1d_500\n",
      "conv1d_491\n",
      "conv1d_482\n",
      "conv1d_473\n",
      "conv1d_464\n",
      "conv1d_455\n",
      "conv1d_446\n",
      "conv1d_437\n",
      "conv1d_428\n",
      "conv1d_419\n",
      "conv1d_410\n",
      "conv1d_401\n",
      "conv1d_392\n",
      "conv1d_383\n",
      "conv1d_374\n",
      "conv1d_365\n",
      "conv1d_356\n",
      "conv1d_347\n",
      "conv1d_338\n",
      "conv1d_329\n",
      "conv1d_320\n",
      "conv1d_311\n",
      "conv1d_302\n",
      "conv1d_293\n",
      "conv1d_284\n",
      "conv1d_275\n",
      "batch_normalization_773\n",
      "batch_normalization_760\n",
      "batch_normalization_747\n",
      "batch_normalization_734\n",
      "batch_normalization_721\n",
      "batch_normalization_708\n",
      "batch_normalization_695\n",
      "batch_normalization_682\n",
      "batch_normalization_669\n",
      "batch_normalization_656\n",
      "batch_normalization_643\n",
      "batch_normalization_630\n",
      "batch_normalization_617\n",
      "batch_normalization_604\n",
      "batch_normalization_591\n",
      "batch_normalization_578\n",
      "batch_normalization_565\n",
      "batch_normalization_552\n",
      "batch_normalization_539\n",
      "batch_normalization_526\n",
      "batch_normalization_513\n",
      "batch_normalization_500\n",
      "batch_normalization_487\n",
      "batch_normalization_474\n",
      "batch_normalization_461\n",
      "batch_normalization_448\n",
      "batch_normalization_435\n",
      "batch_normalization_422\n",
      "batch_normalization_409\n",
      "batch_normalization_396\n",
      "activation_832\n",
      "activation_818\n",
      "activation_804\n",
      "activation_790\n",
      "activation_776\n",
      "activation_762\n",
      "activation_748\n",
      "activation_734\n",
      "activation_720\n",
      "activation_706\n",
      "activation_692\n",
      "activation_678\n",
      "activation_664\n",
      "activation_650\n",
      "activation_636\n",
      "activation_622\n",
      "activation_608\n",
      "activation_594\n",
      "activation_580\n",
      "activation_566\n",
      "activation_552\n",
      "activation_538\n",
      "activation_524\n",
      "activation_510\n",
      "activation_496\n",
      "activation_482\n",
      "activation_468\n",
      "activation_454\n",
      "activation_440\n",
      "activation_426\n",
      "gru_238\n",
      "gru_234\n",
      "gru_230\n",
      "gru_226\n",
      "gru_222\n",
      "gru_218\n",
      "gru_214\n",
      "gru_210\n",
      "gru_206\n",
      "gru_202\n",
      "gru_198\n",
      "gru_194\n",
      "gru_190\n",
      "gru_186\n",
      "gru_182\n",
      "gru_178\n",
      "gru_174\n",
      "gru_170\n",
      "gru_166\n",
      "gru_162\n",
      "gru_158\n",
      "gru_154\n",
      "gru_150\n",
      "gru_146\n",
      "gru_142\n",
      "gru_138\n",
      "gru_134\n",
      "gru_130\n",
      "gru_126\n",
      "gru_122\n",
      "batch_normalization_774\n",
      "batch_normalization_761\n",
      "batch_normalization_748\n",
      "batch_normalization_735\n",
      "batch_normalization_722\n",
      "batch_normalization_709\n",
      "batch_normalization_696\n",
      "batch_normalization_683\n",
      "batch_normalization_670\n",
      "batch_normalization_657\n",
      "batch_normalization_644\n",
      "batch_normalization_631\n",
      "batch_normalization_618\n",
      "batch_normalization_605\n",
      "batch_normalization_592\n",
      "batch_normalization_579\n",
      "batch_normalization_566\n",
      "batch_normalization_553\n",
      "batch_normalization_540\n",
      "batch_normalization_527\n",
      "batch_normalization_514\n",
      "batch_normalization_501\n",
      "batch_normalization_488\n",
      "batch_normalization_475\n",
      "batch_normalization_462\n",
      "batch_normalization_449\n",
      "batch_normalization_436\n",
      "batch_normalization_423\n",
      "batch_normalization_410\n",
      "batch_normalization_397\n",
      "add_59\n",
      "add_58\n",
      "add_57\n",
      "add_56\n",
      "add_55\n",
      "add_54\n",
      "add_53\n",
      "add_52\n",
      "add_51\n",
      "add_50\n",
      "add_49\n",
      "add_48\n",
      "add_47\n",
      "add_46\n",
      "add_45\n",
      "add_44\n",
      "add_43\n",
      "add_42\n",
      "add_41\n",
      "add_40\n",
      "add_39\n",
      "add_38\n",
      "add_37\n",
      "add_36\n",
      "add_35\n",
      "add_34\n",
      "add_33\n",
      "add_32\n",
      "add_31\n",
      "add_30\n",
      "activation_834\n",
      "activation_820\n",
      "activation_806\n",
      "activation_792\n",
      "activation_778\n",
      "activation_764\n",
      "activation_750\n",
      "activation_736\n",
      "activation_722\n",
      "activation_708\n",
      "activation_694\n",
      "activation_680\n",
      "activation_666\n",
      "activation_652\n",
      "activation_638\n",
      "activation_624\n",
      "activation_610\n",
      "activation_596\n",
      "activation_582\n",
      "activation_568\n",
      "activation_554\n",
      "activation_540\n",
      "activation_526\n",
      "activation_512\n",
      "activation_498\n",
      "activation_484\n",
      "activation_470\n",
      "activation_456\n",
      "activation_442\n",
      "activation_428\n",
      "conv1d_537\n",
      "conv1d_528\n",
      "conv1d_519\n",
      "conv1d_510\n",
      "conv1d_501\n",
      "conv1d_492\n",
      "conv1d_483\n",
      "conv1d_474\n",
      "conv1d_465\n",
      "conv1d_456\n",
      "conv1d_447\n",
      "conv1d_438\n",
      "conv1d_429\n",
      "conv1d_420\n",
      "conv1d_411\n",
      "conv1d_402\n",
      "conv1d_393\n",
      "conv1d_384\n",
      "conv1d_375\n",
      "conv1d_366\n",
      "conv1d_357\n",
      "conv1d_348\n",
      "conv1d_339\n",
      "conv1d_330\n",
      "conv1d_321\n",
      "conv1d_312\n",
      "conv1d_303\n",
      "conv1d_294\n",
      "conv1d_285\n",
      "conv1d_276\n",
      "batch_normalization_775\n",
      "batch_normalization_762\n",
      "batch_normalization_749\n",
      "batch_normalization_736\n",
      "batch_normalization_723\n",
      "batch_normalization_710\n",
      "batch_normalization_697\n",
      "batch_normalization_684\n",
      "batch_normalization_671\n",
      "batch_normalization_658\n",
      "batch_normalization_645\n",
      "batch_normalization_632\n",
      "batch_normalization_619\n",
      "batch_normalization_606\n",
      "batch_normalization_593\n",
      "batch_normalization_580\n",
      "batch_normalization_567\n",
      "batch_normalization_554\n",
      "batch_normalization_541\n",
      "batch_normalization_528\n",
      "batch_normalization_515\n",
      "batch_normalization_502\n",
      "batch_normalization_489\n",
      "batch_normalization_476\n",
      "batch_normalization_463\n",
      "batch_normalization_450\n",
      "batch_normalization_437\n",
      "batch_normalization_424\n",
      "batch_normalization_411\n",
      "batch_normalization_398\n",
      "activation_835\n",
      "activation_821\n",
      "activation_807\n",
      "activation_793\n",
      "activation_779\n",
      "activation_765\n",
      "activation_751\n",
      "activation_737\n",
      "activation_723\n",
      "activation_709\n",
      "activation_695\n",
      "activation_681\n",
      "activation_667\n",
      "activation_653\n",
      "activation_639\n",
      "activation_625\n",
      "activation_611\n",
      "activation_597\n",
      "activation_583\n",
      "activation_569\n",
      "activation_555\n",
      "activation_541\n",
      "activation_527\n",
      "activation_513\n",
      "activation_499\n",
      "activation_485\n",
      "activation_471\n",
      "activation_457\n",
      "activation_443\n",
      "activation_429\n",
      "conv1d_538\n",
      "conv1d_529\n",
      "conv1d_520\n",
      "conv1d_511\n",
      "conv1d_502\n",
      "conv1d_493\n",
      "conv1d_484\n",
      "conv1d_475\n",
      "conv1d_466\n",
      "conv1d_457\n",
      "conv1d_448\n",
      "conv1d_439\n",
      "conv1d_430\n",
      "conv1d_421\n",
      "conv1d_412\n",
      "conv1d_403\n",
      "conv1d_394\n",
      "conv1d_385\n",
      "conv1d_376\n",
      "conv1d_367\n",
      "conv1d_358\n",
      "conv1d_349\n",
      "conv1d_340\n",
      "conv1d_331\n",
      "conv1d_322\n",
      "conv1d_313\n",
      "conv1d_304\n",
      "conv1d_295\n",
      "conv1d_286\n",
      "conv1d_277\n",
      "batch_normalization_776\n",
      "batch_normalization_763\n",
      "batch_normalization_750\n",
      "batch_normalization_737\n",
      "batch_normalization_724\n",
      "batch_normalization_711\n",
      "batch_normalization_698\n",
      "batch_normalization_685\n",
      "batch_normalization_672\n",
      "batch_normalization_659\n",
      "batch_normalization_646\n",
      "batch_normalization_633\n",
      "batch_normalization_620\n",
      "batch_normalization_607\n",
      "batch_normalization_594\n",
      "batch_normalization_581\n",
      "batch_normalization_568\n",
      "batch_normalization_555\n",
      "batch_normalization_542\n",
      "batch_normalization_529\n",
      "batch_normalization_516\n",
      "batch_normalization_503\n",
      "batch_normalization_490\n",
      "batch_normalization_477\n",
      "batch_normalization_464\n",
      "batch_normalization_451\n",
      "batch_normalization_438\n",
      "batch_normalization_425\n",
      "batch_normalization_412\n",
      "batch_normalization_399\n",
      "activation_836\n",
      "activation_822\n",
      "activation_808\n",
      "activation_794\n",
      "activation_780\n",
      "activation_766\n",
      "activation_752\n",
      "activation_738\n",
      "activation_724\n",
      "activation_710\n",
      "activation_696\n",
      "activation_682\n",
      "activation_668\n",
      "activation_654\n",
      "activation_640\n",
      "activation_626\n",
      "activation_612\n",
      "activation_598\n",
      "activation_584\n",
      "activation_570\n",
      "activation_556\n",
      "activation_542\n",
      "activation_528\n",
      "activation_514\n",
      "activation_500\n",
      "activation_486\n",
      "activation_472\n",
      "activation_458\n",
      "activation_444\n",
      "activation_430\n",
      "conv1d_539\n",
      "conv1d_530\n",
      "conv1d_521\n",
      "conv1d_512\n",
      "conv1d_503\n",
      "conv1d_494\n",
      "conv1d_485\n",
      "conv1d_476\n",
      "conv1d_467\n",
      "conv1d_458\n",
      "conv1d_449\n",
      "conv1d_440\n",
      "conv1d_431\n",
      "conv1d_422\n",
      "conv1d_413\n",
      "conv1d_404\n",
      "conv1d_395\n",
      "conv1d_386\n",
      "conv1d_377\n",
      "conv1d_368\n",
      "conv1d_359\n",
      "conv1d_350\n",
      "conv1d_341\n",
      "conv1d_332\n",
      "conv1d_323\n",
      "conv1d_314\n",
      "conv1d_305\n",
      "conv1d_296\n",
      "conv1d_287\n",
      "conv1d_278\n",
      "batch_normalization_777\n",
      "batch_normalization_764\n",
      "batch_normalization_751\n",
      "batch_normalization_738\n",
      "batch_normalization_725\n",
      "batch_normalization_712\n",
      "batch_normalization_699\n",
      "batch_normalization_686\n",
      "batch_normalization_673\n",
      "batch_normalization_660\n",
      "batch_normalization_647\n",
      "batch_normalization_634\n",
      "batch_normalization_621\n",
      "batch_normalization_608\n",
      "batch_normalization_595\n",
      "batch_normalization_582\n",
      "batch_normalization_569\n",
      "batch_normalization_556\n",
      "batch_normalization_543\n",
      "batch_normalization_530\n",
      "batch_normalization_517\n",
      "batch_normalization_504\n",
      "batch_normalization_491\n",
      "batch_normalization_478\n",
      "batch_normalization_465\n",
      "batch_normalization_452\n",
      "batch_normalization_439\n",
      "batch_normalization_426\n",
      "batch_normalization_413\n",
      "batch_normalization_400\n",
      "activation_837\n",
      "activation_823\n",
      "activation_809\n",
      "activation_795\n",
      "activation_781\n",
      "activation_767\n",
      "activation_753\n",
      "activation_739\n",
      "activation_725\n",
      "activation_711\n",
      "activation_697\n",
      "activation_683\n",
      "activation_669\n",
      "activation_655\n",
      "activation_641\n",
      "activation_627\n",
      "activation_613\n",
      "activation_599\n",
      "activation_585\n",
      "activation_571\n",
      "activation_557\n",
      "activation_543\n",
      "activation_529\n",
      "activation_515\n",
      "activation_501\n",
      "activation_487\n",
      "activation_473\n",
      "activation_459\n",
      "activation_445\n",
      "activation_431\n",
      "gru_239\n",
      "gru_235\n",
      "gru_231\n",
      "gru_227\n",
      "gru_223\n",
      "gru_219\n",
      "gru_215\n",
      "gru_211\n",
      "gru_207\n",
      "gru_203\n",
      "gru_199\n",
      "gru_195\n",
      "gru_191\n",
      "gru_187\n",
      "gru_183\n",
      "gru_179\n",
      "gru_175\n",
      "gru_171\n",
      "gru_167\n",
      "gru_163\n",
      "gru_159\n",
      "gru_155\n",
      "gru_151\n",
      "gru_147\n",
      "gru_143\n",
      "gru_139\n",
      "gru_135\n",
      "gru_131\n",
      "gru_127\n",
      "gru_123\n",
      "batch_normalization_778\n",
      "batch_normalization_765\n",
      "batch_normalization_752\n",
      "batch_normalization_739\n",
      "batch_normalization_726\n",
      "batch_normalization_713\n",
      "batch_normalization_700\n",
      "batch_normalization_687\n",
      "batch_normalization_674\n",
      "batch_normalization_661\n",
      "batch_normalization_648\n",
      "batch_normalization_635\n",
      "batch_normalization_622\n",
      "batch_normalization_609\n",
      "batch_normalization_596\n",
      "batch_normalization_583\n",
      "batch_normalization_570\n",
      "batch_normalization_557\n",
      "batch_normalization_544\n",
      "batch_normalization_531\n",
      "batch_normalization_518\n",
      "batch_normalization_505\n",
      "batch_normalization_492\n",
      "batch_normalization_479\n",
      "batch_normalization_466\n",
      "batch_normalization_453\n",
      "batch_normalization_440\n",
      "batch_normalization_427\n",
      "batch_normalization_414\n",
      "batch_normalization_401\n",
      "activation_838\n",
      "activation_824\n",
      "activation_810\n",
      "activation_796\n",
      "activation_782\n",
      "activation_768\n",
      "activation_754\n",
      "activation_740\n",
      "activation_726\n",
      "activation_712\n",
      "activation_698\n",
      "activation_684\n",
      "activation_670\n",
      "activation_656\n",
      "activation_642\n",
      "activation_628\n",
      "activation_614\n",
      "activation_600\n",
      "activation_586\n",
      "activation_572\n",
      "activation_558\n",
      "activation_544\n",
      "activation_530\n",
      "activation_516\n",
      "activation_502\n",
      "activation_488\n",
      "activation_474\n",
      "activation_460\n",
      "activation_446\n",
      "activation_432\n",
      "gru_240\n",
      "gru_236\n",
      "gru_232\n",
      "gru_228\n",
      "gru_224\n",
      "gru_220\n",
      "gru_216\n",
      "gru_212\n",
      "gru_208\n",
      "gru_204\n",
      "gru_200\n",
      "gru_196\n",
      "gru_192\n",
      "gru_188\n",
      "gru_184\n",
      "gru_180\n",
      "gru_176\n",
      "gru_172\n",
      "gru_168\n",
      "gru_164\n",
      "gru_160\n",
      "gru_156\n",
      "gru_152\n",
      "gru_148\n",
      "gru_144\n",
      "gru_140\n",
      "gru_136\n",
      "gru_132\n",
      "gru_128\n",
      "gru_124\n",
      "dense_61\n",
      "dense_60\n",
      "dense_59\n",
      "dense_58\n",
      "dense_57\n",
      "dense_56\n",
      "dense_55\n",
      "dense_54\n",
      "dense_53\n",
      "dense_52\n",
      "dense_51\n",
      "dense_50\n",
      "dense_49\n",
      "dense_48\n",
      "dense_47\n",
      "dense_46\n",
      "dense_45\n",
      "dense_44\n",
      "dense_43\n",
      "dense_42\n",
      "dense_41\n",
      "dense_40\n",
      "dense_39\n",
      "dense_38\n",
      "dense_37\n",
      "dense_36\n",
      "dense_35\n",
      "dense_34\n",
      "dense_33\n",
      "dense_32\n",
      "batch_normalization_779\n",
      "batch_normalization_766\n",
      "batch_normalization_753\n",
      "batch_normalization_740\n",
      "batch_normalization_727\n",
      "batch_normalization_714\n",
      "batch_normalization_701\n",
      "batch_normalization_688\n",
      "batch_normalization_675\n",
      "batch_normalization_662\n",
      "batch_normalization_649\n",
      "batch_normalization_636\n",
      "batch_normalization_623\n",
      "batch_normalization_610\n",
      "batch_normalization_597\n",
      "batch_normalization_584\n",
      "batch_normalization_571\n",
      "batch_normalization_558\n",
      "batch_normalization_545\n",
      "batch_normalization_532\n",
      "batch_normalization_519\n",
      "batch_normalization_506\n",
      "batch_normalization_493\n",
      "batch_normalization_480\n",
      "batch_normalization_467\n",
      "batch_normalization_454\n",
      "batch_normalization_441\n",
      "batch_normalization_428\n",
      "batch_normalization_415\n",
      "batch_normalization_402\n",
      "activation_839\n",
      "activation_825\n",
      "activation_811\n",
      "activation_797\n",
      "activation_783\n",
      "activation_769\n",
      "activation_755\n",
      "activation_741\n",
      "activation_727\n",
      "activation_713\n",
      "activation_699\n",
      "activation_685\n",
      "activation_671\n",
      "activation_657\n",
      "activation_643\n",
      "activation_629\n",
      "activation_615\n",
      "activation_601\n",
      "activation_587\n",
      "activation_573\n",
      "activation_559\n",
      "activation_545\n",
      "activation_531\n",
      "activation_517\n",
      "activation_503\n",
      "activation_489\n",
      "activation_475\n",
      "activation_461\n",
      "activation_447\n",
      "activation_433\n",
      "tf.expand_dims_59\n",
      "tf.expand_dims_58\n",
      "tf.expand_dims_57\n",
      "tf.expand_dims_56\n",
      "tf.expand_dims_55\n",
      "tf.expand_dims_54\n",
      "tf.expand_dims_53\n",
      "tf.expand_dims_52\n",
      "tf.expand_dims_51\n",
      "tf.expand_dims_50\n",
      "tf.expand_dims_49\n",
      "tf.expand_dims_48\n",
      "tf.expand_dims_47\n",
      "tf.expand_dims_46\n",
      "tf.expand_dims_45\n",
      "tf.expand_dims_44\n",
      "tf.expand_dims_43\n",
      "tf.expand_dims_42\n",
      "tf.expand_dims_41\n",
      "tf.expand_dims_40\n",
      "tf.expand_dims_39\n",
      "tf.expand_dims_38\n",
      "tf.expand_dims_37\n",
      "tf.expand_dims_36\n",
      "tf.expand_dims_35\n",
      "tf.expand_dims_34\n",
      "tf.expand_dims_33\n",
      "tf.expand_dims_32\n",
      "tf.expand_dims_31\n",
      "tf.expand_dims_30\n",
      "concatenate_1\n",
      "bidirectional_1\n",
      "dense_62\n",
      "dense_63\n"
     ]
    }
   ],
   "source": [
    "for layer in source_model.layers:\n",
    "    print(layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "469f044b-91bf-4e9e-9a3a-6892778078cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_group = ['BBCA']\n",
    "target_group_name = '_'.join(target_group)\n",
    "\n",
    "# Load dataset for retraining\n",
    "shift = 0\n",
    "interval = 1\n",
    "recurrent = 120\n",
    "db_ver = '3'\n",
    "dataset_ver = '4'\n",
    "split = 0.8\n",
    "generator = False\n",
    "batch_size = 32\n",
    "train_inputs, train_labels, train_changes, test_inputs, test_labels, test_changes, data_version = core.load_dataset(target_group, shift, interval, recurrent, db_ver, dataset_ver, split, ROOT_PATH, generator=generator, batch_size=batch_size)\n",
    "\n",
    "# Eliminate nan\n",
    "# train_inputs = core.np.nan_to_num(train_inputs, posinf=0.0, neginf=0.0)\n",
    "# test_inputs = core.np.nan_to_num(test_inputs, posinf=0.0, neginf=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58111dd7-d49f-401f-90f0-66a938ef83dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPEN MODEL (whole-data-traning_11, epochs-subepochs)\n",
    "# 0.4 vid_constituents\n",
    "hotstart_from = '327-3_cl-0.5_idc-1_mid-0.4_ep-5_bz-1024_ldtv-2_sbuffs-1_gen-1_1-4'\n",
    "\n",
    "# 0.5 vid_constituents\n",
    "hotstart_from = '327-3_cl-0.5_idc-1_mid-0.5_ep-5_bz-2048_ldtv-2_sbuffs-1_gen-1_4-16'\n",
    "hotstart_from = '327-3_cl-0.5_idc-1_mid-0.5_ep-5_bz-2048_ldtv-2_sbuffs-1_gen-1_4-19'\n",
    "\n",
    "ROOT_PATH = './'\n",
    "\n",
    "hotstart_save_path = core.os.path.join(ROOT_PATH, f'models/preloaded/{hotstart_from}')\n",
    "model = core.tf.keras.models.load_model(f'{hotstart_save_path}/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4b96e68-fe28-48d1-9c32-1893daa2d725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Safe dataset list\n",
    "dataset = 'vid'\n",
    "db_ver = '8'\n",
    "DB_ROOT_PATH = 'E:\\#PROJECT\\idx'\n",
    "model_no = '327'\n",
    "sdl = core.safe_dataset_list(dataset, db_ver=db_ver, DB_ROOT_PATH=DB_ROOT_PATH, return_full_filename=True, model_no=model_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34f672ad-76e3-4ddf-9847-e1a1e4959ad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AALI.JK_shift0_interval5_recurrent120_split0.8.hdf5',\n",
       " 'ABMM.JK_shift0_interval5_recurrent120_split0.8.hdf5',\n",
       " 'ACES.JK_shift0_interval5_recurrent120_split0.8.hdf5',\n",
       " 'ACST.JK_shift0_interval5_recurrent120_split0.8.hdf5',\n",
       " 'ADES.JK_shift0_interval5_recurrent120_split0.8.hdf5',\n",
       " 'ADHI.JK_shift0_interval5_recurrent120_split0.8.hdf5',\n",
       " 'ADMF.JK_shift0_interval5_recurrent120_split0.8.hdf5',\n",
       " 'ADRO.JK_shift0_interval5_recurrent120_split0.8.hdf5',\n",
       " 'AGII.JK_shift0_interval5_recurrent120_split0.8.hdf5',\n",
       " 'AGRO.JK_shift0_interval5_recurrent120_split0.8.hdf5']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdl[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c4c7ebd9-91f1-4623-bd80-920034a859ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load v2 dataset to test wheter the model is valid or not with newer db version\n",
    "DB_ROOT_PATH = 'E:\\\\#PROJECT\\\\idx'\n",
    "db_path = core.os.path.join(DB_ROOT_PATH, 'db')\n",
    "db_path = core.os.path.join(db_path, 'v8')\n",
    "db_path = core.os.path.join(db_path, 'dataset')\n",
    "db_path = core.os.path.join(db_path, 'vid')\n",
    "db_path = core.os.path.join(db_path, 'INDF.JK_shift0_interval5_recurrent120_split0.8.hdf5')\n",
    "# db_path = 'J:\\#PROJECT\\idx\\db\\v8\\dataset\\vid\\BBCA.JK_shift0_interval5_recurrent120_split0.8.hdf5'\n",
    "with core.h5py.File(db_path, 'r') as f:\n",
    "    train_inputs = f['train_inputs'][()]\n",
    "    test_inputs = f['test_inputs'][()]\n",
    "    train_labels = f['train_labels'][()]\n",
    "    train_changes = f['train_changes'][()]\n",
    "    test_labels = f['test_labels'][()]\n",
    "    test_changes = f['test_changes'][()]\n",
    "    \n",
    "train_inputs = core.np.nan_to_num(train_inputs, posinf=0.0, neginf=0.0)\n",
    "test_inputs = core.np.nan_to_num(test_inputs, posinf=0.0, neginf=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5cb5a2d1-4f9f-47b2-80b5-4d12a0d6023f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107/107 [==============================] - 22s 205ms/step - loss: 0.6107 - accuracy: 0.6721\n",
      "24/24 [==============================] - 5s 204ms/step - loss: 0.6928 - accuracy: 0.5645\n",
      "Evaluation results: \n",
      "TRAIN\n",
      "Loss: 0.611, accuracy: 0.672\n",
      "TEST\n",
      "Loss: 0.693, accuracy: 0.564\n",
      "0.6107 0.6721 0.6928 0.5645\n"
     ]
    }
   ],
   "source": [
    "evaluate_train = model.evaluate(train_inputs, train_labels, verbose=1)\n",
    "evaluate_test = model.evaluate(test_inputs, test_labels, verbose=1)\n",
    "print(f'Evaluation results: \\nTRAIN\\nLoss: {evaluate_train[0]:.3f}, accuracy: {evaluate_train[1]:.3f}\\nTEST\\nLoss: {evaluate_test[0]:.3f}, accuracy: {evaluate_test[1]:.3f}')\n",
    "\n",
    "print(f'{evaluate_train[0]:.4f} {evaluate_train[1]:.4f} {evaluate_test[0]:.4f} {evaluate_test[1]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2efabfa3-5007-4268-a78c-7f58c098edb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ideal:  [ 43.25730415 -26.21570061] \n",
      "Train real:  [19.85126233 -2.80965879] \n",
      "Test ideal:  [ 6.69984972 -6.52452376] \n",
      "Test real:  [ 0.92507786 -0.7497519 ]\n",
      "\n",
      "Model backtest performance: tr 0.46, tnr 0.11, vr 0.14, vnr 0.11\n",
      "above results is in performance fraction between ideal (observed) and real condition produced from the model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print(f'Best model metadata:\\n', best_model)\n",
    "train_ideal, train_real, test_ideal, test_real = core.backtest_v1(model, train_inputs, test_inputs, train_labels, test_labels, train_changes, test_changes)\n",
    "print('Train ideal: ', train_ideal[0], '\\nTrain real: ', train_real[0], '\\nTest ideal: ', test_ideal[0], '\\nTest real: ', test_real[0])\n",
    "train_r, train_nr, test_r, test_nr = core.performance_ratio(train_ideal, train_real, test_ideal, test_real)\n",
    "print(f'\\nModel backtest performance: tr {train_r:.2f}, tnr {train_nr:.2f}, vr {test_r:.2f}, vnr {test_nr:.2f}')\n",
    "print('above results is in performance fraction between ideal (observed) and real condition produced from the model\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e30c927d-8aa1-4f0e-b8e7-0153e2d6f623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "927074"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43f80f40-ea80-42d4-8f65-20cc0d3eb966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 120, 60)]         0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 112)               77504     \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 112)              448       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " activation (Activation)     (None, 112)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 112)               12656     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 112)              448       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 112)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 96)                10848     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 194       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 102,098\n",
      "Trainable params: 101,650\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelv3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a490fff-1dee-45bb-bce4-7a11db8e1deb",
   "metadata": {},
   "source": [
    "### Count and negate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "95427f6e-d6f8-4072-a875-939d260241b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "full_page = np.arange(1,101,1)\n",
    "laminating = [1,2,3,4,5,6,7,8,9,10,37,38,39,40,46,47,48,49,50,51,52, 61,62,63,64]\n",
    "non_laminating = [page for page in full_page if page not in laminating]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "cc7730e0-2a73-4e38-855d-44498bca3b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 37, 38, 39, 40, 46, 47, 48, 49, 50, 51, 52, 61, 62, 63, 64]\n"
     ]
    }
   ],
   "source": [
    "print(laminating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "570f4706-a2be-4046-add8-3453320ea2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 41, 42, 43, 44, 45, 53, 54, 55, 56, 57, 58, 59, 60, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100]\n"
     ]
    }
   ],
   "source": [
    "print(non_laminating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "b0bd9c09-a4da-4206-adf9-2d26edce0311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(laminating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "d31bcdbe-f96a-4fa3-9d01-4f211f783eac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(non_laminating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "86c48177-fa37-4f5a-a23f-b5dee5ef6913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count | halaman yang dilaminating\n",
      "1    1\n",
      "2    2\n",
      "3    3\n",
      "4    4\n",
      "5    5\n",
      "6    6\n",
      "7    7\n",
      "8    8\n",
      "9    9\n",
      "10    10\n",
      "11    37\n",
      "12    38\n",
      "13    39\n",
      "14    40\n",
      "15    46\n",
      "16    47\n",
      "17    48\n",
      "18    49\n",
      "19    50\n",
      "20    51\n",
      "21    52\n",
      "22    61\n",
      "23    62\n",
      "24    63\n",
      "25    64\n"
     ]
    }
   ],
   "source": [
    "print(\"count | halaman yang dilaminating\")\n",
    "for i, l in enumerate(laminating):\n",
    "    print(i+1, '  ', l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "858d5c3d-cf29-4f15-b57e-55456ea5626a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count | halaman yang tidak dilaminating\n",
      "11-36\n",
      "41-45\n",
      "53-60\n",
      "65-100\n"
     ]
    }
   ],
   "source": [
    "print(\"count | halaman yang tidak dilaminating\")\n",
    "expected = non_laminating[0]\n",
    "start_range = None\n",
    "for i, l in enumerate(non_laminating):\n",
    "    l = int(l)\n",
    "    # print(l)\n",
    "    if l == expected:\n",
    "        if start_range == None:\n",
    "            start_range = l\n",
    "        elif start_range != None:\n",
    "            end_range = l\n",
    "    elif l != expected:\n",
    "        if start_range != None:\n",
    "            # print(f'exp:{expected}')\n",
    "            print(f'{start_range}-{end_range}')\n",
    "            start_range = l\n",
    "        elif start_range == None:\n",
    "            print(l)\n",
    "    if i == len(non_laminating) - 1:\n",
    "        # print(f'exp:{expected}')\n",
    "        print(f'{start_range}-{end_range}')\n",
    "            \n",
    "    expected = l + 1\n",
    "        \n",
    "    # print(i+1, '  ', l)\n",
    "    # expected = l + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333965ac-de7f-4705-96d1-f3ad5a714cc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ea1e131-8b22-4e17-bdec-922862d58e65",
   "metadata": {},
   "source": [
    "### Try to train model and verify whether it could provide nan results or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df63388c-a12d-4363-974c-d003597a1632",
   "metadata": {},
   "outputs": [],
   "source": [
    "import coremlv2 as core\n",
    "import importlib\n",
    "importlib.reload(core)\n",
    "core.os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "core._init_ml()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c70a6949-e56d-40fa-a1b4-8f25391ca249",
   "metadata": {},
   "outputs": [],
   "source": [
    "import coremlv2 as core; core._init_ml(); core.os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "core.safe_dataset_analyzer(db_ver='8', DB_ROOT_PATH='J:\\#PROJECT\\idx', model_no='321', start_from='vcn') # deployed\n",
    "core.safe_dataset_analyzer(db_ver='8', DB_ROOT_PATH='J:\\#PROJECT\\idx', model_no='322', start_from='vcn') # deployed\n",
    "core.safe_dataset_analyzer(db_ver='8', DB_ROOT_PATH='J:\\#PROJECT\\idx', model_no='323', start_from='vcn') # deployed\n",
    "core.safe_dataset_analyzer(db_ver='8', DB_ROOT_PATH='J:\\#PROJECT\\idx', model_no='324', start_from='vcn') # deployed\n",
    "core.safe_dataset_analyzer(db_ver='8', DB_ROOT_PATH='J:\\#PROJECT\\idx', model_no='325', start_from='vcn') # deployed\n",
    "\n",
    "# Broadcast result to other model (focus on 321)\n",
    "core.safe_dataset_analyzer(db_ver='8', DB_ROOT_PATH='J:\\#PROJECT\\idx', model_no='321', start_from='vde', stop_at='vfr')\n",
    "core.safe_dataset_analyzer(db_ver='8', DB_ROOT_PATH='J:\\#PROJECT\\idx', model_no='321', start_from='vfr', stop_at='vid')\n",
    "core.safe_dataset_analyzer(db_ver='8', DB_ROOT_PATH='J:\\#PROJECT\\idx', model_no='321', start_from='vid', stop_at='vis')\n",
    "core.safe_dataset_analyzer(db_ver='8', DB_ROOT_PATH='J:\\#PROJECT\\idx', model_no='321', start_from='vis', stop_at='vmx')\n",
    "core.safe_dataset_analyzer(db_ver='8', DB_ROOT_PATH='J:\\#PROJECT\\idx', model_no='321', start_from='vmx', stop_at='vnz')\n",
    "core.safe_dataset_analyzer(db_ver='8', DB_ROOT_PATH='J:\\#PROJECT\\idx', model_no='321', start_from='vnz', stop_at='vru')\n",
    "core.safe_dataset_analyzer(db_ver='8', DB_ROOT_PATH='J:\\#PROJECT\\idx', model_no='321', start_from='vru', stop_at='vtr')\n",
    "core.safe_dataset_analyzer(db_ver='8', DB_ROOT_PATH='J:\\#PROJECT\\idx', model_no='321', start_from='vtr', stop_at=None)\n",
    "\n",
    "# Analyze only certain stocks that throw nan_loss either in loss / val_loss\n",
    "# Use G: as DB_ROOT_PATH to avoid interference with ongoing dataset nan_loss scan in J:\n",
    "core.safe_dataset_analyzer(db_ver='8', DB_ROOT_PATH='J:\\#PROJECT\\idx', model_no='326', analyze_only='vbr') # finished\n",
    "core.safe_dataset_analyzer(db_ver='8', DB_ROOT_PATH='J:\\#PROJECT\\idx', model_no='326', analyze_only='vid') # finished\n",
    "core.safe_dataset_analyzer(db_ver='8', DB_ROOT_PATH='J:\\#PROJECT\\idx', model_no='326', analyze_only='vcn') # finished\n",
    "core.safe_dataset_analyzer(db_ver='8', DB_ROOT_PATH='J:\\#PROJECT\\idx', model_no='326', analyze_only='vde') # finished #\n",
    "core.safe_dataset_analyzer(db_ver='8', DB_ROOT_PATH='J:\\#PROJECT\\idx', model_no='326', analyze_only='vdk') # finished\n",
    "core.safe_dataset_analyzer(db_ver='8', DB_ROOT_PATH='J:\\#PROJECT\\idx', model_no='326', analyze_only='ves') # finished\n",
    "core.safe_dataset_analyzer(db_ver='8', DB_ROOT_PATH='J:\\#PROJECT\\idx', model_no='326', analyze_only='vfr') # finished\n",
    "core.safe_dataset_analyzer(db_ver='8', DB_ROOT_PATH='J:\\#PROJECT\\idx', model_no='326', analyze_only='vgb') # finished\n",
    "core.safe_dataset_analyzer(db_ver='8', DB_ROOT_PATH='J:\\#PROJECT\\idx', model_no='326', analyze_only='vgr') # finished\n",
    "core.safe_dataset_analyzer(db_ver='8', DB_ROOT_PATH='J:\\#PROJECT\\idx', model_no='326', analyze_only='vhk') # finished\n",
    "core.safe_dataset_analyzer(db_ver='8', DB_ROOT_PATH='J:\\#PROJECT\\idx', model_no='326', analyze_only='vus') # finished\n",
    "core.safe_dataset_analyzer(db_ver='8', DB_ROOT_PATH='J:\\#PROJECT\\idx', model_no='326', analyze_only='vil') # finished\n",
    "\n",
    "# vau had no eligible tickers\n",
    "core.safe_dataset_analyzer(db_ver='8', DB_ROOT_PATH='J:\\#PROJECT\\idx', model_no='326', analyze_only='vtw') # finished\n",
    "core.safe_dataset_analyzer(db_ver='8', DB_ROOT_PATH='J:\\#PROJECT\\idx', model_no='326', analyze_only='vau') # skipped--\n",
    "core.safe_dataset_analyzer(db_ver='8', DB_ROOT_PATH='J:\\#PROJECT\\idx', model_no='326', analyze_only='vat') # finished\n",
    "core.safe_dataset_analyzer(db_ver='8', DB_ROOT_PATH='J:\\#PROJECT\\idx', model_no='326', analyze_only='vbr') # finished\n",
    "core.safe_dataset_analyzer(db_ver='8', DB_ROOT_PATH='J:\\#PROJECT\\idx', model_no='326', analyze_only='vin') # finished\n",
    "core.safe_dataset_analyzer(db_ver='8', DB_ROOT_PATH='J:\\#PROJECT\\idx', model_no='326', analyze_only='vkr') # finished\n",
    "core.safe_dataset_analyzer(db_ver='8', DB_ROOT_PATH='J:\\#PROJECT\\idx', model_no='326', analyze_only='vjp') # finished\n",
    "\n",
    "# Speculative analysis\n",
    "core.safe_dataset_analyzer(db_ver='8', DB_ROOT_PATH='J:\\#PROJECT\\idx', model_no='326', analyze_only='vlv') # finished\n",
    "core.safe_dataset_analyzer(db_ver='8', DB_ROOT_PATH='J:\\#PROJECT\\idx', model_no='326', analyze_only='vmx') # finished\n",
    "core.safe_dataset_analyzer(db_ver='8', DB_ROOT_PATH='J:\\#PROJECT\\idx', model_no='326', analyze_only='vmy') # finished\n",
    "core.safe_dataset_analyzer(db_ver='8', DB_ROOT_PATH='J:\\#PROJECT\\idx', model_no='326', analyze_only='vnl') # finished\n",
    "core.safe_dataset_analyzer(db_ver='8', DB_ROOT_PATH='J:\\#PROJECT\\idx', model_no='326', analyze_only='vno') # finished\n",
    "core.safe_dataset_analyzer(db_ver='8', DB_ROOT_PATH='J:\\#PROJECT\\idx', model_no='326', analyze_only='vnz') # finished\n",
    "core.safe_dataset_analyzer(db_ver='8', DB_ROOT_PATH='J:\\#PROJECT\\idx', model_no='326', analyze_only='vpl') # finished\n",
    "core.safe_dataset_analyzer(db_ver='8', DB_ROOT_PATH='J:\\#PROJECT\\idx', model_no='326', analyze_only='vpt') # finished\n",
    "core.safe_dataset_analyzer(db_ver='8', DB_ROOT_PATH='J:\\#PROJECT\\idx', model_no='326', analyze_only='vqa') # finished\n",
    "core.safe_dataset_analyzer(db_ver='8', DB_ROOT_PATH='J:\\#PROJECT\\idx', model_no='326', analyze_only='vru') # finished\n",
    "core.safe_dataset_analyzer(db_ver='8', DB_ROOT_PATH='J:\\#PROJECT\\idx', model_no='326', analyze_only='vse') # finished\n",
    "core.safe_dataset_analyzer(db_ver='8', DB_ROOT_PATH='J:\\#PROJECT\\idx', model_no='326', analyze_only='vsg') # finished\n",
    "core.safe_dataset_analyzer(db_ver='8', DB_ROOT_PATH='J:\\#PROJECT\\idx', model_no='326', analyze_only='vth') # finished\n",
    "core.safe_dataset_analyzer(db_ver='8', DB_ROOT_PATH='J:\\#PROJECT\\idx', model_no='326', analyze_only='vtr') # finished\n",
    "\n",
    "# Unavailable nan_loss\n",
    "core.safe_dataset_analyzer(db_ver='8', DB_ROOT_PATH='J:\\#PROJECT\\idx', model_no='326', analyze_only='var')\n",
    "core.safe_dataset_analyzer(db_ver='8', DB_ROOT_PATH='J:\\#PROJECT\\idx', model_no='326', analyze_only='vbe')\n",
    "core.safe_dataset_analyzer(db_ver='8', DB_ROOT_PATH='J:\\#PROJECT\\idx', model_no='326', analyze_only='vch')\n",
    "core.safe_dataset_analyzer(db_ver='8', DB_ROOT_PATH='J:\\#PROJECT\\idx', model_no='326', analyze_only='vcz')\n",
    "core.safe_dataset_analyzer(db_ver='8', DB_ROOT_PATH='J:\\#PROJECT\\idx', model_no='326', analyze_only='vfi')\n",
    "core.safe_dataset_analyzer(db_ver='8', DB_ROOT_PATH='J:\\#PROJECT\\idx', model_no='326', analyze_only='vie')\n",
    "core.safe_dataset_analyzer(db_ver='8', DB_ROOT_PATH='J:\\#PROJECT\\idx', model_no='326', analyze_only='vis')\n",
    "\n",
    "# New model analysis\n",
    "core.safe_dataset_analyzer(db_ver='8', DB_ROOT_PATH='J:\\#PROJECT\\idx', model_no='326', analyze_only='vid') # finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2255987-5231-4b68-9aed-dd4d250e3f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total constituents: 12\n"
     ]
    }
   ],
   "source": [
    "# Run simple model test\n",
    "gen = core.load_dataset_wsd_traintest(subset='validation', dataset_size='small', ROOT_PATH='H:\\#PROJECT\\idx', db_ver='8', batch_size=128, shuffle_buffer_size=1024, seed=0, generator=False)\n",
    "\n",
    "internal_file_path = core.os.path.join(dataset_path, 'vid')\n",
    "internal_file_path = core.os.path.join(internal_file_path, 'ACES.JK_shift0_interval5_recurrent120_split0.8.hdf5')\n",
    "\n",
    "with core.h5py.File(internal_file_path, 'r') as f:\n",
    "    val_inputs, val_labels = f['train_inputs'][()], f['train_labels'][()]\n",
    "    inputs, labels = f['test_inputs'][()], f['test_labels'][()]\n",
    "\n",
    "# Run model\n",
    "model = core.sample_model_v1()\n",
    "history = model.fit(inputs, labels, validation_data=(val_inputs, val_labels), epochs=1, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7eb77f1-3439-47a1-8d40-3cf4fb27b338",
   "metadata": {},
   "source": [
    "### Check composition of nan_loss to all available tickers\n",
    "It turns out that all dataset produce nan_loss. Which part that it turns odd?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49203190-309b-44f7-9a70-872fb880eaf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'coremlv2' from 'C:\\\\Users\\\\Jonathan Raditya\\\\documents\\\\#PROJECT\\\\idx\\\\coremlv2.py'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import coremlv2 as core\n",
    "import importlib\n",
    "importlib.reload(core)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "291b4a6a-2c11-44d6-ae80-5e330cc236aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var: 61\n",
      "vat: 111\n",
      "vbe: 116\n",
      "vbr: 395\n",
      "vch: 89\n",
      "vcn: 1809\n",
      "vcz: 20\n",
      "vde: 4233\n",
      "vdk: 116\n",
      "ves: 133\n",
      "vfi: 154\n",
      "vfr: 572\n",
      "vgb: 1760\n",
      "vgr: 83\n",
      "vhk: 1863\n",
      "vid: 419\n",
      "vie: 29\n",
      "vil: 288\n",
      "vin: 3698\n",
      "vis: 15\n",
      "vjp: 2488\n",
      "vkr: 1550\n",
      "vlv: 7\n",
      "vmx: 279\n",
      "vmy: 757\n",
      "vnl: 70\n",
      "vno: 186\n",
      "vnz: 106\n",
      "vpl: 327\n",
      "vpt: 32\n",
      "vqa: 38\n",
      "vru: 203\n",
      "vse: 616\n",
      "vsg: 482\n",
      "vth: 507\n",
      "vtr: 369\n",
      "vtw: 1685\n",
      "vus: 4409\n"
     ]
    }
   ],
   "source": [
    "db_ver = '8'\n",
    "DB_ROOT_PATH = 'H:\\#PROJECT\\idx'\n",
    "vabbrs = core.extract_dataset_vabbrs()\n",
    "for vabbr in vabbrs:\n",
    "    available_tickers = core.safe_dataset_list(vabbr, db_ver=db_ver, DB_ROOT_PATH=DB_ROOT_PATH)\n",
    "    print(f'{vabbr}: {len(available_tickers)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bc61b1-a585-4a2a-ba3f-7ae874649874",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "397fface-156c-449a-88da-c45289b55d7e",
   "metadata": {},
   "source": [
    "### Check load data shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c30f8997-9060-470f-9e8a-10dbbd36096c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade tensorflow --quiet\n",
    "# !pip install keras_tuner --quiet\n",
    "# !pip install tensorflow-io --quiet\n",
    "# # Google colab modules\n",
    "# from google.colab import drive\n",
    "import sys, importlib\n",
    "\n",
    "# # Mount drive\n",
    "# drive.mount('/content/gdrive', force_remount=True)\n",
    "ROOT_PATH = './'\n",
    "# sys.path.append(ROOT_PATH)\n",
    "\n",
    "import coremlv2 as core\n",
    "core._init_ml()\n",
    "# core._init_models()\n",
    "# core.os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "# Reload coreml\n",
    "importlib.reload(core)\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5c9983f0-1feb-4356-9153-aa802a4ac19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total constituents: 42\n",
      "Total constituents: 42\n"
     ]
    }
   ],
   "source": [
    "# RERUN WITH REVISED traintest slice code\n",
    "model_no = '326'\n",
    "kt_iter = '33'\n",
    "ticker_group = ['wsd']\n",
    "epochs = 40\n",
    "max_epochs = 40\n",
    "batch_size = 64\n",
    "\n",
    "dataset_size = 'half_new_wsd'\n",
    "shuffle_buffer_size = 2048\n",
    "generator = False\n",
    "\n",
    "train_inputs, train_labels = core.load_dataset_wsd_traintest(subset='training', dataset_size=dataset_size, ROOT_PATH='''J:\\#PROJECT\\idx''', db_ver='8', batch_size=batch_size, shuffle_buffer_size=shuffle_buffer_size, seed=0, generator=generator, model_no=model_no)\n",
    "\n",
    "validation_inputs, validation_labels = core.load_dataset_wsd_traintest(subset='validation', dataset_size=dataset_size, ROOT_PATH='''J:\\#PROJECT\\idx''', db_ver='8', batch_size=batch_size, shuffle_buffer_size=shuffle_buffer_size, seed=0, generator=generator, model_no=model_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3173a9dc-0089-4f20-99aa-36c9ce18c761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99531, 120, 60)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c84ebe81-1034-40f5-b89f-509f1e0d7c4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18686, 120, 60)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_inputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a375127d-1b88-4bc5-88b7-3ec019a965c1",
   "metadata": {},
   "source": [
    "### Check nan_loss availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3f7db2e-f0d4-42a1-a471-6aaccdf8cbd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'coremlv2' from 'C:\\\\Users\\\\Jonathan Raditya\\\\documents\\\\#PROJECT\\\\idx\\\\coremlv2.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import coremlv2 as core\n",
    "import importlib\n",
    "importlib.reload(core)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b6a8daf-46d9-48dd-9fdb-105e63acc575",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = 'J:\\\\#PROJECT\\\\idx\\\\db\\\\v8\\\\dataset'\n",
    "current_model_no = '326'\n",
    "benchmark_model_no = '321'\n",
    "db_path_current = core.os.path.join(db_path, f'nan_loss_{current_model_no}')\n",
    "db_path_benchmark = core.os.path.join(db_path, f'nan_loss_{benchmark_model_no}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ceccbd16-517d-45d3-bae4-6de5ea29a68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_content = core.os.listdir(db_path_current)\n",
    "benchmark_content = core.os.listdir(db_path_benchmark)\n",
    "unavailable_slice = [content.split('_')[0] for content in benchmark_content if content not in current_content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0364e30f-900b-4f62-8aad-d3794780442f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['var', 'vbe', 'vch', 'vcz', 'vfi', 'vie', 'vis']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unavailable_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2420302-9962-4545-a901-224c82c40a46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b868185a-eb98-4f58-bff2-ab05ee89dd0b",
   "metadata": {},
   "source": [
    "### Recreate generator so it can be used in NAS\n",
    "- Create simple modified function and NAS with 5 chars random kt_iter version\n",
    "- use \"small\" dataset_size\n",
    "- max_epochs = 2\n",
    "- test if NAS can be continued to next iteration\n",
    "\n",
    "`env:tensorflow`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7409bba8-2ad4-48dc-816f-610464bbf22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary modules\n",
    "import importlib\n",
    "ROOT_PATH = './'\n",
    "\n",
    "import coremlv2 as core\n",
    "core._init_ml()\n",
    "# core.os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "# Reload coreml\n",
    "importlib.reload(core)\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "011eaea3-ca40-468b-b06b-78a597579664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs,  1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "# Limiting GPU memory growth\n",
    "gpus = core.tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            core.tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = core.tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs, \", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b83da4d5-e753-4463-8dc6-436479381fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use generator\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import numpy as np\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, x_set, y_set, batch_size):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        return batch_x, batch_y     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4425ccb1-15f0-4ce7-8113-5814461f5e01",
   "metadata": {},
   "source": [
    "#### Create custom hdf5 pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e8b609e-ff74-4eea-a4be-612daf252941",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_wsd_traintest(subset, dataset_size, ROOT_PATH='''H:\\#PROJECT\\idx''', db_ver='8', batch_size=128, shuffle_buffer_size=8192, seed=0, generator=False, model_no='sample1'):\n",
    "    '''\n",
    "    subset: `training`, `validation`\n",
    "    dataset_size: `small`, `medium`, `large`\n",
    "    \n",
    "    use env:tensorflow.\n",
    "    '''\n",
    "    dataset_size_dict = {'xsmall':\n",
    "                         {'training':[True,0.0005,0.005],\n",
    "                          'validation':[False,0.00025,0.0025]},\n",
    "                         'small':\n",
    "                         {'training':[True,0.001,0.01],\n",
    "                          'validation':[False,0.0005,0.005]},\n",
    "                         'medium':\n",
    "                         {'training':[True,0.1,0.4],\n",
    "                          'validation':[False,0,1]},\n",
    "                         'large':\n",
    "                         {'training':[True,0.2,0.8],\n",
    "                          'validation':[False,0,1]},\n",
    "                         'extralarge':\n",
    "                         {'training':[True,0.5,1],\n",
    "                          'validation':[False,0,1]},\n",
    "                         'exploration':\n",
    "                         {'training':[True,1,1],\n",
    "                          'validation':[False,1,1]},\n",
    "                         'full':\n",
    "                         {'training':[True,0.4,0.8],\n",
    "                          'validation':[False,0,0.2]},\n",
    "                         'half_new_wsd':\n",
    "                         {'training':[True,0,0.1],\n",
    "                          'validation':[False,0,0.1]},\n",
    "                         'full_new_wsd':\n",
    "                         {'training':[True,0,1],\n",
    "                          'validation':[False,0,1]},\n",
    "                         'full_new_wsd_mix':\n",
    "                         {'training':[True,0.005,1],\n",
    "                          'validation':[False,0,1]},\n",
    "                         'full_new_wsd_mix1':\n",
    "                         {'training':[True,0.01,1],\n",
    "                          'validation':[False,0,1]},\n",
    "                         'full_new_wsd_mix2':\n",
    "                         {'training':[True,0.02,1],\n",
    "                          'validation':[False,0,1]},}\n",
    "    slice_from_beginning = dataset_size_dict[dataset_size][subset][0]\n",
    "    constituent_limits = dataset_size_dict[dataset_size][subset][1]\n",
    "    id_constituent = dataset_size_dict[dataset_size][subset][2]\n",
    "    \n",
    "    return load_dataset_wsd(ROOT_PATH=ROOT_PATH, db_ver=db_ver, constituent_limits=constituent_limits, id_constituent=id_constituent, batch_size=batch_size, shuffle_buffer_size=shuffle_buffer_size, seed=seed, slice_from_beginning=slice_from_beginning, generator=generator, model_no=model_no)\n",
    "\n",
    "def load_dataset_wsd(ROOT_PATH='''H:\\#PROJECT\\idx''', db_ver='8', constituent_limits=0.01, id_constituent=0.1, batch_size=128, shuffle_buffer_size=8192, seed=0, slice_from_beginning=True, generator=True, model_no='sample1'):\n",
    "    '''Load random world stock database constituent\n",
    "    to make up training database as generator.\n",
    "    \n",
    "    /test_inputs are used to make sure whole\n",
    "    dataset cover most of abstraction from various\n",
    "    tickers & stock markets (smaller constituent\n",
    "    compared to /train_inputs)\n",
    "    ex\n",
    "    Try:\n",
    "        (x) 1. remove shuffle constituent to prevent\n",
    "            stopiteration (aborted.)\n",
    "            \n",
    "        (v) 2. If vid is being looped and `slice_from_beginning` == True\n",
    "            (indicating vid for training set), load from /train_inputs\n",
    "            and /train_labels.\n",
    "            \n",
    "            Otherwise (`slice_from_beginning` == False) while vid being looped,\n",
    "            load from /test_inputs and /test_labels.\n",
    "    \n",
    "    use env:tensorflow.\n",
    "    \n",
    "    #### Run log\n",
    "    0. Default state from coremlv2\n",
    "    1. Move important functions to randomtry and construct pipeline. **Result**: Run and error after 1st iteration, as expected.\n",
    "    2. Add xsmall dataset_size for faster iteration, comment random.seed & random.shuffle to test if shuffling can cause error. **Result**: Stop iteration error.\n",
    "    3. Uncomment random.seed & random.shuffle. Use conventional h5py read and then pass it to DataGenerator class if generator. Continue as usual to next section. **Result**: Stop iteration error.\n",
    "    4. Try itertools.repeat. **Result**: Attribute error, no shape attribute.\n",
    "    5. Try itertools.cycle. **Result**: Infinite loop (never ending 1st epoch)\n",
    "    6. Restore to 0 state, try .concatenate method to chaining multiple IODataset. **Result**: SUCCESS!\n",
    "    7. Try using full_new_wsd to test if any memory concern occurs. **Result**: SUCCESS!. Fast dataset loading, shuffling, and able to continue to next iteration. No memory problem occurs.\n",
    "    8. Try using GPU, 5 epochs. **Result**: Strange result, the process finish so fast although all idx data were used. Turns out that .concatenate returns a modified object. \n",
    "    9. Modify `inputs = inputs.concatenate(next_inputs)`. **Result**: SUCCESS! Implemented version.\n",
    "\n",
    "    #### Additional info:\n",
    "    - full_new_wsd with model 327 only occupy 400mb memory. Shiet! wkwk\n",
    "    - next potential problem: how to increase gpu utilization and direct storage read speed.\n",
    "    - try different batch_size to see if overall speed is increased\n",
    "        Using 327/full_new_wsd\n",
    "        2048 -> , 2.2gb\n",
    "        512 -> 321ms/step, 536sec, 1.4gb\n",
    "        128 -> 96ms/step, 641sec, 400mb\n",
    "        64 -> 46ms/step, 726sec, 200mb\n",
    "    - more batch_size -> same gpu utilization but with higher board power draw & gpu clock (gpu-z)\n",
    "    - kt explore 327/34/full_new_wsd_mix terakhir lupa diganti hypermodelnya, masih model_326_kt. Di re-run aja\n",
    "    '''\n",
    "    # Construct dataset_path\n",
    "    dataset_path = core.os.path.join(ROOT_PATH, 'db')\n",
    "    dataset_path = core.os.path.join(dataset_path, f'v{db_ver}')\n",
    "    dataset_path = core.os.path.join(dataset_path, 'dataset')\n",
    "    \n",
    "    if not core.os.path.exists(dataset_path):\n",
    "        raise ValueError(f'Dataset path not exists: {dataset_path}')\n",
    "    elif core.os.path.exists(dataset_path):\n",
    "        dataset_list = core.extract_dataset_vabbrs(db_ver=db_ver, DB_ROOT_PATH=ROOT_PATH)\n",
    "        # dataset_list = os.listdir(dataset_path)\n",
    "        \n",
    "    # Fetch constituent list from database\n",
    "    file_list = {}\n",
    "    for dataset in dataset_list:\n",
    "        file_list[dataset] = core.safe_dataset_list(dataset, db_ver=db_ver, DB_ROOT_PATH=ROOT_PATH, return_full_filename=True, model_no=model_no)\n",
    "        # dataset_ver_path = os.path.join(dataset_path, dataset)\n",
    "        # if os.path.isdir(dataset_ver_path):\n",
    "        #     dataset_ver_files = os.listdir(dataset_ver_path)\n",
    "        #     file_list[dataset] = dataset_ver_files\n",
    "        # elif os.path.isfile(dataset_ver_path):\n",
    "        #     pass\n",
    "\n",
    "    # Select constituent based on consitituent_limits\n",
    "    '''\n",
    "    constituent_limits: percentage of different stocks that\n",
    "        make up the dataset\n",
    "    '''\n",
    "    constituent_list = {}\n",
    "    for file_list_key in file_list:\n",
    "        file = file_list[file_list_key]\n",
    "        if file_list_key == 'vid':\n",
    "            slicer = int(len(file) * id_constituent)\n",
    "        elif file_list_key != 'vid':\n",
    "            slicer = int(len(file) * constituent_limits)\n",
    "        if slicer < 1:\n",
    "            continue\n",
    "        elif slicer >= 1:\n",
    "            if slice_from_beginning:\n",
    "                constituent_list[file_list_key] = file[:slicer]\n",
    "            elif not slice_from_beginning:\n",
    "                constituent_list[file_list_key] = file[-slicer:]\n",
    "   \n",
    "    # Load from /test_inputs & /test_labels\n",
    "    for i, constituent_list_key in enumerate(constituent_list):\n",
    "        # `constituent_list_key`: vid, vus, etc.\n",
    "        constituents = constituent_list[constituent_list_key]\n",
    "        constituent_dir_path = core.os.path.join(dataset_path, constituent_list_key)\n",
    "        for j, constituent in enumerate(constituents):\n",
    "            # Full file path\n",
    "            constituent_file_path = core.os.path.join(constituent_dir_path, constituent)\n",
    "            if generator:\n",
    "                pass\n",
    "#                 if (constituent_list_key == 'vid') and slice_from_beginning:\n",
    "#                     # vid, training\n",
    "#                     inputs = core.tfio.IODataset.from_hdf5(constituent_file_path, '/train_inputs')\n",
    "#                     labels = core.tfio.IODataset.from_hdf5(constituent_file_path, '/train_labels')\n",
    "#                 elif ((constituent_list_key == 'vid') and not slice_from_beginning) or (constituent_list_key != 'vid'):\n",
    "#                     # vid, testing\n",
    "#                     # another stock market for train/testing\n",
    "#                     inputs = core.tfio.IODataset.from_hdf5(constituent_file_path, '/test_inputs')\n",
    "#                     labels = core.tfio.IODataset.from_hdf5(constituent_file_path, '/test_labels')\n",
    "\n",
    "#                 # Zip inputs and labels\n",
    "#                 gen = core.tf.data.Dataset.zip((inputs, labels))\n",
    "\n",
    "#                 # Apply batches\n",
    "#                 gen = gen.batch(batch_size)\n",
    "\n",
    "#                 # Apply shuffle\n",
    "#                 gen = gen.shuffle(buffer_size=shuffle_buffer_size)\n",
    "            elif not generator:\n",
    "                if (constituent_list_key == 'vid') and slice_from_beginning:\n",
    "                    # vid, training\n",
    "                    with core.h5py.File(constituent_file_path, 'r') as f:\n",
    "                        inputs = f['train_inputs'][()]\n",
    "                        labels = f['train_labels'][()]\n",
    "                elif ((constituent_list_key == 'vid') and not slice_from_beginning) or (constituent_list_key != 'vid'):\n",
    "                    # vid, testing\n",
    "                    # another stock market for train/testing\n",
    "                    with core.h5py.File(constituent_file_path, 'r') as f:\n",
    "                        inputs = f['test_inputs'][()]\n",
    "                        labels = f['test_labels'][()]\n",
    "                \n",
    "            # if generator:\n",
    "            #     gen = DataGenerator(inputs, labels, batch_size)\n",
    "            # elif not generator:\n",
    "            #     pass\n",
    "\n",
    "            if i + j == 0:\n",
    "                gen_counter = 0\n",
    "                if generator:\n",
    "                    # combined_gen = [gen]\n",
    "                    \n",
    "                    if (constituent_list_key == 'vid') and slice_from_beginning:\n",
    "                        # vid, training\n",
    "                        inputs = core.tfio.IODataset.from_hdf5(constituent_file_path, '/train_inputs')\n",
    "                        labels = core.tfio.IODataset.from_hdf5(constituent_file_path, '/train_labels')\n",
    "                    elif ((constituent_list_key == 'vid') and not slice_from_beginning) or (constituent_list_key != 'vid'):\n",
    "                        # vid, testing\n",
    "                        # another stock market for train/testing\n",
    "                        inputs = core.tfio.IODataset.from_hdf5(constituent_file_path, '/test_inputs')\n",
    "                        labels = core.tfio.IODataset.from_hdf5(constituent_file_path, '/test_labels')\n",
    "                    \n",
    "                elif not generator:\n",
    "                    # print(inputs.shape)\n",
    "                    combined_inputs = inputs.copy()\n",
    "                    combined_labels = labels.copy()\n",
    "                    constituents_count = 1\n",
    "            elif i + j > 0:\n",
    "                gen_counter += 1\n",
    "                if generator:\n",
    "                    # combined_gen.append(gen)\n",
    "                    \n",
    "                    if (constituent_list_key == 'vid') and slice_from_beginning:\n",
    "                        # vid, training\n",
    "                        next_inputs = core.tfio.IODataset.from_hdf5(constituent_file_path, '/train_inputs')\n",
    "                        next_labels = core.tfio.IODataset.from_hdf5(constituent_file_path, '/train_labels')\n",
    "                    elif ((constituent_list_key == 'vid') and not slice_from_beginning) or (constituent_list_key != 'vid'):\n",
    "                        # vid, testing\n",
    "                        # another stock market for train/testing\n",
    "                        next_inputs = core.tfio.IODataset.from_hdf5(constituent_file_path, '/test_inputs')\n",
    "                        next_labels = core.tfio.IODataset.from_hdf5(constituent_file_path, '/test_labels')\n",
    "                        \n",
    "                    inputs = inputs.concatenate(next_inputs)\n",
    "                    labels = labels.concatenate(next_labels)\n",
    "                    \n",
    "                elif not generator:\n",
    "                    combined_inputs = core.np.vstack((combined_inputs, inputs))\n",
    "                    combined_labels = core.np.vstack((combined_labels, labels))\n",
    "                    constituents_count += 1\n",
    "\n",
    "    if generator:\n",
    "        # print(f'Total constituents: {len(combined_gen)}')\n",
    "        # Shuffle constituent\n",
    "        # core.random.seed(seed)\n",
    "        # core.random.shuffle(combined_gen)\n",
    "        # combined_gen = core.itertools.chain(*combined_gen)\n",
    "        # return combined_gen\n",
    "        # return core.itertools.repeat(combined_gen, 2)\n",
    "        # return core.itertools.cycle(combined_gen)\n",
    "        \n",
    "        # New framework using .concatenate method\n",
    "        print(f'Total constituents: {gen_counter}')\n",
    "        # Zip inputs and labels\n",
    "        gen = core.tf.data.Dataset.zip((inputs, labels))\n",
    "\n",
    "        # Apply batches\n",
    "        gen = gen.batch(batch_size)\n",
    "\n",
    "        # Apply shuffle\n",
    "        gen = gen.shuffle(buffer_size=shuffle_buffer_size)\n",
    "        return gen\n",
    "    elif not generator:\n",
    "        print(f'Total constituents: {constituents_count}')\n",
    "        core.np.random.seed(seed)\n",
    "        rand_index = core.np.random.permutation(len(combined_inputs))\n",
    "        combined_inputs, combined_labels = combined_inputs[rand_index], combined_labels[rand_index]\n",
    "        return combined_inputs, combined_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d128d4-3927-46ea-9e7d-32ce4b264282",
   "metadata": {},
   "source": [
    "#### Run log\n",
    "0. Default state from coremlv2\n",
    "1. Move important functions to randomtry and construct pipeline. **Result**: Run and error after 1st iteration, as expected.\n",
    "2. Add xsmall dataset_size for faster iteration, comment random.seed & random.shuffle to test if shuffling can cause error. **Result**: Stop iteration error.\n",
    "3. Uncomment random.seed & random.shuffle. Use conventional h5py read and then pass it to DataGenerator class if generator. Continue as usual to next section. **Result**: Stop iteration error.\n",
    "4. Try itertools.repeat. **Result**: Attribute error, no shape attribute.\n",
    "5. Try itertools.cycle. **Result**: Infinite loop (never ending 1st epoch)\n",
    "6. Restore to 0 state, try .concatenate method to chaining multiple IODataset. **Result**: SUCCESS!\n",
    "7. Try using full_new_wsd to test if any memory concern occurs. **Result**: SUCCESS!. Fast dataset loading, shuffling, and able to continue to next iteration. No memory problem occurs.\n",
    "8. Try using GPU, 5 epochs. **Result**: Strange result, the process finish so fast although all idx data were used. Turns out that .concatenate returns a modified object. \n",
    "9. Modify `inputs = inputs.concatenate(next_inputs)`. **Result**: SUCCESS! Implemented version.\n",
    "\n",
    "#### Additional info:\n",
    "- full_new_wsd with model 327 only occupy 400mb memory. Shiet! wkwk\n",
    "- next potential problem: how to increase gpu utilization and direct storage read speed.\n",
    "- try different batch_size to see if overall speed is increased\n",
    "    Using 327/full_new_wsd\n",
    "    2048 -> , 2.2gb\n",
    "    512 -> 321ms/step, 536sec, 1.4gb\n",
    "    128 -> 96ms/step, 641sec, 400mb\n",
    "    64 -> 46ms/step, 726sec, 200mb\n",
    "- more batch_size -> same gpu utilization but with higher board power draw & gpu clock (gpu-z)\n",
    "- kt explore 327/34/full_new_wsd_mix terakhir lupa diganti hypermodelnya, masih model_326_kt. Di re-run aja\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58a21cbe-3891-4861-b5cf-c0fff0feb1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total constituents: 422\n",
      "Total constituents: 422\n"
     ]
    }
   ],
   "source": [
    "model_no = '327'\n",
    "kt_iter = 'try9'\n",
    "ticker_group = ['wsd']\n",
    "epochs = 5\n",
    "max_epochs = 5\n",
    "batch_size = 2048\n",
    "\n",
    "dataset_size = 'full_new_wsd'\n",
    "shuffle_buffer_size = 10\n",
    "generator = True\n",
    "\n",
    "train_gen = load_dataset_wsd_traintest(subset='training', dataset_size=dataset_size, ROOT_PATH='''J:\\#PROJECT\\idx''', db_ver='8', batch_size=batch_size, shuffle_buffer_size=shuffle_buffer_size, seed=0, generator=generator, model_no=model_no)\n",
    "\n",
    "validation_gen = load_dataset_wsd_traintest(subset='validation', dataset_size=dataset_size, ROOT_PATH='''J:\\#PROJECT\\idx''', db_ver='8', batch_size=batch_size, shuffle_buffer_size=shuffle_buffer_size, seed=0, generator=generator, model_no=model_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d1653d-f164-4398-8ca7-97fa2bcee7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6 Complete [00h 27m 10s]\n",
      "val_loss: 0.6478671431541443\n",
      "\n",
      "Best val_loss So Far: 0.6478671431541443\n",
      "Total elapsed time: 01h 59m 57s\n",
      "\n",
      "Search: Running Trial #7\n",
      "\n",
      "Hyperparameter    |Value             |Best Value So Far \n",
      "lr                |0.00084556        |0.0027236         \n",
      "r_units_1         |96                |48                \n",
      "r_units_2         |64                |32                \n",
      "d_units_1         |96                |80                \n",
      "d_units_2         |64                |112               \n",
      "tuner/epochs      |5                 |5                 \n",
      "tuner/initial_e...|2                 |2                 \n",
      "tuner/bracket     |1                 |1                 \n",
      "tuner/round       |1                 |1                 \n",
      "tuner/trial_id    |20e7c70ed815b8c...|fd080aa3e84c365...\n",
      "\n",
      "Epoch 3/5\n",
      "    413/Unknown - 362s 845ms/step - loss: 0.6742 - accuracy: 0.5840"
     ]
    }
   ],
   "source": [
    "tuner = kt.Hyperband(hypermodel=core.model_327_kt, objective='val_loss', max_epochs=max_epochs, hyperband_iterations=1, overwrite=True, directory=f'{ROOT_PATH}models/kt/v{kt_iter}/', project_name='_'.join(ticker_group))\n",
    "\n",
    "tuner.search(train_gen, validation_data=validation_gen, epochs=epochs, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4070fd98-9e7d-4492-a178-e60e246f4336",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f6e2bc-d3a7-477e-b5c9-4f54e44d2b37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ae44492-f8f0-4607-a1f0-e8b78ccdc337",
   "metadata": {},
   "source": [
    "#### Hitmat assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "806da0fd-c9bd-43dd-b0ae-6123e65cb045",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8fdd112-39fc-4977-b8b7-4c6f625df3ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'time' has no attribute 'now'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\JONATH~1\\AppData\\Local\\Temp/ipykernel_2552/473153597.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'time' has no attribute 'now'"
     ]
    }
   ],
   "source": [
    "time.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "131f71a7-ca6e-44a5-b10a-f33a4d4ec8dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(datetime.datetime.now().strftime('%H'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16a5c6f6-40eb-4be1-8a6d-6d42a86eb1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hour_truth_map(start, stop):\n",
    "    # Truth mapper generator\n",
    "    start = 20\n",
    "    stop = 4\n",
    "\n",
    "    # start = 2\n",
    "    # stop = 17\n",
    "    assert start != stop, 'Enter different value for start and stop'\n",
    "    direction = stop - start\n",
    "    if direction < 0:\n",
    "        # Over midnight\n",
    "        before_midnight = [start + i for i in range(24 - start)]\n",
    "        after_midnight = [i for i in range(stop)]\n",
    "        truth_map = before_midnight + after_midnight\n",
    "    elif direction > 0:\n",
    "        # Not over midnight\n",
    "        truth_map = [start + i for i in range(stop - start)]\n",
    "    return truth_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d813ca6-b490-4ec5-bc3c-34ab8cc89936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20, 21, 22, 23, 0, 1, 2, 3]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truth_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4486254-9dc8-4bd5-81f0-e49abfbfbca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3,4,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c10d8790-d616-42e8-bae0-6d36b9d74bfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7eca40c7-7b35-4886-9caa-aba884532daf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2022-06-03 00:00:00')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "core.pd.Timestamp(1654214400000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48d85830-48cb-410a-919e-eff21abb1735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 5, 6]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2,3,4,5,6]\n",
    "a[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1568497-05a3-4779-9e90-b9c61a9771fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691660b0-965c-49ff-a7e4-52ac382cfd15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d65730-e154-49fd-99c4-941875225445",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
