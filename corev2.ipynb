{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1411806-9094-4856-af31-87139fe28c36",
   "metadata": {},
   "source": [
    "`env:talib`\n",
    "\n",
    "# All Functions for indicator calculation and machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9a7f311-f84a-4c84-97b4-44379bad38b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import datetime\n",
    "\n",
    "# Indicator Calculation modules\n",
    "import talib\n",
    "import math\n",
    "from sklearn.metrics import r2_score\n",
    "from pathlib import Path\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "# # Machine learning calculation modules\n",
    "# import tensorflow as tf\n",
    "# import re\n",
    "# from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "# import json\n",
    "\n",
    "# # Google colab modules\n",
    "# from google.colab import drive\n",
    "# import sys\n",
    "# drive._mount('/content/gdrive', force_remount=True)\n",
    "# ROOT_PATH = './gdrive/MyDrive/#PROJECT/idx/'\n",
    "# sys.path.append(ROOT_PATH)\n",
    "\n",
    "def stock_list_100_highestrank_and_availability():\n",
    "    '''Return a list of idx stock that have\n",
    "    highest availability and most popular.\n",
    "    Algorithm:\n",
    "    - Get 150 stock with highest data availability\n",
    "    - From stock filtered above, get 100 most popular\n",
    "        stock (by relative idx volume rank)\n",
    "    '''\n",
    "    tickers = ['BBRI', 'BUMI', 'ELTY', 'TLKM', 'BRPT', 'LPKR', 'BKSL', 'BMRI', 'KLBF', 'BTEK', 'ASRI', 'KIJA', 'FREN', 'ANTM', 'ASII', 'ADRO', 'MEDC', 'BHIT', 'PWON', 'PNLF', 'BNII', 'PTBA', 'TINS', 'ELSA', 'MLPL', 'DOID', 'KREN', 'CNKO', 'CTRA', 'META', 'ENRG', 'SMRA', 'MDLN', 'BBNI', 'APIC', 'BMTR', 'LSIP', 'MAPI', 'BSDE', 'INDF', 'BBKP', 'MNCN', 'CPIN', 'WIKA', 'BNBR', 'SSIA', 'BNGA', 'BEKS', 'ADHI', 'RAJA', 'DILD', 'BBCA', 'PBRX', 'DGIK', 'PNBN', 'INDY', 'ACES', 'MYOR', 'INCO', 'AKRA', 'TBLA', 'KPIG', 'GZCO', 'TOTL', 'UNVR', 'INKP', 'SMCB', 'MPPA', 'GJTL', 'INTA', 'JSMR', 'HMSP', 'CMNP', 'MASA', 'SRSN', 'BNLI', 'INAF', 'RALS', 'ADMG', 'UNTR', 'SCMA', 'ISAT', 'DSFI', 'BDMN', 'MTDL', 'SULI', 'TURI', 'SMGR', 'TMAS', 'MAIN', 'KAEF', 'EXCL', 'SMSM', 'LPPS', 'POLY', 'KBLI', 'UNSP', 'PKPK', 'BUDI', 'CSAP']\n",
    "    return tickers\n",
    "\n",
    "def excluded_stock():\n",
    "    '''Stocks that causes error in the process.\n",
    "    '''\n",
    "    tickers = ['HDTX','TRIO','CMPP','NIPS','SUGI','TRIL','APIC']\n",
    "    return tickers\n",
    "\n",
    "def yfinance_db(stock_db='idx_raw.db', if_exists='replace', selected_stock_only=False, ROOT_PATH='./'):\n",
    "    '''Fetch data from yfinance and store\n",
    "    it into defined stock_db.\n",
    "    '''\n",
    "    stocks_db_conn = sqlite3.connect(f'{ROOT_PATH}{stock_db}')\n",
    "    investing_metadata = stock_metadata(ROOT_PATH=ROOT_PATH)\n",
    "    selected_stock = stock_list_100_highestrank_and_availability()\n",
    "    \n",
    "    count=0\n",
    "    excluded_stock = []\n",
    "    for index in range(len(investing_metadata)):\n",
    "        tick = datetime.datetime.now()\n",
    "        ticker = investing_metadata.loc[index]['ticker']\n",
    "        \n",
    "        if ticker not in selected_stock and selected_stock_only:\n",
    "            continue    \n",
    "            \n",
    "        try:\n",
    "            # Fetch and store data\n",
    "            stock_object = yf.Ticker(f'{ticker}.JK')\n",
    "            df = stock_object.history(period='max')\n",
    "            df = df.reset_index()\n",
    "            df['Date'] = df['Date'].apply(lambda x: x.value)\n",
    "            df = df.drop(['Dividends', 'Stock Splits'], axis='columns')\n",
    "            df = df.rename(columns={'Date':'time','Open':'open','High':'high','Low':'low','Close':'close'})\n",
    "            df.to_sql(name=ticker, con=stocks_db_conn, index=False, if_exists=if_exists)\n",
    "        except KeyError:\n",
    "            excluded_stock.append(ticker)\n",
    "        tock = datetime.datetime.now()\n",
    "        print(f'{tock-tick} {count} {ticker}')\n",
    "        count+=1\n",
    "        time.sleep(1.5)\n",
    "    return excluded_stock\n",
    "\n",
    "def stock_metadata(ROOT_PATH='./'):\n",
    "    '''Fetch available (traded)\n",
    "    stock reference.\n",
    "    '''\n",
    "    # Read investing metadata\n",
    "    metadata_conn = sqlite3.connect(f'{ROOT_PATH}investing_data.db', timeout=10)\n",
    "\n",
    "    sql = f'select * from metadata'\n",
    "    investing_metadata = pd.read_sql(sql, metadata_conn)\n",
    "    return investing_metadata\n",
    "      \n",
    "def calculate_stock_volume_contribution(origin_db, target_db, excluded_stock, selected_stock_only=False, ROOT_PATH='./'):\n",
    "    '''Calculate ratio of traded stock volume\n",
    "    for each day compared to IDX total volume\n",
    "    in each day.\n",
    "    \n",
    "    Store calculated ratio in new table 'IDX'\n",
    "    '''\n",
    "    origin_db_conn = sqlite3.connect(origin_db)\n",
    "    target_db_conn = sqlite3.connect(target_db)\n",
    "    \n",
    "    # Delete all df variables that exists\n",
    "    try:\n",
    "        del df\n",
    "    except NameError:\n",
    "        pass\n",
    "    \n",
    "    investing_metadata = stock_metadata(ROOT_PATH=ROOT_PATH)\n",
    "    selected_stock = stock_list_100_highestrank_and_availability()\n",
    "    \n",
    "    for index in range(len(investing_metadata)):\n",
    "        ticker1 = investing_metadata.loc[index]['ticker']\n",
    "        \n",
    "        if (ticker1 not in selected_stock and selected_stock_only) or ticker1 in excluded_stock:\n",
    "            continue\n",
    "\n",
    "        df1 = pd.read_sql(f'select time, Volume from {ticker1}', origin_db_conn)\n",
    "\n",
    "        try:\n",
    "            df = df.merge(df1, how='outer')\n",
    "            df = df.rename(columns={'Volume':ticker1})\n",
    "        except NameError:\n",
    "            df = df1.copy(deep=True)\n",
    "            df = df.rename(columns={'Volume':ticker1})\n",
    "\n",
    "    # Calculate stock volume\n",
    "    df_volume = df.copy(deep=True)\n",
    "    df_volume = df.drop('time', axis='columns')\n",
    "    df_volume = df_volume.fillna(0)\n",
    "    df_volume['IDX'] = df_volume.sum(axis=1)\n",
    "\n",
    "    # df_volume\n",
    "    df['IDX'] = df_volume['IDX']\n",
    "    df = df.sort_values(by='time')\n",
    "\n",
    "    # Calculate the stock volume relative to total volume\n",
    "    for index in range(len(investing_metadata)):\n",
    "        ticker1 = investing_metadata.loc[index]['ticker']\n",
    "        \n",
    "        if (ticker1 not in selected_stock and selected_stock_only) or ticker1 in excluded_stock:\n",
    "            continue\n",
    "        \n",
    "        df[f'{ticker1}_'] = df[ticker1] / df['IDX']\n",
    "\n",
    "    # Drop stock traded volume\n",
    "    df = df.drop([x for x in list(investing_metadata['ticker']) if x not in excluded_stock], axis='columns')\n",
    "    \n",
    "    # Drop row that has zero sum of volumes\n",
    "    df = df[df.IDX != 0.0]\n",
    "\n",
    "    # Save dataframe into database\n",
    "    df.to_sql(name='IDX', con=target_db_conn, index=False, if_exists='replace')\n",
    "        \n",
    "def calculate_EMA(df, column, emas=(3,10,30,200)):\n",
    "    for ema in emas:\n",
    "        df[f'{column}_EMA{ema}'] = df[column].ewm(span=ema, adjust=False).mean()\n",
    "    return df\n",
    "\n",
    "# Old version\n",
    "# calculate gradient from 4 samples\n",
    "def calculate_EMA_gradient(df, column, emas=(3,10,30,200)):\n",
    "    for ema in emas:\n",
    "        multiplier = [1, 1/2, 3/4, 7/8]\n",
    "        samples = [int(m * ema) for m in multiplier]\n",
    "        sample_columns = []\n",
    "        for i, sample in enumerate(samples):\n",
    "            sample_column_name = f'{column}_EMA{ema}_G{i}'\n",
    "            sample_columns.append(sample_column_name)\n",
    "            df[sample_column_name] = (df[f'{column}_EMA{ema}'] - df[f'{column}_EMA{ema}'].shift(sample)) / ema\n",
    "        df[f'{column}_EMA{ema}_G'] = df.loc[:, sample_columns].sum(axis=1) / len(samples)\n",
    "        df = df.drop(sample_columns, axis='columns')\n",
    "    return df\n",
    "\n",
    "# Revised version\n",
    "# Calculate gradient from tn-1 only\n",
    "def calculate_EMA_gradient(df, column, emas=(3,10,30,200)):\n",
    "    for ema in emas:\n",
    "        df[f'{column}_EMA{ema}_G'] = df[f'{column}_EMA{ema}'] - df[f'{column}_EMA{ema}'].shift(1)\n",
    "    return df\n",
    "\n",
    "def calculate_signal_EMA_offset(df, column, signal=3, emas=(10,30,200)):\n",
    "    signal_column = f'{column}_EMA{signal}'\n",
    "    for ema in emas:\n",
    "        source_column = f'{column}_EMA{ema}'\n",
    "        target_column = f'{column}_EMA{signal}_EMA{ema}_offset'\n",
    "        df[target_column] = (df[signal_column] - df[source_column]) / df[source_column]\n",
    "    return df\n",
    "\n",
    "def calculate_candle_score(df, columns=('open','high','low','close','change')):\n",
    "    open_c, high_c, low_c, close_c, change_c = columns\n",
    "    candle_I0, candle_I1, candle_I2, candle_I3, candle_I4, candle_I5 = ('candle_I0', 'candle_I1', 'candle_I2', 'candle_I3', 'candle_I4', 'candle_I5')\n",
    "    candle_S1, candle_S2, candle_S3, candle_S4 = ('candle_S1', 'candle_S2', 'candle_S3', 'candle_S4')\n",
    "    \n",
    "    # Candle body\n",
    "    df[candle_I0] = df[open_c] - df[close_c]\n",
    "    \n",
    "    # Identify red / green candle status\n",
    "    df[candle_I1] = np.select([df[candle_I0] < 0, df[candle_I0] > 0, df[candle_I0].isna()],\n",
    "                              [-1, 1, np.nan], default=0)\n",
    "    \n",
    "    # High-low range, relative to close price\n",
    "    df[candle_I2] = (df[high_c] - df[low_c]) / df[close_c]\n",
    "    \n",
    "    # Absolute relative body length to close price\n",
    "    df[candle_I3] = (df[candle_I0] / df[close_c]).abs()\n",
    "    \n",
    "    # Body length / high-low range ratio\n",
    "    df[candle_I4] = df[candle_I3] / df[candle_I2]\n",
    "    \n",
    "    # Candle body offset relative to high-low mean\n",
    "    df[candle_I5] = ((((df[high_c] - df[low_c]) / 2) + \n",
    "                      ((df[close_c] - df[open_c]) / 2)) / \n",
    "                     ((df[high_c] - df[low_c]) / 2))\n",
    "        \n",
    "    # Score1: product of I1 * sum of I2-I5\n",
    "    df[candle_S1] = df[candle_I1] * (df[candle_I2] + df[candle_I3] + df[candle_I4] + df[candle_I5])\n",
    "    \n",
    "    # Score2: product of I1 * average if I2-I5\n",
    "    df[candle_S2] = df[candle_I1] * (df[candle_I2] + df[candle_I3] + df[candle_I4] + df[candle_I5]) / 4\n",
    "    \n",
    "    # Score3: product of I1-I5\n",
    "    df[candle_S3] = df[candle_I1] * df[candle_I2] * df[candle_I3] * df[candle_I4] * df[candle_I5]\n",
    "    \n",
    "    # Score4: product of I1 * absolute of I2-I5 product\n",
    "    df[candle_S4] = df[candle_I1] * (df[candle_I2] * df[candle_I3] * df[candle_I4] * df[candle_I5]).abs()\n",
    "    \n",
    "    return df\n",
    "    \n",
    "def calculate_favorite_stock(FAVORITE_STOCK, DB_PATH, threshold=20, column_groups=('day', 'month', 'year'), ROOT_PATH='./'):\n",
    "    '''day-month-year favorite\n",
    "    FAVORITE_STOCK: string, path to save the results.\n",
    "    '''\n",
    "    stocks_db_conn = sqlite3.connect(f'{ROOT_PATH}{DB_PATH}')\n",
    "    df_IDX = pd.read_sql('select * from IDX', stocks_db_conn)\n",
    "\n",
    "    # Convert integer timestamp into date\n",
    "    df_IDX['time'] = pd.to_datetime(df_IDX['time'])\n",
    "\n",
    "    # Make new column for year/month/day\n",
    "    df_IDX.loc[:,'year'] = df_IDX['time'].dt.year\n",
    "    df_IDX.loc[:,'month'] = df_IDX['time'].dt.month\n",
    "    df_IDX.loc[:,'day'] = df_IDX['time'].dt.day\n",
    "\n",
    "    # Drop time column to avoid interference to the rank\n",
    "    df_IDX_rank = df_IDX.drop(['time','IDX','day','month','year'], axis='columns')\n",
    "\n",
    "    # Calculate rank\n",
    "    df_IDX_rank = df_IDX_rank.rank(axis=1, ascending=True)\n",
    "\n",
    "    # Normalize rank\n",
    "    df_IDX_rank = df_IDX_rank.apply(lambda x: x / df_IDX_rank.count(axis=1))\n",
    "\n",
    "    all_results = {}\n",
    "    for column_group in column_groups:\n",
    "        groups_result = {}\n",
    "        for group_value in df_IDX[column_group].unique():\n",
    "            tick = datetime.datetime.now()\n",
    "\n",
    "            # Calculate rank for every specified range\n",
    "            df_IDX_filtered = df_IDX.loc[df_IDX[column_group] == group_value]\n",
    "            record_length = len(df_IDX_filtered)\n",
    "            \n",
    "            group_result = {}\n",
    "            weight = np.arange(1, threshold+1)[::-1] / record_length\n",
    "\n",
    "            # Loop through selected index\n",
    "            for index in df_IDX_filtered.index:\n",
    "                top_tickers = df_IDX_rank.iloc[index].sort_values(ascending=False)[:threshold].index\n",
    "\n",
    "\n",
    "                for i, top_ticker in enumerate(top_tickers):\n",
    "                    if top_ticker in group_result:\n",
    "                        group_result[top_ticker] = group_result[top_ticker] + weight[i]\n",
    "                    else:\n",
    "                        group_result[top_ticker] = weight[i]\n",
    "\n",
    "            groups_result[str(group_value)] = group_result\n",
    "\n",
    "            tock = datetime.datetime.now()\n",
    "            print(f'{str(column_group)} - {group_value}')\n",
    "\n",
    "        all_results[column_group] = groups_result\n",
    "        with open(FAVORITE_STOCK, 'w') as f:\n",
    "            json.dump(all_results, f)\n",
    "            \n",
    "def calculate_favorite_stockv2(FAVORITE_STOCK, DB_PATH, threshold=20, main_group='year', sub_group='day', ROOT_PATH='./'):\n",
    "    '''day-year / month-year\n",
    "    FAVORITE_STOCK: string, path to save the results.\n",
    "    '''\n",
    "    stocks_db_conn = sqlite3.connect(f'{ROOT_PATH}{DB_PATH}')\n",
    "    df_IDX = pd.read_sql('select * from IDX', stocks_db_conn)\n",
    "\n",
    "    # Convert integer timestamp into date\n",
    "    df_IDX['time'] = pd.to_datetime(df_IDX['time'])\n",
    "\n",
    "    # Make new column for year/month/day\n",
    "    df_IDX.loc[:,'year'] = df_IDX['time'].dt.year\n",
    "    df_IDX.loc[:,'month'] = df_IDX['time'].dt.month\n",
    "    df_IDX.loc[:,'day'] = df_IDX['time'].dt.day\n",
    "\n",
    "    # Drop time column to avoid interference to the rank\n",
    "    df_IDX_rank = df_IDX.drop(['time','IDX','day','month','year'], axis='columns')\n",
    "\n",
    "    # Calculate rank\n",
    "    df_IDX_rank = df_IDX_rank.rank(axis=1, ascending=True)\n",
    "\n",
    "    # Normalize rank\n",
    "    df_IDX_rank = df_IDX_rank.apply(lambda x: x / df_IDX_rank.count(axis=1))\n",
    "\n",
    "    groups_result = {}\n",
    "    for main_value in df_IDX[main_group].unique():\n",
    "        for sub_value in df_IDX[sub_group].unique():\n",
    "            tick = datetime.datetime.now()\n",
    "\n",
    "            # Calculate rank for every specified range\n",
    "            df_IDX_filtered = df_IDX.loc[(df_IDX[main_group] == main_value) & (df_IDX[sub_group] == sub_value)]\n",
    "            record_length = len(df_IDX_filtered)\n",
    "\n",
    "            group_result = {}\n",
    "            weight = np.arange(1, threshold+1)[::-1] / record_length\n",
    "\n",
    "            # Loop through selected index\n",
    "            for index in df_IDX_filtered.index:\n",
    "                top_tickers = df_IDX_rank.iloc[index].sort_values(ascending=False)[:threshold].index\n",
    "\n",
    "                for i, top_ticker in enumerate(top_tickers):\n",
    "                    if top_ticker in group_result:\n",
    "                        group_result[top_ticker] = group_result[top_ticker] + weight[i]\n",
    "                    else:\n",
    "                        group_result[top_ticker] = weight[i]\n",
    "\n",
    "            groups_result[f\"{main_value}_{f'0{sub_value}' if sub_value < 10 else f'{sub_value}'}\"] = group_result\n",
    "\n",
    "            tock = datetime.datetime.now()\n",
    "            print(f'{main_value} - {sub_value}')\n",
    "\n",
    "            with open(FAVORITE_STOCK, 'w') as f:\n",
    "                json.dump(groups_result, f)\n",
    "                \n",
    "def calculate_horizontal_support_resistance(df, ticker, indicators):\n",
    "    '''\n",
    "    ticker: string\n",
    "    indicator: dict\n",
    "    '''\n",
    "#     dfhs = pd.read_sql(f'select * from {ticker}', stocks_db_conn)\n",
    "    dfhs = df.copy(deep=True)\n",
    "    \n",
    "    sr_lines = {}\n",
    "    # Calculate horizontal support/resistance lines\n",
    "    for indicator in indicators:\n",
    "        # Stack multiple column to single column\n",
    "        dfhs_sliced = dfhs[indicators[indicator]]\n",
    "        try:\n",
    "            bins = int(len(dfhs_sliced.stack()) / 10)\n",
    "\n",
    "            # Calculate histogram intervals\n",
    "            x = pd.cut(dfhs_sliced.stack(), bins).value_counts()\n",
    "        except ValueError:\n",
    "            bins = len(dfhs_sliced.stack())\n",
    "\n",
    "            # Calculate histogram intervals\n",
    "            x = pd.cut(dfhs_sliced.stack(), bins).value_counts()\n",
    "\n",
    "        # Calculate middle value of interval and store to new df\n",
    "        mid_values = []\n",
    "        for mid in x.index:\n",
    "            mid_values.append((mid.mid, x[mid]))\n",
    "        hist_pd = pd.DataFrame(mid_values)\n",
    "\n",
    "        # Filter horizontal s/r lines that has less than mean frequency\n",
    "        sr_line_df = hist_pd.loc[hist_pd[1] <= hist_pd[1].mean()].sort_values(by=0)\n",
    "        sr_line_df = sr_line_df.reset_index(drop=True)\n",
    "\n",
    "        # Get 10 s/r lines with same range between them\n",
    "        sr_thresholds = np.linspace(0,1,10)\n",
    "        calculated_sr = []\n",
    "        for sr_threshold in sr_thresholds:\n",
    "            calculated_sr.append(sr_line_df.iloc[int((len(sr_line_df) - 1) * sr_threshold)][0])\n",
    "        sr_lines[indicator] = calculated_sr\n",
    "    return sr_lines\n",
    "\n",
    "def __comp__calculate_stock_change_ratio(df, shift=1):\n",
    "    '''Calculate stock change in %.\n",
    "    Default shift value is 1, meaning that\n",
    "    it's calculating daily stock price change.    \n",
    "    '''\n",
    "    df['change'] = (df['close'] - df['close'].shift(shift)) / df['close'].shift(shift)\n",
    "    return df\n",
    "\n",
    "def __comp__calculate_stock_volume_rank(df, df_IDX_rank, ticker):\n",
    "    '''Calculate stock volume rank from `IDX` table\n",
    "    and insert it into individual stock table\n",
    "    '''\n",
    "    df = df.merge(df_IDX_rank[['time', f'{ticker}_']], how='inner')\n",
    "    df = df.rename(columns={f'{ticker}_':'Volume_rank'})\n",
    "    return df\n",
    "\n",
    "def __comp__calculate_stock_indicator(df):\n",
    "    source_columns = ['close', 'rsi14', 'Volume', 'Volume_rank', 'change']\n",
    "    df['rsi14'] = talib.RSI(df['close'], timeperiod=14)\n",
    "    df = calculate_candle_score(df)\n",
    "    for source_column in source_columns:\n",
    "        df = calculate_EMA(df, source_column)\n",
    "        df = calculate_EMA_gradient(df, source_column)\n",
    "        df = calculate_signal_EMA_offset(df, source_column)\n",
    "    return df\n",
    "\n",
    "def __comp__calculate_oscillation_between_sr(df, origin_db_conn, ticker):\n",
    "    indicators = {'close':['open','high','low','close'],'rsi14':['rsi14'],'Volume':['Volume'],'Volume_rank':['Volume_rank'],'change':['change']}\n",
    "    sr_lines = calculate_horizontal_support_resistance(df, ticker, indicators)\n",
    "\n",
    "    # Calculate indicator progress between interval\n",
    "    for indicator in indicators:\n",
    "        # Define condition and choice list\n",
    "        condlist = [df[indicator] <= sr for sr in sr_lines[indicator][1:]]\n",
    "        choicelist_t = [sr for sr in sr_lines[indicator][1:]]\n",
    "        choicelist_b = [sr for sr in sr_lines[indicator][:len(sr_lines[indicator]) - 1]]\n",
    "        df[f'{indicator}_b'] = np.select(condlist, choicelist_b, default=choicelist_b[-1])\n",
    "        df[f'{indicator}_t'] = np.select(condlist, choicelist_t, default=choicelist_t[-1])\n",
    "\n",
    "        # Calculate progress between interval.\n",
    "        # *basically, just min/max norm between bottom/top interval\n",
    "        df[f'{indicator}_srp'] = (df[indicator] - df[f'{indicator}_b']) / (df[f'{indicator}_t'] - df[f'{indicator}_b'])\n",
    "        df = df.drop([f'{indicator}_b',f'{indicator}_t'], axis='columns')\n",
    "\n",
    "    # Add close price relative to all time low / high\n",
    "    df['close_rel'] = (df['close'] - df['close'].min()) / df['close'].max()\n",
    "    return df\n",
    "\n",
    "def __comp__calculate_cumulative_change(df):\n",
    "    '''Calculate price change f_shift ahead from\n",
    "    previous day.\n",
    "    '''\n",
    "    f_shifts=(3,5,7,10)\n",
    "    b_shifts = [1 for _ in range(len(f_shifts))]\n",
    "    for i, f_shift in enumerate(f_shifts):\n",
    "        b_shift = b_shifts[i]\n",
    "        df[f'change_b{b_shift}f{f_shift}'] = (df['close'].shift(-f_shift) - df['close'].shift(b_shift)) / df['close'].shift(b_shift)\n",
    "    return df\n",
    "\n",
    "def __comp__calculate_forecast_column(df):\n",
    "    '''Calculate close_EMA3_G and close_EMA10_G\n",
    "    at +1 and +2 forecast\n",
    "    '''\n",
    "    columns_to_forecast = ('close_EMA3_G','close_EMA10_G')\n",
    "    forecast_lengths = (1,2)\n",
    "    for column_to_forecast in columns_to_forecast:\n",
    "        for forecast_length in forecast_lengths:\n",
    "            df[f'{column_to_forecast}_s{forecast_length}'] = df[column_to_forecast].shift(-forecast_length)\n",
    "    return df\n",
    "\n",
    "def _calculate_stock_volume_rank(target_db):\n",
    "    '''Component of calculate_stock_volume_rank    \n",
    "    '''\n",
    "    # Fetch portion to IHSG\n",
    "    stocks_db_conn = sqlite3.connect(target_db)\n",
    "    df_IDX = pd.read_sql(f'select * from IDX', stocks_db_conn)\n",
    "\n",
    "    # Drop time column to avoid interference to the rank\n",
    "    df_IDX_rank = df_IDX.drop(['time','IDX'], axis='columns')\n",
    "\n",
    "    # Calculate rank\n",
    "    df_IDX_rank = df_IDX_rank.rank(axis=1, ascending=True)\n",
    "\n",
    "    # Normalize rank\n",
    "    df_IDX_rank = df_IDX_rank.apply(lambda x: x / df_IDX_rank.count(axis=1))\n",
    "\n",
    "    # Bring back time column\n",
    "    df_IDX_rank.insert(0, 'time', df_IDX['time'])\n",
    "    return df_IDX_rank\n",
    "\n",
    "def calculate_all_indicator(origin_db, target_db, excluded_stock, verbose=1, selected_stock_only=False, ROOT_PATH='./'):\n",
    "    origin_db_conn = sqlite3.connect(origin_db)\n",
    "    target_db_conn = sqlite3.connect(target_db)\n",
    "    investing_metadata = stock_metadata(ROOT_PATH=ROOT_PATH)\n",
    "    count = 0\n",
    "    \n",
    "    # calculate_stock_volume_contribution\n",
    "    # Calculate ratio of traded stock volume for\n",
    "    # each day compared to IDX total volume each day\n",
    "    calculate_stock_volume_contribution(origin_db, target_db, excluded_stock, selected_stock_only=selected_stock_only, ROOT_PATH=ROOT_PATH)\n",
    "    \n",
    "    # Fetch calculated IDX rank data frame\n",
    "    df_IDX_rank = _calculate_stock_volume_rank(target_db)\n",
    "    \n",
    "    selected_stock = stock_list_100_highestrank_and_availability()\n",
    "    \n",
    "    for index in range(len(investing_metadata)):\n",
    "        ticker = investing_metadata.loc[index]['ticker']\n",
    "        \n",
    "        if (ticker not in selected_stock and selected_stock_only) or ticker in excluded_stock:\n",
    "            continue    \n",
    "        \n",
    "        # Read origin stock data\n",
    "        df = pd.read_sql(f'select * from {ticker}', origin_db_conn)\n",
    "        \n",
    "        # Some indicator calculation #\n",
    "        # calculate_stock_change_ratio\n",
    "        df = __comp__calculate_stock_change_ratio(df, shift=1)\n",
    "        \n",
    "        # calculate_stock_volume_rank\n",
    "        # Merge volume rank into individual stock df\n",
    "        df = __comp__calculate_stock_volume_rank(df, df_IDX_rank, ticker)\n",
    "        \n",
    "        # calculate_stock_indicator\n",
    "        df = __comp__calculate_stock_indicator(df)\n",
    "            \n",
    "        # calculate_oscillation_between_sr\n",
    "        df = __comp__calculate_oscillation_between_sr(df, origin_db_conn, ticker)\n",
    "        \n",
    "        # calculate_cumulative_change\n",
    "        df = __comp__calculate_cumulative_change(df)\n",
    "            \n",
    "        # calculate_forecast_column\n",
    "        df = __comp__calculate_forecast_column(df)     \n",
    "        ##############################\n",
    "        \n",
    "        # Write back into table\n",
    "        df.to_sql(name=ticker, con=target_db_conn, index=False, if_exists='replace')\n",
    "        \n",
    "        count+=1\n",
    "        if verbose and count%50 == 0:\n",
    "            print(f'Current progress: {count}/{len(investing_metadata)}')\n",
    "            \n",
    "# Additional indicators\n",
    "# Excluded indicators (unclear variable definition)\n",
    "# MAVP\n",
    "# Failed with error code2: Bad Parameter (TA_BAD_PARAM): MAMA (f'{target_columns}_mama', f'{target_columns}_fama')\n",
    "\n",
    "def _universal_indicators(df, target_columns, source_columns):\n",
    "    close = df[target_columns]\n",
    "    add_source_columns = [f'{target_columns}_bbands_upper', f'{target_columns}_bbands_middle', f'{target_columns}_bbands_lower', f'{target_columns}_dema', f'{target_columns}_ema', f'{target_columns}_ht_trendline', f'{target_columns}_kama', f'{target_columns}_real', f'{target_columns}_midpoint', f'{target_columns}_sma', f'{target_columns}_t3', f'{target_columns}_tema', f'{target_columns}_trima', f'{target_columns}_wma', f'{target_columns}_apo', f'{target_columns}_cmo', f'{target_columns}_macd', f'{target_columns}_macdsignal', f'{target_columns}_macdhist', f'{target_columns}_macd_ext', f'{target_columns}_macdsignal_ext', f'{target_columns}_macdhist_ext', f'{target_columns}_macd_fix', f'{target_columns}_macdsignal_fix', f'{target_columns}_macdhist_fix', f'{target_columns}_mom', f'{target_columns}_ppo', f'{target_columns}_roc', f'{target_columns}_rocp', f'{target_columns}_rocr', f'{target_columns}_rocr100', f'{target_columns}_rsi', f'{target_columns}_stochrsi_fastk', f'{target_columns}_stochrsi_fastd', f'{target_columns}_trix', f'{target_columns}_ht_dcperiod', f'{target_columns}_ht_dcphase', f'{target_columns}_ht_phasor_inphase', f'{target_columns}_ht_phasor_quadrature', f'{target_columns}_ht_sine_sine', f'{target_columns}_ht_sine_leadsine', f'{target_columns}_ht_trendmode']\n",
    "    for i in add_source_columns: source_columns.append(i)\n",
    "    # Overlap Studies Functions\n",
    "    df[f'{target_columns}_bbands_upper'], df[f'{target_columns}_bbands_middle'], df[f'{target_columns}_bbands_lower'] = talib.BBANDS(close, timeperiod=5, nbdevup=2, nbdevdn=2, matype=0)\n",
    "    df[f'{target_columns}_dema'] = talib.DEMA(close, timeperiod=30)\n",
    "    df[f'{target_columns}_ema'] = talib.EMA(close, timeperiod=30)\n",
    "    df[f'{target_columns}_ht_trendline'] = talib.HT_TRENDLINE(close)\n",
    "    df[f'{target_columns}_kama'] = talib.KAMA(close, timeperiod=30)\n",
    "    df[f'{target_columns}_real'] = talib.MA(close, timeperiod=30, matype=0)\n",
    "    # df[f'{target_columns}_mama'], df[f'{target_columns}_fama'] = talib.MAMA(close, fastlimit=0, slowlimit=0)\n",
    "    df[f'{target_columns}_midpoint'] = talib.MIDPOINT(close, timeperiod=14)\n",
    "    df[f'{target_columns}_sma'] = talib.SMA(close, timeperiod=30)\n",
    "    df[f'{target_columns}_t3'] = talib.T3(close, timeperiod=5, vfactor=0)\n",
    "    df[f'{target_columns}_tema'] = talib.TEMA(close, timeperiod=30)\n",
    "    df[f'{target_columns}_trima'] = talib.TRIMA(close, timeperiod=30)\n",
    "    df[f'{target_columns}_wma'] = talib.WMA(close, timeperiod=30)\n",
    "    \n",
    "    # Momentum indicator functions\n",
    "    df[f'{target_columns}_apo'] = talib.APO(close, fastperiod=12, slowperiod=26, matype=0)\n",
    "    df[f'{target_columns}_cmo'] = talib.CMO(close, timeperiod=14)\n",
    "    df[f'{target_columns}_macd'], df[f'{target_columns}_macdsignal'], df[f'{target_columns}_macdhist'] = talib.MACD(close, fastperiod=12, slowperiod=26, signalperiod=9)\n",
    "    df[f'{target_columns}_macd_ext'], df[f'{target_columns}_macdsignal_ext'], df[f'{target_columns}_macdhist_ext'] = talib.MACDEXT(close, fastperiod=12, fastmatype=0, slowperiod=26, slowmatype=0, signalperiod=9, signalmatype=0)\n",
    "    df[f'{target_columns}_macd_fix'], df[f'{target_columns}_macdsignal_fix'], df[f'{target_columns}_macdhist_fix'] = talib.MACDFIX(close, signalperiod=9)\n",
    "    df[f'{target_columns}_mom'] = talib.MOM(close, timeperiod=10)\n",
    "    df[f'{target_columns}_ppo'] = talib.PPO(close, fastperiod=12, slowperiod=26, matype=0)\n",
    "    df[f'{target_columns}_roc'] = talib.ROC(close, timeperiod=10)\n",
    "    df[f'{target_columns}_rocp'] = talib.ROCP(close, timeperiod=10)\n",
    "    df[f'{target_columns}_rocr'] = talib.ROCR(close, timeperiod=10)\n",
    "    df[f'{target_columns}_rocr100'] = talib.ROCR100(close, timeperiod=10)\n",
    "    df[f'{target_columns}_rsi'] = talib.RSI(close, timeperiod=14)\n",
    "    df[f'{target_columns}_stochrsi_fastk'], df[f'{target_columns}_stochrsi_fastd'] = talib.STOCHRSI(close, timeperiod=14, fastk_period=5, fastd_period=3, fastd_matype=0)\n",
    "    df[f'{target_columns}_trix'] = talib.TRIX(close, timeperiod=30)\n",
    "    \n",
    "    # Cycle indicator functions\n",
    "    df[f'{target_columns}_ht_dcperiod'] = talib.HT_DCPERIOD(close)\n",
    "    df[f'{target_columns}_ht_dcphase'] = talib.HT_DCPHASE(close)\n",
    "    df[f'{target_columns}_ht_phasor_inphase'], df[f'{target_columns}_ht_phasor_quadrature'] = talib.HT_PHASOR(close)\n",
    "    df[f'{target_columns}_ht_sine_sine'], df[f'{target_columns}_ht_sine_leadsine'] = talib.HT_SINE(close)\n",
    "    df[f'{target_columns}_ht_trendmode'] = talib.HT_TRENDMODE(close)\n",
    "    return df, source_columns\n",
    "\n",
    "def _price_indicators(df, source_columns):\n",
    "    open = df['open']\n",
    "    high = df['high']\n",
    "    low = df['low']\n",
    "    close = df['close']\n",
    "    volume = df['Volume']\n",
    "    add_source_columns = ['midprice', 'sar', 'sarext', 'adx', 'adxr', 'aroondown', 'aroonup', 'aroonosc', 'bop', 'cci', 'dx', 'mfi', 'minus_di', 'minus_dm', 'plus_di', 'plus_dm', 'stoch_slowk', 'stoch_slowd', 'stochf_fastk', 'stochf_fastd', 'ultosc', 'willr', 'ad', 'adosc', 'obv', 'avgprice', 'medprice', 'typprice', 'wclprice', 'atr', 'natr', 'trange']\n",
    "    for i in add_source_columns: source_columns.append(i)\n",
    "    # Overlap Studies Functions\n",
    "    df['midprice'] = talib.MIDPRICE(high, low, timeperiod=14)\n",
    "    df['sar'] = talib.SAR(high, low, acceleration=0, maximum=0)\n",
    "    df['sarext'] = talib.SAREXT(high, low, startvalue=0, offsetonreverse=0, accelerationinitlong=0, accelerationlong=0, accelerationmaxlong=0, accelerationinitshort=0, accelerationshort=0, accelerationmaxshort=0)\n",
    "    \n",
    "    # Momentum indicator functions\n",
    "    df['adx'] = talib.ADX(high, low, close, timeperiod=14)\n",
    "    df['adxr'] = talib.ADXR(high, low, close, timeperiod=14)\n",
    "    df['aroondown'], df['aroonup'] = talib.AROON(high, low, timeperiod=14)\n",
    "    df['aroonosc'] = talib.AROONOSC(high, low, timeperiod=14)\n",
    "    df['bop'] = talib.BOP(open, high, low, close)\n",
    "    df['cci'] = talib.CCI(high, low, close, timeperiod=14)\n",
    "    df['dx'] = talib.DX(high, low, close, timeperiod=14)\n",
    "    df['mfi'] = talib.MFI(high, low, close, volume, timeperiod=14)\n",
    "    df['minus_di'] = talib.MINUS_DI(high, low, close, timeperiod=14)\n",
    "    df['minus_dm'] = talib.MINUS_DM(high, low, timeperiod=14)\n",
    "    df['plus_di'] = talib.PLUS_DI(high, low, close, timeperiod=14)\n",
    "    df['plus_dm'] = talib.PLUS_DM(high, low, timeperiod=14)\n",
    "    df['stoch_slowk'], df['stoch_slowd'] = talib.STOCH(high, low, close, fastk_period=5, slowk_period=3, slowk_matype=0, slowd_period=3, slowd_matype=0)\n",
    "    df['stochf_fastk'], df['stochf_fastd'] = talib.STOCHF(high, low, close, fastk_period=5, fastd_period=3, fastd_matype=0)\n",
    "    df['ultosc'] = talib.ULTOSC(high, low, close, timeperiod1=7, timeperiod2=14, timeperiod3=28)\n",
    "    df['willr'] = talib.WILLR(high, low, close, timeperiod=14)\n",
    "    \n",
    "    # Volume indicator functions\n",
    "    df['ad'] = talib.AD(high, low, close, volume)\n",
    "    df['adosc'] = talib.ADOSC(high, low, close, volume, fastperiod=3, slowperiod=10)\n",
    "    df['obv'] = talib.OBV(close, volume)\n",
    "    \n",
    "    # Price transform functions\n",
    "    df['avgprice'] = talib.AVGPRICE(open, high, low, close)\n",
    "    df['medprice'] = talib.MEDPRICE(high, low)\n",
    "    df['typprice'] = talib.TYPPRICE(high, low, close)\n",
    "    df['wclprice'] = talib.WCLPRICE(high, low, close)\n",
    "    \n",
    "    # Volatility indicator functions\n",
    "    df['atr'] = talib.ATR(high, low, close, timeperiod=14)\n",
    "    df['natr'] = talib.NATR(high, low, close, timeperiod=14)\n",
    "    df['trange'] = talib.TRANGE(high, low, close)\n",
    "    return df, source_columns\n",
    "\n",
    "# Pattern recognition functions\n",
    "def _pattern_recognition(df, source_columns):\n",
    "    open = df['open']\n",
    "    high = df['high']\n",
    "    low = df['low']\n",
    "    close = df['close']\n",
    "    add_source_columns = ['cdl2crows', 'cdl3blackrows', 'cdl3inside', 'cdl3linestrike', 'cdl3outside', 'cdl3starsinsouth', 'cdl3whitesoldiers', 'cdlabandonedbaby', 'cdladvanceblock', 'cdlbelthold', 'cdlbreakaway', 'cdlclosingmarubozu', 'cdlconcealbabyswall', 'cdlcounterattack', 'cdldarkcloudcover', 'cdldoji', 'cdldojistar', 'cdldragonflydoji', 'cdlengulfing', 'cdleveningdojistar', 'cdleveningstar', 'cdlgapinsidewhite', 'cdlgravestonedoji', 'cdlhammer', 'cdlhangingman', 'cdlharami', 'cdlharamicross', 'cdlhighwave', 'cdlhikkake', 'cdlhikkakemod', 'cdlhomingpigeon', 'cdlidentical3crows', 'cdlinneck', 'cdlinvertedhammer', 'cdlkicking', 'cdlkickingbylength', 'cdlladderbottom', 'cdllongleggeddoji', 'cdllongline', 'cdlmarubozu', 'cdlmatchinglow', 'cdlmathold', 'cdlmorningdojistar', 'cdlmorningstar', 'cdlonneck', 'cdlpiercing', 'cdlrickshawman', 'cdlrisefall3methods', 'cdlseparatinglines', 'cdlshootingstar', 'cdlshortline', 'cdlspinningtop', 'cdlstalledpattern', 'cdlsticksandwich', 'cdltakuri', 'cdltasukigap', 'cdlthrusting', 'cdltristar', 'cdlunique3river', 'cdlupsidegap2crows', 'cdlxsidegap3methods']\n",
    "    for i in add_source_columns: source_columns.append(i)\n",
    "    df['cdl2crows'] = talib.CDL2CROWS(open, high, low, close)\n",
    "    df['cdl3blackrows'] = talib.CDL3BLACKCROWS(open, high, low, close)\n",
    "    df['cdl3inside'] = talib.CDL3INSIDE(open, high, low, close)\n",
    "    df['cdl3linestrike'] = talib.CDL3LINESTRIKE(open, high, low, close)\n",
    "    df['cdl3outside'] = talib.CDL3OUTSIDE(open, high, low, close)\n",
    "    df['cdl3starsinsouth'] = talib.CDL3STARSINSOUTH(open, high, low, close)\n",
    "    df['cdl3whitesoldiers'] = talib.CDL3WHITESOLDIERS(open, high, low, close)\n",
    "    df['cdlabandonedbaby'] = talib.CDLABANDONEDBABY(open, high, low, close, penetration=0)\n",
    "    df['cdladvanceblock'] = talib.CDLADVANCEBLOCK(open, high, low, close)\n",
    "    df['cdlbelthold'] = talib.CDLBELTHOLD(open, high, low, close)\n",
    "    df['cdlbreakaway'] = talib.CDLBREAKAWAY(open, high, low, close)\n",
    "    df['cdlclosingmarubozu'] = talib.CDLCLOSINGMARUBOZU(open, high, low, close)\n",
    "    df['cdlconcealbabyswall'] = talib.CDLCONCEALBABYSWALL(open, high, low, close)\n",
    "    df['cdlcounterattack'] = talib.CDLCOUNTERATTACK(open, high, low, close)\n",
    "    df['cdldarkcloudcover'] = talib.CDLDARKCLOUDCOVER(open, high, low, close, penetration=0)\n",
    "    df['cdldoji'] = talib.CDLDOJI(open, high, low, close)\n",
    "    df['cdldojistar'] = talib.CDLDOJISTAR(open, high, low, close)\n",
    "    df['cdldragonflydoji'] = talib.CDLDRAGONFLYDOJI(open, high, low, close)\n",
    "    df['cdlengulfing'] = talib.CDLENGULFING(open, high, low, close)\n",
    "    df['cdleveningdojistar'] = talib.CDLEVENINGDOJISTAR(open, high, low, close, penetration=0)\n",
    "    df['cdleveningstar'] = talib.CDLEVENINGSTAR(open, high, low, close, penetration=0)\n",
    "    df['cdlgapinsidewhite'] = talib.CDLGAPSIDESIDEWHITE(open, high, low, close)\n",
    "    df['cdlgravestonedoji'] = talib.CDLGRAVESTONEDOJI(open, high, low, close)\n",
    "    df['cdlhammer'] = talib.CDLHAMMER(open, high, low, close)\n",
    "    df['cdlhangingman'] = talib.CDLHANGINGMAN(open, high, low, close)\n",
    "    df['cdlharami'] = talib.CDLHARAMI(open, high, low, close)\n",
    "    df['cdlharamicross'] = talib.CDLHARAMICROSS(open, high, low, close)\n",
    "    df['cdlhighwave'] = talib.CDLHIGHWAVE(open, high, low, close)\n",
    "    df['cdlhikkake'] = talib.CDLHIKKAKE(open, high, low, close)\n",
    "    df['cdlhikkakemod'] = talib.CDLHIKKAKEMOD(open, high, low, close)\n",
    "    df['cdlhomingpigeon'] = talib.CDLHOMINGPIGEON(open, high, low, close)\n",
    "    df['cdlidentical3crows'] = talib.CDLIDENTICAL3CROWS(open, high, low, close)\n",
    "    df['cdlinneck'] = talib.CDLINNECK(open, high, low, close)\n",
    "    df['cdlinvertedhammer'] = talib.CDLINVERTEDHAMMER(open, high, low, close)\n",
    "    df['cdlkicking'] = talib.CDLKICKING(open, high, low, close)\n",
    "    df['cdlkickingbylength'] = talib.CDLKICKINGBYLENGTH(open, high, low, close)\n",
    "    df['cdlladderbottom'] = talib.CDLLADDERBOTTOM(open, high, low, close)\n",
    "    df['cdllongleggeddoji'] = talib.CDLLONGLEGGEDDOJI(open, high, low, close)\n",
    "    df['cdllongline'] = talib.CDLLONGLINE(open, high, low, close)\n",
    "    df['cdlmarubozu'] = talib.CDLMARUBOZU(open, high, low, close)\n",
    "    df['cdlmatchinglow'] = talib.CDLMATCHINGLOW(open, high, low, close)\n",
    "    df['cdlmathold'] = talib.CDLMATHOLD(open, high, low, close, penetration=0)\n",
    "    df['cdlmorningdojistar'] = talib.CDLMORNINGDOJISTAR(open, high, low, close, penetration=0)\n",
    "    df['cdlmorningstar'] = talib.CDLMORNINGSTAR(open, high, low, close, penetration=0)\n",
    "    df['cdlonneck'] = talib.CDLONNECK(open, high, low, close)\n",
    "    df['cdlpiercing'] = talib.CDLPIERCING(open, high, low, close)\n",
    "    df['cdlrickshawman'] = talib.CDLRICKSHAWMAN(open, high, low, close)\n",
    "    df['cdlrisefall3methods'] = talib.CDLRISEFALL3METHODS(open, high, low, close)\n",
    "    df['cdlseparatinglines'] = talib.CDLSEPARATINGLINES(open, high, low, close)\n",
    "    df['cdlshootingstar'] = talib.CDLSHOOTINGSTAR(open, high, low, close)\n",
    "    df['cdlshortline'] = talib.CDLSHORTLINE(open, high, low, close)\n",
    "    df['cdlspinningtop'] = talib.CDLSPINNINGTOP(open, high, low, close)\n",
    "    df['cdlstalledpattern'] = talib.CDLSTALLEDPATTERN(open, high, low, close)\n",
    "    df['cdlsticksandwich'] = talib.CDLSTICKSANDWICH(open, high, low, close)\n",
    "    df['cdltakuri'] = talib.CDLTAKURI(open, high, low, close)\n",
    "    df['cdltasukigap'] = talib.CDLTASUKIGAP(open, high, low, close)\n",
    "    df['cdlthrusting'] = talib.CDLTHRUSTING(open, high, low, close)\n",
    "    df['cdltristar'] = talib.CDLTRISTAR(open, high, low, close)\n",
    "    df['cdlunique3river'] = talib.CDLUNIQUE3RIVER(open, high, low, close)\n",
    "    df['cdlupsidegap2crows'] = talib.CDLUPSIDEGAP2CROWS(open, high, low, close)\n",
    "    df['cdlxsidegap3methods'] = talib.CDLXSIDEGAP3METHODS(open, high, low, close)\n",
    "    return df, source_columns\n",
    "\n",
    "def _indicator_derivatives(df, source_columns):\n",
    "    # Calculate EMA, their gradient, and signal offset for every indicator\n",
    "    for source_column in source_columns:\n",
    "        df = calculate_EMA(df, source_column)\n",
    "        df = calculate_EMA_gradient(df, source_column)\n",
    "        df = calculate_signal_EMA_offset(df, source_column)\n",
    "    return df\n",
    "\n",
    "def calculate_talib_indicators_primary(df):\n",
    "    '''Without gradient and their derivatives\n",
    "    without candle pattern\n",
    "    '''\n",
    "    source_columns = []\n",
    "    # Calculate universal_indicators for every target columns\n",
    "    target_columns = ['open','high','low','close','Volume','change','Volume_rank']\n",
    "\n",
    "    for target_column in target_columns:\n",
    "        df, source_columns = _universal_indicators(df, target_column, source_columns)\n",
    "    # Calculate price_indicators for price\n",
    "    df, source_columns = _price_indicators(df, source_columns)\n",
    "    return df, source_columns\n",
    "\n",
    "def calculate_talib_indicators(df):\n",
    "    source_columns = []\n",
    "    # Calculate universal_indicators for every target columns\n",
    "    target_columns = ['open','high','low','close','Volume','change','Volume_rank']\n",
    "\n",
    "    for target_column in target_columns:\n",
    "        df, source_columns = _universal_indicators(df, target_column, source_columns)\n",
    "    # Calculate price_indicators for price\n",
    "    df, source_columns = _price_indicators(df, source_columns)\n",
    "    # Recognize candle pattern\n",
    "    df, source_columns = _pattern_recognition(df, source_columns)\n",
    "\n",
    "    # Calculate EMA, their gradient, and signal offset for every indicator\n",
    "    df = _indicator_derivatives(df, source_columns)\n",
    "    return df\n",
    "\n",
    "def store_splitdf(df, primary_index, conn, max_column=500):\n",
    "    column_length = len(df.columns)\n",
    "    splits = math.ceil(column_length / max_column)\n",
    "    for split in range(splits):\n",
    "        columns = df.columns[max_column * split:max_column * (1 + split)]\n",
    "        columns = np.insert(columns, 0, primary_index) if primary_index not in columns else columns\n",
    "        temp_df = df[columns]\n",
    "        temp_df.to_sql(str(split), conn, if_exists='replace', index=False)\n",
    "    \n",
    "def tablename_list(conn):\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(f\"select name from sqlite_master where type='table'\")\n",
    "    return [x[0] for x in cursor.fetchall()]\n",
    "\n",
    "def restore_splitdf(conn):\n",
    "    table_names = tablename_list(conn)\n",
    "    first = True\n",
    "    for table_name in table_names:\n",
    "        temp_df = pd.read_sql(f'select * from `{table_name}`', conn)\n",
    "        if first:\n",
    "            df = temp_df.copy(deep=True)\n",
    "            first = False\n",
    "        elif not first:\n",
    "            df = df.merge(temp_df)\n",
    "    return df\n",
    "\n",
    "def clean_dataset(df):\n",
    "    assert isinstance(df, pd.DataFrame), 'df needs to be a pd.DataFrame'\n",
    "    df.dropna(inplace=True)\n",
    "    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(1)\n",
    "    return df[indices_to_keep]\n",
    "\n",
    "def calculate_indicator_correlation(df):\n",
    "    primary_index = 'time'\n",
    "    # Standard score normalization\n",
    "    df = (df - df.mean()) / df.std()\n",
    "\n",
    "    columns = df.columns\n",
    "    results = {}\n",
    "    combinations = []\n",
    "    count = 0\n",
    "    tick = datetime.datetime.now()\n",
    "    for i in columns:\n",
    "        for j in columns:\n",
    "            combination = [i,j]\n",
    "            if (i == primary_index) or (j == primary_index) or (i == j) or (combination in combinations) or (combination[::-1] in combinations):\n",
    "                continue\n",
    "            combinations.append(combination)\n",
    "\n",
    "            slice_df = clean_dataset(df[[i,j]])\n",
    "\n",
    "            try:\n",
    "                results[f'{i},{j}'] = r2_score(slice_df[i], slice_df[j])\n",
    "            except ValueError:\n",
    "                continue\n",
    "        count+=1\n",
    "    return results\n",
    "\n",
    "def uncorrelated_indicators(results, minimum=20, maximum=30, interval=0.001, increment=0.00005):\n",
    "    rdf = pd.DataFrame({'keys':results.keys(), 'values':results.values()})\n",
    "    len_unique_columns = 0\n",
    "    while (len_unique_columns < minimum) or (len_unique_columns > maximum):\n",
    "        # Conditions\n",
    "        if len_unique_columns < minimum:\n",
    "            interval+=increment\n",
    "        elif len_unique_columns > maximum:\n",
    "            interval-=increment\n",
    "\n",
    "        # Get slice\n",
    "        zero_corr_df = rdf.loc[(rdf['values'] <= +interval) & (rdf['values'] >= -interval)]\n",
    "\n",
    "        # Put value and avoid duplicate\n",
    "        no_corr_combs = zero_corr_df['keys'].values\n",
    "        unique_columns = []\n",
    "        for no_corr_comb in no_corr_combs:\n",
    "            a, b = no_corr_comb.split(',')\n",
    "            if a not in unique_columns:\n",
    "                unique_columns.append(a) \n",
    "            if b not in unique_columns:\n",
    "                unique_columns.append(b) \n",
    "        len_unique_columns = len(unique_columns)\n",
    "    return unique_columns\n",
    "\n",
    "def calculate_uncorrelated_indicators_db_100stocks(ROOT_PATH='./', db_ver='v1', calculate_v1_indicators=True):\n",
    "    '''Each DB contains 1 ticker data.\n",
    "    - v1 indicators + additional primary indicators filtered using correlation coefficient\n",
    "    - 20-30 most uncorrelated indicators and their derivatives stored into .db\n",
    "    - close_EMA3 / close_EMA10 derivatives as required columns\n",
    "    - Max column per table: {selected_stock_only}\n",
    "    '''\n",
    "    excluded_stock = excluded_stock()\n",
    "    required_columns = ['close_EMA3_G', 'close_EMA10_G', 'close_EMA3_G_s1', 'close_EMA3_G_s2', 'close_EMA10_G_s1', 'close_EMA10_G_s2']\n",
    "    primary_index = 'time'\n",
    "    max_column_per_table = 250\n",
    "    selected_stock_only = False\n",
    "\n",
    "    DB_ROOT_PATH = f'{ROOT_PATH}db/{db_ver}/'\n",
    "    Path(DB_ROOT_PATH).mkdir(parents=True, exist_ok=True)\n",
    "    db_readme = f'''DB {db_ver}\n",
    "\n",
    "    Each DB contains 1 ticker data.\n",
    "    - v1 indicators + additional primary indicators filtered using correlation coefficient\n",
    "    - 20-30 most uncorrelated indicators and their derivatives stored into .db\n",
    "    - close_EMA3 / close_EMA10 derivatives as required columns\n",
    "    - Max column per table: {selected_stock_only}\n",
    "    '''\n",
    "    with open(f'{DB_ROOT_PATH}readme.txt', 'w') as f:\n",
    "        f.write(db_readme)\n",
    "\n",
    "    # New flow\n",
    "    origin_db = f'{ROOT_PATH}idx_raw.db'\n",
    "    v1_db = f'{ROOT_PATH}idx_indicators.db'\n",
    "    v2_db_placeholder = '{}idx_{}.db'\n",
    "\n",
    "    origin_db_conn = sqlite3.connect(origin_db)\n",
    "    v1_db_conn = sqlite3.connect(v1_db)\n",
    "\n",
    "    # Calculate v1 indicators\n",
    "    if calculate_v1_indicators:\n",
    "        calculate_all_indicator(origin_db, v1_db, excluded_stock, verbose=1, selected_stock_only=selected_stock_only, ROOT_PATH=ROOT_PATH)\n",
    "    elif not calculate_v1_indicators:\n",
    "        pass\n",
    "\n",
    "    tickers = stock_list_100_highestrank_and_availability()\n",
    "\n",
    "    # Try to resume from previous progress if any\n",
    "    try:\n",
    "        with open(f'{DB_ROOT_PATH}progress.cache', 'r') as f:\n",
    "            hotstart = int(f.read())\n",
    "    except FileNotFoundError:\n",
    "        hotstart = 0\n",
    "\n",
    "    count = 0\n",
    "    for ticker in tickers:\n",
    "        if count < hotstart:\n",
    "            count+=1\n",
    "            continue\n",
    "        if ticker in excluded_stock:\n",
    "            continue\n",
    "        tick = datetime.datetime.now()\n",
    "        v2_db = v2_db_placeholder.format(DB_ROOT_PATH, ticker)\n",
    "        v2_db_conn = sqlite3.connect(v2_db)\n",
    "\n",
    "        v1_df = pd.read_sql(f'select * from {ticker}', v1_db_conn)\n",
    "\n",
    "        # Calculate primary indicators\n",
    "        prim_df, source_columns = calculate_talib_indicators_primary(v1_df)\n",
    "\n",
    "        # Calculate indicator correlation\n",
    "        results = calculate_indicator_correlation(prim_df)\n",
    "\n",
    "        # Eliminate most uncorrelated indicators (20-30 indicators)\n",
    "        unique_columns = uncorrelated_indicators(results)\n",
    "\n",
    "        # Check if required_columns are in the unique_columns list, else, append those required_columns\n",
    "        # close_EMA3_G, close_EMA10_G, close_EMA3_G_s1, close_EMA3_G_s2, close_EMA10_G_s1, close_EMA10_G_s2 are in the list\n",
    "        for required_column in required_columns:\n",
    "            if required_column not in unique_columns:\n",
    "                unique_columns.append(required_column)\n",
    "\n",
    "        # Add primary_index column at the beginning\n",
    "        unique_columns.insert(0, primary_index)\n",
    "\n",
    "        # Add pattern recognition\n",
    "        prim_df, unique_columns = _pattern_recognition(prim_df, unique_columns)\n",
    "\n",
    "        # Slice dataframe by unique_columns\n",
    "        unique_df = prim_df[unique_columns]\n",
    "\n",
    "        # Calculate indicator derivatives for additional calculated indicator in source_columns\n",
    "        derivatives_columns = []\n",
    "        for unique_column in unique_columns:\n",
    "            if unique_column in source_columns:\n",
    "                derivatives_columns.append(unique_column)\n",
    "        derivatives_df = _indicator_derivatives(unique_df, derivatives_columns)\n",
    "\n",
    "        # Store calculated df as separated .db for each stock\n",
    "        # with 0->inf. as table name for specified split value\n",
    "        store_splitdf(derivatives_df, primary_index, v2_db_conn, max_column=max_column_per_table)\n",
    "\n",
    "        count+=1\n",
    "        tock = datetime.datetime.now()\n",
    "        print(f'{count}/{len(tickers)}: {ticker} -- Elapsed time: {tock-tick}')\n",
    "        with open(f'{DB_ROOT_PATH}progress.cache', 'w') as f:\n",
    "            f.write(str(count))\n",
    "            \n",
    "def load_ml_database(DB_PATH, columns_to_drop=['time']):\n",
    "    '''Instead of regular read_sql,\n",
    "    the ml-specialized df need to fill\n",
    "    null value and drop unnecessary column\n",
    "    '''\n",
    "    db_conn = sqlite3.connection(DB_PATH)\n",
    "    df = restore_splitdf(db_conn)\n",
    "    df = df.fillna(0)\n",
    "    for column_to_drop in columns_to_drop:\n",
    "        try:\n",
    "            df = df.drop(column_to_drop, axis='columns')\n",
    "        except ValueError:\n",
    "            pass\n",
    "    return df\n",
    "\n",
    "def input_c_from_sa_v2(ROOT_PATH, model_version, ticker, output_c):\n",
    "    '''Fetch input_c list from\n",
    "    sensitivity analysis file\n",
    "    \n",
    "    Revision from v1:\n",
    "    - Revising path to grouping based on \n",
    "        simulation version.\n",
    "    '''\n",
    "    filename = f'{ROOT_PATH}statistics/v{model_version}/sa_{ticker}_{output_c}.json'\n",
    "\n",
    "    with open(filename, 'r') as f:\n",
    "        sa_dict = json.load(f)\n",
    "\n",
    "    final_column = sorted(sa_dict.items(), key=lambda x: x[1])[0][0]\n",
    "\n",
    "    input_c = []\n",
    "    for sa_key in sa_dict.keys():\n",
    "        input_c.append(sa_key)\n",
    "        if sa_key == final_column:\n",
    "            break\n",
    "    return input_c\n",
    "\n",
    "def split_traintest(df, split=0.8):\n",
    "    '''Splif df with train fraction.\n",
    "    Normalize data using train set mean-std normalization.\n",
    "    Return normalized train-test set, mean, and std.\n",
    "    '''\n",
    "    # Train/test split\n",
    "    train_df = df[:int(split*len(df))]\n",
    "    test_df = df[int(split*len(df)):]\n",
    "    test_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    train_mean = train_df.mean()\n",
    "    train_std = train_df.std()\n",
    "\n",
    "    # Normalize dataset\n",
    "    train_df = (train_df - train_mean) / train_std\n",
    "    test_df = (test_df - train_mean) / train_std\n",
    "    return train_df, test_df, train_mean, train_std\n",
    "\n",
    "def model_compiler(model):\n",
    "    '''Adam optimizer, mse loss, RMSE metrics\n",
    "    '''\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='mse', metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "    return model\n",
    "\n",
    "def simpleRNN_1layer(units, return_sequences=False, create_input_layer=False, shape=None):\n",
    "    if create_input_layer:\n",
    "        model = tf.keras.models.Sequential([\n",
    "            tf.keras.Input(shape),\n",
    "            tf.keras.layers.SimpleRNN(units, return_sequences=return_sequences),\n",
    "            tf.keras.layers.Dense(1),\n",
    "        ])\n",
    "    else:\n",
    "        model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.SimpleRNN(units, return_sequences=return_sequences),\n",
    "            tf.keras.layers.Dense(1),\n",
    "        ])\n",
    "    return model_compiler(model)\n",
    "\n",
    "def GRU_1layer(units, return_sequences=False, create_input_layer=False, shape=None):\n",
    "    if create_input_layer:\n",
    "        model = tf.keras.models.Sequential([\n",
    "            tf.keras.Input(shape),\n",
    "            tf.keras.layers.GRU(units, return_sequences=return_sequences),\n",
    "            tf.keras.layers.Dense(1),\n",
    "        ])\n",
    "    else:\n",
    "        model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.GRU(units, return_sequences=return_sequences),\n",
    "            tf.keras.layers.Dense(1),\n",
    "        ])\n",
    "    return model_compiler(model)\n",
    "\n",
    "def LSTM_1layer(units, return_sequences=False, create_input_layer=False, shape=None):\n",
    "    if create_input_layer:\n",
    "        model = tf.keras.models.Sequential([\n",
    "            tf.keras.Input(shape),\n",
    "            tf.keras.layers.LSTM(units, return_sequences=return_sequences),\n",
    "            tf.keras.layers.Dense(1),\n",
    "        ])\n",
    "    else:\n",
    "        model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.LSTM(units, return_sequences=return_sequences),\n",
    "            tf.keras.layers.Dense(1),\n",
    "        ])\n",
    "    return model_compiler(model)\n",
    "\n",
    "def read_optimal_model_config(statistics_file_path, indicator='test_rmse'):\n",
    "    '''Read most optimal architecture, recurrent\n",
    "    lstmu, and epoch from optimization simulation.\n",
    "    '''\n",
    "    columns = ('ticker','input_c','output_c','architecture','recurrent','lstmu','epoch','train_mse','train_rmse','test_mse','test_rmse')\n",
    "    sa_DF = pd.read_csv(statistics_file_path, header=0, names=columns)\n",
    "    best_config = sa_DF.sort_values(by='test_rmse').values[0]\n",
    "    return best_config[3], best_config[4], best_config[5], best_config[6]\n",
    "\n",
    "def execute_input_c_v2(ticker, ROOT_PATH, output_c='close_EMA3_G_s1', model_version='2', input_c_limit=25, epochs=(15,), rnnus=(14,), recurrents=(7,), train=0.8):\n",
    "    '''Performing sensitivity input_c sensitivity analysis\n",
    "    that satisfies passed parameters\n",
    "    \n",
    "    Revision from v1:\n",
    "    - fixing results/temporary directory confusion\n",
    "        when running a new version. Grouping results by\n",
    "        `model_version` params.\n",
    "    - Fixing output_c temporary fixing that previously\n",
    "        think that the original EMAx_G is already a +1\n",
    "        offset value.\n",
    "    - Replace data loading with `restore_splitdf` function\n",
    "    \n",
    "    Important directories\n",
    "    ./db -> source stock database\n",
    "    ./statistics -> model performance histories\n",
    "    '''    \n",
    "    DB_PATH = f'{ROOT_PATH}db/v{model_version}/idx_{ticker}.db'\n",
    "    STATISTICS_FILE = f'{ROOT_PATH}statistics/v{model_version}/sa_{ticker}_{output_c}.csv'\n",
    "    \n",
    "    # Excluded columns\n",
    "    excluded_columns_res = ('change_b.f.', '_s[0-9]*')\n",
    "\n",
    "    # Load data\n",
    "    df = load_ml_database(DB_PATH)\n",
    "    \n",
    "    # Split train-test\n",
    "    train_df, test_df, train_mean, train_std = split_traintest(df, split=train)\n",
    "\n",
    "    # Convert all possible input column into dictionary\n",
    "    # as `additional_c`\n",
    "    additional_c = [x for x in df.columns]\n",
    "\n",
    "    # Delete columns that satisfy `excluded_columns_re`\n",
    "    for excluded_columns_re in excluded_columns_res:\n",
    "        additional_c = [x for x in additional_c if not re.search(excluded_columns_re, x)]\n",
    "\n",
    "    # Replace csv with plain statistics file name\n",
    "    sa_statistics = STATISTICS_FILE.replace('.csv', '')\n",
    "\n",
    "    try:\n",
    "        # Fetch `input_s` as hotstart\n",
    "        # Loop through `additional_c`, and delete keys that have been simulated before\n",
    "        # in `additional_c`. Using condition in `input_s.keys()`\n",
    "        with open(f'{sa_statistics}.json', 'r') as f:\n",
    "            input_s = json.load(f)\n",
    "\n",
    "        # Delete `additional_c` that have been simulated before\n",
    "        additional_c = [x for x in additional_c if x not in input_s.keys()]\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        input_s = {}\n",
    "\n",
    "    # Do a simulation as long as there are more `additional_c` to simulate\n",
    "    # Loop through `additional_c` everytime, until no more left.\n",
    "    # Better executed with while\n",
    "    while len(additional_c) > 0 and len(input_s.keys()) < input_c_limit:\n",
    "        # Use `additional_c_results` dictionary as `additional_c` sim results control. \n",
    "        # the format: {add_c: RMSE_performance}\n",
    "        additional_c_results = {}\n",
    "\n",
    "        current_ac_loop = 1\n",
    "        for ac in additional_c:\n",
    "            # Add additional simulated column into previous input order\n",
    "            # input_c = [x for x in input_s.keys()]\n",
    "            # input_c.append(ac)\n",
    "            # This `input_c` resetted in every `additional_c` loop\n",
    "\n",
    "            # Configure input column by adding +1 column to a previous\n",
    "            # configuration to see how much the performance increasing\n",
    "            input_c = [x for x in input_s.keys()]\n",
    "            input_c.append(ac)\n",
    "\n",
    "            for recurrent in recurrents:\n",
    "                for rnnu in rnnus:\n",
    "                    for epoch in epochs:\n",
    "                        tick = datetime.datetime.now()\n",
    "\n",
    "                        # Make dataset\n",
    "                        train_dataset = TimeseriesGenerator(train_df[input_c], train_df[output_c], length=recurrent, shuffle=True)\n",
    "                        test_dataset = TimeseriesGenerator(test_df[input_c], test_df[output_c], length=recurrent)\n",
    "\n",
    "                        # RNN Model\n",
    "                        model = GRU_1layer(rnnu, return_sequences=False)\n",
    "                        \n",
    "                        # Feed and train the model\n",
    "                        model.fit(train_dataset, epochs=epoch, verbose=0)\n",
    "\n",
    "                        # Evaluate model\n",
    "                        train_eval = model.evaluate(train_dataset, verbose=0)\n",
    "                        test_eval = model.evaluate(test_dataset, verbose=0)\n",
    "\n",
    "                        input_c_joined = '|'.join(input_c)\n",
    "                        statistics = f'''{ticker},{input_c_joined},{output_c},{recurrent},{rnnu},{epoch},{train_eval[0]},{train_eval[1]},{test_eval[0]},{test_eval[1]}\\n'''\n",
    "                        with open(STATISTICS_FILE, 'a') as f:\n",
    "                            f.write(statistics)\n",
    "\n",
    "                        tock = datetime.datetime.now()\n",
    "\n",
    "                        print(f'''{ticker} i[{len(input_c)}] o[{output_c}] acleft[{current_ac_loop}/{len(additional_c)}] aic[{ac}] R[{recurrent}] RU[{rnnu}] E[{epoch}] trainRMSE[{round(train_eval[1], 4)}] testRMSE[{round(test_eval[1], 4)}] time: {tock-tick}''')\n",
    "                        current_ac_loop+=1\n",
    "\n",
    "                        # Add testRMSE into `additional_c_results`\n",
    "                        additional_c_results[ac] = test_eval[1]\n",
    "\n",
    "        # Everytime the loop through `additional_c` finish,\n",
    "        # sort the `additional_c_results` using\n",
    "        # max_res = sorted(additional_c_results.items(), keys=lambda x: x[1])\n",
    "        max_res = sorted(additional_c_results.items(), key=lambda x: x[1])\n",
    "\n",
    "        # Fetch max_res[0] to `input_s`\n",
    "        # input_s[max_res[0][0]] = max_res[0][1]\n",
    "        input_s[max_res[0][0]] = max_res[0][1]\n",
    "\n",
    "        # Everytime the SA period finish, the most sensitive\n",
    "        # additional column deleted from `additional_c`\n",
    "        additional_c.remove(max_res[0][0])\n",
    "\n",
    "        print(f'=== MOST SENSITIVE: {max_res[0][0]} with combined RMSE: {max_res[0][1]} ===')\n",
    "\n",
    "        # Save current `input_s` to file\n",
    "        with open(f'{sa_statistics}.json', 'w') as f:\n",
    "            json.dump(input_s, f)\n",
    "            \n",
    "def execute_configuration_optimization_v2(ticker, ROOT_PATH, output_c='close_EMA3_G_s1', model_version='2', epochs=(10,20,40), rnnus=(4,8,12,16), recurrents=(5,7,10,15,20), train=0.8):\n",
    "    '''\n",
    "    Differences with v1:\n",
    "    - remove offset parameter, integerating it with 'correct'\n",
    "        output_c instead of manipulating it on the go. \n",
    "    '''\n",
    "    \n",
    "    DB_PATH = f'{ROOT_PATH}db/v{model_version}/idx_{ticker}.db'\n",
    "    STATISTICS_FILE = f'{ROOT_PATH}statistics/v{model_version}/sa_{ticker}_{output_c}.csv'\n",
    "    input_c = input_c_from_sa_v2(ROOT_PATH, model_version, ticker, output_c)\n",
    "    architectures = ('SimpleRNN', 'GRU', 'LSTM')\n",
    "\n",
    "    # Load data\n",
    "    df = load_ml_database(DB_PATH)\n",
    "    \n",
    "    # Split and normalize train-test set\n",
    "    train_df, test_df, train_mean, train_std = split_traintest(df, split=train)\n",
    "    \n",
    "    # Create progress cache\n",
    "    progress_cache_file = f'{ROOT_PATH}progress_cache/v{model_version}/opt_{ticker}_{output_c}.txt'\n",
    "    try:\n",
    "        with open(progress_cache_file, 'r') as f:\n",
    "            HOTSTART = int(f.read())\n",
    "    except FileNotFoundError:\n",
    "        HOTSTART = 0\n",
    "\n",
    "    loops = 0\n",
    "    for architecture in architectures:\n",
    "        for recurrent in recurrents:\n",
    "            for rnnu in rnnus:\n",
    "                for epoch in epochs:\n",
    "                    tick = datetime.datetime.now()\n",
    "                    # Hotstart\n",
    "                    if loops < HOTSTART:\n",
    "                        loops+=1\n",
    "                        continue\n",
    "\n",
    "                    # Make dataset\n",
    "                    train_dataset = TimeseriesGenerator(train_df[input_c], train_df[output_c], length=recurrent, shuffle=True)\n",
    "                    test_dataset = TimeseriesGenerator(test_df[input_c], test_df[output_c], length=recurrent)\n",
    "\n",
    "                    # RNN Model\n",
    "                    if architecture == 'SimpleRNN':\n",
    "                        model = simpleRNN_1layer(rnnu, return_sequences=False)\n",
    "                    elif architecture == 'GRU':\n",
    "                        model = GRU_1layer(rnnu, return_sequences=False)\n",
    "                    elif architecture == 'LSTM':\n",
    "                        model = LSTM_1layer(rnnu, return_sequences=False)\n",
    "\n",
    "                    # Feed and train the model\n",
    "                    model.fit(train_dataset, epochs=epoch, verbose=0)\n",
    "\n",
    "                    # Evaluate model\n",
    "                    train_eval = model.evaluate(train_dataset, verbose=0)\n",
    "                    test_eval = model.evaluate(test_dataset, verbose=0)\n",
    "\n",
    "                    input_c_joined = '|'.join(input_c)\n",
    "                    statistics = f'''{ticker},{input_c_joined},{output_c},{architecture},{recurrent},{rnnu},{epoch},{train_eval[0]},{train_eval[1]},{test_eval[0]},{test_eval[1]}\\n'''\n",
    "                    with open(STATISTICS_FILE, 'a') as f:\n",
    "                        f.write(statistics)\n",
    "\n",
    "                    tock = datetime.datetime.now()\n",
    "\n",
    "                    print(f'''{loops} {ticker} {len(input_c)} {architecture} o[{output_c}] R[{recurrent}] RU[{rnnu}] E[{epoch}] trainRMSE[{round(train_eval[1], 5)}] testRMSE[{round(test_eval[1], 5)}] time: {tock-tick}''')\n",
    "\n",
    "                    loops+=1\n",
    "                    # Save progress to cache\n",
    "                    with open(progress_cache_file, 'w') as f:\n",
    "                        f.write(str(loops))\n",
    "            \n",
    "def execute_batch_v2(simtype, offset, instance_no, ROOT_PATH, split=5, output_cs=('close_EMA3_G_s1','close_EMA10_G_s1'), model_version='2', input_c_limit=25, epochs=(10,), rnnus=(14,), recurrents=(5,), train=0.8):\n",
    "    '''\n",
    "    type: string\n",
    "        input_c\n",
    "        opt\n",
    "        \n",
    "    Revision compared to v1:\n",
    "    - add excluded stock\n",
    "    - small revision to adapt to newer `input_c` and `opt` version\n",
    "    '''\n",
    "    # Create progress cache\n",
    "    progress_cache_file = f'{ROOT_PATH}progress_cache/v{model_version}/{simtype}_{offset}_{instance_no}.txt'\n",
    "    try:\n",
    "        with open(progress_cache_file, 'r') as f:\n",
    "            HOTSTART = int(f.read())\n",
    "    except FileNotFoundError:\n",
    "        HOTSTART = 0\n",
    "    \n",
    "    # Fetch stock metadata\n",
    "    # investing_metadata = stock_metadata(ROOT_PATH)\n",
    "    excluded_stocks = excluded_stock()\n",
    "    tickers = stock_list_100_highestrank_and_availability()\n",
    "    tickers = [ticker for ticker in tickers if ticker not in excluded_stocks]\n",
    "    \n",
    "    # Total batch per instances\n",
    "    batch = int(len(tickers) / split)\n",
    "    \n",
    "    batch_metadata = tickers[instance_no * batch:(instance_no + 1) * batch]\n",
    "    \n",
    "    loops = 0\n",
    "    for ticker in batch_metadata:\n",
    "        for output_c in output_cs:\n",
    "            # Hotstart\n",
    "            if loops < HOTSTART:\n",
    "                loops+=1\n",
    "                continue\n",
    "            if simtype == 'input_c':\n",
    "                execute_input_c_v2(ticker, ROOT_PATH, output_c=output_c, model_version=model_version, input_c_limit=input_c_limit, epochs=epochs, rnnus=rnnus, recurrents=recurrents, train=train)\n",
    "            elif simtype == 'opt':\n",
    "                execute_configuration_optimization_v2(ticker, ROOT_PATH, output_c=output_c, model_version=model_version, epochs=epochs, rnnus=rnnus, recurrents=recurrents, train=train)\n",
    "            loops+=1\n",
    "            \n",
    "            # Save progress to cache\n",
    "            with open(progress_cache_file, 'w') as f:\n",
    "                f.write(str(loops))  \n",
    "                \n",
    "def execute_retrain_model_v2(ROOT_PATH='./', output_cs=('close_EMA3_G_s1','close_EMA10_G_s1','close_EMA3_G_s2','close_EMA10_G_s2'), model_version='2', iteration_version ='1', backtest=True):\n",
    "    '''\n",
    "    source_db = 'idx_data_v1.3.db'\n",
    "    For backtest:\n",
    "        backtest = True\n",
    "        train = 0.9\n",
    "    For production:\n",
    "        backtest = False\n",
    "        train = 0.99\n",
    "        \n",
    "    Revision in v2\n",
    "    - Create new directory path for easier\n",
    "        model version grouping\n",
    "    '''\n",
    "    if backtest:\n",
    "        train = 0.9\n",
    "        STATISTICS_FILE = f'{ROOT_PATH}statistics/v{model_version}/backtest/{iteration_version}.csv'\n",
    "    else:\n",
    "        train = 0.99\n",
    "        STATISTICS_FILE = f'{ROOT_PATH}statistics/v{model_version}/production/{iteration_version}.csv'\n",
    "\n",
    "    excluded_stocks = excluded_stock()\n",
    "    tickers = stock_list_100_highestrank_and_availability()\n",
    "    tickers = [ticker for ticker in tickers if ticker not in excluded_stocks]\n",
    "    \n",
    "    for ticker in tickers:\n",
    "        DB_PATH = f'{ROOT_PATH}db/v{model_version}/idx_{ticker}.db'\n",
    "        # Load data\n",
    "        df = load_ml_database(DB_PATH)\n",
    "        train_df, test_df, train_mean, train_std = split_traintest(df, split=train)\n",
    "\n",
    "        loops = 0\n",
    "        for output_c in output_cs:\n",
    "            tick = datetime.datetime.now()\n",
    "\n",
    "            # Read optimal input_c configuration\n",
    "            input_c = input_c_from_sa_v2(ROOT_PATH, model_version, ticker, output_c)\n",
    "\n",
    "            # Read optimal model configuration\n",
    "            optimization_statistics_path = f'{ROOT_PATH}statistics/v{model_version}/sa_{ticker}_{output_c}.csv'\n",
    "            architecture, recurrent, rnnu, epoch = read_optimal_model_config(optimization_statistics_path, indicator='test_rmse')\n",
    "\n",
    "            # Make dataset\n",
    "            train_dataset = TimeseriesGenerator(train_df[input_c], train_df[output_c], length=recurrent, shuffle=True)\n",
    "            test_dataset = TimeseriesGenerator(test_df[input_c], test_df[output_c], length=recurrent)\n",
    "\n",
    "            # RNN Model\n",
    "            if architecture == 'SimpleRNN':\n",
    "                model = simpleRNN_1layer(rnnu, return_sequences=False)\n",
    "            elif architecture == 'GRU':\n",
    "                model = GRU_1layer(rnnu, return_sequences=False)\n",
    "            elif architecture == 'LSTM':\n",
    "                model = LSTM_1layer(rnnu, return_sequences=False)\n",
    "\n",
    "            # Feed and train the model\n",
    "            model.fit(train_dataset, epochs=epoch, verbose=0)\n",
    "\n",
    "            # Save model weights\n",
    "            weights_save_path = f'{ROOT_PATH}models/v{model_version}/{iteration_version}/{ticker}_{output_c}/'\n",
    "            model.save_weights(weights_save_path)\n",
    "\n",
    "            # Delete current model and re-load weights to make sure that weight is recoverable\n",
    "            del model\n",
    "            # RNN Model\n",
    "            if architecture == 'SimpleRNN':\n",
    "                model = simpleRNN_1layer(rnnu, return_sequences=False)\n",
    "            elif architecture == 'GRU':\n",
    "                model = GRU_1layer(rnnu, return_sequences=False)\n",
    "            elif architecture == 'LSTM':\n",
    "                model = LSTM_1layer(rnnu, return_sequences=False)\n",
    "            model.load_weights(weights_save_path)\n",
    "\n",
    "            # Evaluate model\n",
    "            train_eval = model.evaluate(train_dataset, verbose=0)\n",
    "            test_eval = model.evaluate(test_dataset, verbose=0)\n",
    "\n",
    "            input_c_joined = '|'.join(input_c)\n",
    "            train_mean_joined = '|'.join([str(x) for x in list(train_mean[input_c])])\n",
    "            train_std_joined = '|'.join([str(x) for x in list(train_std[input_c])])\n",
    "            statistics = f'''{ticker},{input_c_joined},{offset},{output_c},{architecture},{recurrent},{rnnu},{epoch},{train_eval[0]},{train_eval[1]},{test_eval[0]},{test_eval[1]},{train_mean_joined},{train_std_joined},{train_mean[output_c]},{train_std[output_c]}\\n'''\n",
    "            with open(STATISTICS_FILE, 'a') as f:\n",
    "                f.write(statistics)\n",
    "\n",
    "            tock = datetime.datetime.now()\n",
    "\n",
    "            print(f'''{loops} {ticker} {len(input_c)} {offset} {output_c} {architecture} R[{recurrent}] RU[{rnnu}] E[{epoch}] trainRMSE[{round(train_eval[1], 5)}] testRMSE[{round(test_eval[1], 5)}] time: {tock-tick}''')\n",
    "            loops+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3bbad2-2cd4-4d4e-8469-9346c08d7efc",
   "metadata": {},
   "source": [
    "# Model development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d436cad7-7a76-434b-9f12-ec35a1e9f4f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2233e685-9ebe-46cc-92bd-09e62bca2276",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faa6468-5621-4edf-9cd3-5e6fb80897f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60878a39-55b9-4423-9327-ec7ffd70fbbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>low_apo_EMA3_EMA30_offset</th>\n",
       "      <th>low_apo_EMA3_EMA200_offset</th>\n",
       "      <th>change_ema_EMA3</th>\n",
       "      <th>change_ema_EMA10</th>\n",
       "      <th>change_ema_EMA30</th>\n",
       "      <th>change_ema_EMA200</th>\n",
       "      <th>change_ema_EMA3_G</th>\n",
       "      <th>change_ema_EMA10_G</th>\n",
       "      <th>change_ema_EMA30_G</th>\n",
       "      <th>...</th>\n",
       "      <th>Volume_rank_macd_EMA10</th>\n",
       "      <th>Volume_rank_macd_EMA30</th>\n",
       "      <th>Volume_rank_macd_EMA200</th>\n",
       "      <th>Volume_rank_macd_EMA3_G</th>\n",
       "      <th>Volume_rank_macd_EMA10_G</th>\n",
       "      <th>Volume_rank_macd_EMA30_G</th>\n",
       "      <th>Volume_rank_macd_EMA200_G</th>\n",
       "      <th>Volume_rank_macd_EMA3_EMA10_offset</th>\n",
       "      <th>Volume_rank_macd_EMA3_EMA30_offset</th>\n",
       "      <th>Volume_rank_macd_EMA3_EMA200_offset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1194307200000000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1194393600000000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1194480000000000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1194566400000000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1194825600000000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3356</th>\n",
       "      <td>1635120000000000000</td>\n",
       "      <td>-33.824412</td>\n",
       "      <td>-9.776075</td>\n",
       "      <td>0.001969</td>\n",
       "      <td>0.002336</td>\n",
       "      <td>0.000993</td>\n",
       "      <td>-0.000119</td>\n",
       "      <td>-0.000659</td>\n",
       "      <td>-0.000228</td>\n",
       "      <td>2.184484e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014848</td>\n",
       "      <td>0.010646</td>\n",
       "      <td>-0.000474</td>\n",
       "      <td>-0.007524</td>\n",
       "      <td>-0.003754</td>\n",
       "      <td>-0.000875</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.630962</td>\n",
       "      <td>-0.485329</td>\n",
       "      <td>-12.552256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3357</th>\n",
       "      <td>1635206400000000000</td>\n",
       "      <td>22.758182</td>\n",
       "      <td>-11.543489</td>\n",
       "      <td>0.001483</td>\n",
       "      <td>0.002093</td>\n",
       "      <td>0.000993</td>\n",
       "      <td>-0.000108</td>\n",
       "      <td>-0.000486</td>\n",
       "      <td>-0.000244</td>\n",
       "      <td>2.231413e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009856</td>\n",
       "      <td>0.009146</td>\n",
       "      <td>-0.000595</td>\n",
       "      <td>-0.009043</td>\n",
       "      <td>-0.004992</td>\n",
       "      <td>-0.001500</td>\n",
       "      <td>-0.000121</td>\n",
       "      <td>-1.361602</td>\n",
       "      <td>-1.389664</td>\n",
       "      <td>4.989397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3358</th>\n",
       "      <td>1635292800000000000</td>\n",
       "      <td>8.582718</td>\n",
       "      <td>-13.189471</td>\n",
       "      <td>0.001322</td>\n",
       "      <td>0.001924</td>\n",
       "      <td>0.001004</td>\n",
       "      <td>-0.000095</td>\n",
       "      <td>-0.000161</td>\n",
       "      <td>-0.000169</td>\n",
       "      <td>1.087302e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005195</td>\n",
       "      <td>0.007538</td>\n",
       "      <td>-0.000746</td>\n",
       "      <td>-0.006108</td>\n",
       "      <td>-0.004661</td>\n",
       "      <td>-0.001608</td>\n",
       "      <td>-0.000151</td>\n",
       "      <td>-2.861849</td>\n",
       "      <td>-2.283101</td>\n",
       "      <td>11.962839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3359</th>\n",
       "      <td>1635379200000000000</td>\n",
       "      <td>5.161648</td>\n",
       "      <td>-14.946725</td>\n",
       "      <td>0.001205</td>\n",
       "      <td>0.001771</td>\n",
       "      <td>0.001010</td>\n",
       "      <td>-0.000083</td>\n",
       "      <td>-0.000118</td>\n",
       "      <td>-0.000152</td>\n",
       "      <td>5.335220e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001981</td>\n",
       "      <td>0.006246</td>\n",
       "      <td>-0.000863</td>\n",
       "      <td>-0.001404</td>\n",
       "      <td>-0.003214</td>\n",
       "      <td>-0.001292</td>\n",
       "      <td>-0.000117</td>\n",
       "      <td>-6.590879</td>\n",
       "      <td>-2.773204</td>\n",
       "      <td>11.836199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3360</th>\n",
       "      <td>1635465600000000000</td>\n",
       "      <td>3.582537</td>\n",
       "      <td>-16.928500</td>\n",
       "      <td>0.001111</td>\n",
       "      <td>0.001634</td>\n",
       "      <td>0.001010</td>\n",
       "      <td>-0.000072</td>\n",
       "      <td>-0.000094</td>\n",
       "      <td>-0.000137</td>\n",
       "      <td>4.667215e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001877</td>\n",
       "      <td>0.004602</td>\n",
       "      <td>-0.001046</td>\n",
       "      <td>-0.004082</td>\n",
       "      <td>-0.003858</td>\n",
       "      <td>-0.001644</td>\n",
       "      <td>-0.000183</td>\n",
       "      <td>7.074850</td>\n",
       "      <td>-4.293615</td>\n",
       "      <td>13.494840</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3361 rows  124 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     time  low_apo_EMA3_EMA30_offset  \\\n",
       "0     1194307200000000000                        NaN   \n",
       "1     1194393600000000000                        NaN   \n",
       "2     1194480000000000000                        NaN   \n",
       "3     1194566400000000000                        NaN   \n",
       "4     1194825600000000000                        NaN   \n",
       "...                   ...                        ...   \n",
       "3356  1635120000000000000                 -33.824412   \n",
       "3357  1635206400000000000                  22.758182   \n",
       "3358  1635292800000000000                   8.582718   \n",
       "3359  1635379200000000000                   5.161648   \n",
       "3360  1635465600000000000                   3.582537   \n",
       "\n",
       "      low_apo_EMA3_EMA200_offset  change_ema_EMA3  change_ema_EMA10  \\\n",
       "0                            NaN              NaN               NaN   \n",
       "1                            NaN              NaN               NaN   \n",
       "2                            NaN              NaN               NaN   \n",
       "3                            NaN              NaN               NaN   \n",
       "4                            NaN              NaN               NaN   \n",
       "...                          ...              ...               ...   \n",
       "3356                   -9.776075         0.001969          0.002336   \n",
       "3357                  -11.543489         0.001483          0.002093   \n",
       "3358                  -13.189471         0.001322          0.001924   \n",
       "3359                  -14.946725         0.001205          0.001771   \n",
       "3360                  -16.928500         0.001111          0.001634   \n",
       "\n",
       "      change_ema_EMA30  change_ema_EMA200  change_ema_EMA3_G  \\\n",
       "0                  NaN                NaN                NaN   \n",
       "1                  NaN                NaN                NaN   \n",
       "2                  NaN                NaN                NaN   \n",
       "3                  NaN                NaN                NaN   \n",
       "4                  NaN                NaN                NaN   \n",
       "...                ...                ...                ...   \n",
       "3356          0.000993          -0.000119          -0.000659   \n",
       "3357          0.000993          -0.000108          -0.000486   \n",
       "3358          0.001004          -0.000095          -0.000161   \n",
       "3359          0.001010          -0.000083          -0.000118   \n",
       "3360          0.001010          -0.000072          -0.000094   \n",
       "\n",
       "      change_ema_EMA10_G  change_ema_EMA30_G  ...  Volume_rank_macd_EMA10  \\\n",
       "0                    NaN                 NaN  ...                     NaN   \n",
       "1                    NaN                 NaN  ...                     NaN   \n",
       "2                    NaN                 NaN  ...                     NaN   \n",
       "3                    NaN                 NaN  ...                     NaN   \n",
       "4                    NaN                 NaN  ...                     NaN   \n",
       "...                  ...                 ...  ...                     ...   \n",
       "3356           -0.000228        2.184484e-05  ...                0.014848   \n",
       "3357           -0.000244        2.231413e-07  ...                0.009856   \n",
       "3358           -0.000169        1.087302e-05  ...                0.005195   \n",
       "3359           -0.000152        5.335220e-06  ...                0.001981   \n",
       "3360           -0.000137        4.667215e-07  ...               -0.001877   \n",
       "\n",
       "      Volume_rank_macd_EMA30  Volume_rank_macd_EMA200  \\\n",
       "0                        NaN                      NaN   \n",
       "1                        NaN                      NaN   \n",
       "2                        NaN                      NaN   \n",
       "3                        NaN                      NaN   \n",
       "4                        NaN                      NaN   \n",
       "...                      ...                      ...   \n",
       "3356                0.010646                -0.000474   \n",
       "3357                0.009146                -0.000595   \n",
       "3358                0.007538                -0.000746   \n",
       "3359                0.006246                -0.000863   \n",
       "3360                0.004602                -0.001046   \n",
       "\n",
       "      Volume_rank_macd_EMA3_G  Volume_rank_macd_EMA10_G  \\\n",
       "0                         NaN                       NaN   \n",
       "1                         NaN                       NaN   \n",
       "2                         NaN                       NaN   \n",
       "3                         NaN                       NaN   \n",
       "4                         NaN                       NaN   \n",
       "...                       ...                       ...   \n",
       "3356                -0.007524                 -0.003754   \n",
       "3357                -0.009043                 -0.004992   \n",
       "3358                -0.006108                 -0.004661   \n",
       "3359                -0.001404                 -0.003214   \n",
       "3360                -0.004082                 -0.003858   \n",
       "\n",
       "      Volume_rank_macd_EMA30_G  Volume_rank_macd_EMA200_G  \\\n",
       "0                          NaN                        NaN   \n",
       "1                          NaN                        NaN   \n",
       "2                          NaN                        NaN   \n",
       "3                          NaN                        NaN   \n",
       "4                          NaN                        NaN   \n",
       "...                        ...                        ...   \n",
       "3356                 -0.000875                  -0.000016   \n",
       "3357                 -0.001500                  -0.000121   \n",
       "3358                 -0.001608                  -0.000151   \n",
       "3359                 -0.001292                  -0.000117   \n",
       "3360                 -0.001644                  -0.000183   \n",
       "\n",
       "      Volume_rank_macd_EMA3_EMA10_offset  Volume_rank_macd_EMA3_EMA30_offset  \\\n",
       "0                                    NaN                                 NaN   \n",
       "1                                    NaN                                 NaN   \n",
       "2                                    NaN                                 NaN   \n",
       "3                                    NaN                                 NaN   \n",
       "4                                    NaN                                 NaN   \n",
       "...                                  ...                                 ...   \n",
       "3356                           -0.630962                           -0.485329   \n",
       "3357                           -1.361602                           -1.389664   \n",
       "3358                           -2.861849                           -2.283101   \n",
       "3359                           -6.590879                           -2.773204   \n",
       "3360                            7.074850                           -4.293615   \n",
       "\n",
       "      Volume_rank_macd_EMA3_EMA200_offset  \n",
       "0                                     NaN  \n",
       "1                                     NaN  \n",
       "2                                     NaN  \n",
       "3                                     NaN  \n",
       "4                                     NaN  \n",
       "...                                   ...  \n",
       "3356                           -12.552256  \n",
       "3357                             4.989397  \n",
       "3358                            11.962839  \n",
       "3359                            11.836199  \n",
       "3360                            13.494840  \n",
       "\n",
       "[3361 rows x 124 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test db read\n",
    "dbpath = './db/v2/idx_ACES.db'\n",
    "dbconn = sqlite3.connect(dbpath)\n",
    "df = pd.read_sql('select * from `1`', dbconn)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abc3914-21ba-4ddd-af9d-20f4d00385e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4182a9e0-09c0-428e-9a72-c246fd1a29bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dca0ea-c8f2-46d6-9d53-8a079d239af2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7630dfca-8cad-41d0-afee-077f9aac65c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:talib]",
   "language": "python",
   "name": "conda-env-talib-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
