{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb97cee-a438-4215-af9a-b6beefe1dbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_263_kt(hp):\n",
    "    '''model_263 kt version: \n",
    "    - 120 recurrent, 60 filtered features input\n",
    "    - 2 outputs (1 true / 1 false)'''\n",
    "    first_conv_filters = hp.Int('first_conv_filters', min_value=8, max_value=64, step=8)\n",
    "    second_conv_filters = hp.Int('second_conv_filters', min_value=16, max_value=128, step=16)\n",
    "    second_conv_kernel_size = hp.Int('second_conv_kernel_size', min_value=1, max_value=15, step=2)\n",
    "    second_conv_strides = hp.Int('second_conv_strides', min_value=1, max_value=5, step=1)\n",
    "    third_conv_filters = hp.Int('third_conv_filters', min_value=16, max_value=128, step=16)\n",
    "    third_conv_kernel_size = hp.Int('third_conv_kernel_size', min_value=1, max_value=20, step=3)\n",
    "    third_conv_strides = hp.Int('third_conv_strides', min_value=1, max_value=5, step=1)\n",
    "    recurrent_unit = hp.Int('recurrent_unit', min_value=6, max_value=60, step=6)\n",
    "    dense_unit = hp.Int('dense_unit', min_value=12, max_value=60, step=12)\n",
    "    lr = hp.Float('lr', min_value=0.000001, max_value=0.00009, sampling='log')\n",
    "    \n",
    "    input_shape = tf.keras.Input(shape=(120,60))\n",
    "    X = tf.keras.layers.Conv1D(filters=first_conv_filters, kernel_size=1, strides=1, padding='causal')(input_shape)\n",
    "    X = tf.keras.layers.Conv1D(filters=second_conv_filters, kernel_size=second_conv_kernel_size, strides=second_conv_strides, padding='causal')(X)\n",
    "    X = tf.keras.layers.Conv1D(filters=third_conv_filters, kernel_size=third_conv_kernel_size, strides=third_conv_strides, padding='causal')(X)\n",
    "    X = tf.keras.layers.GRU(units=recurrent_unit, return_sequences=False)(X)\n",
    "    X = tf.keras.layers.Dense(units=dense_unit, activation='relu')(X)\n",
    "    outputs = tf.keras.layers.Dense(units=2, activation='softmax')(X)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=input_shape, outputs=outputs)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def model_300_kt(hp):\n",
    "    '''developed from model_263: \n",
    "    - additional C1D and recurrent layer\n",
    "    - 2x more variable search range\n",
    "    - widening lr range\n",
    "    \n",
    "    - 120 recurrent, 60 filtered features input\n",
    "    - 2 outputs (1 true / 1 false)'''\n",
    "    c_filters_1 = hp.Int('c_filters_1', min_value=8, max_value=256, step=8)\n",
    "    c_filters_2 = hp.Int('c_filters_2', min_value=16, max_value=256, step=16)\n",
    "    c_kernel_2 = hp.Int('c_kernel_2', min_value=1, max_value=48, step=2)\n",
    "    c_strides_2 = hp.Int('c_strides_2', min_value=1, max_value=10, step=1)\n",
    "    c_filters_3 = hp.Int('c_filters_3', min_value=16, max_value=256, step=16)\n",
    "    c_kernel_3 = hp.Int('c_kernel_3', min_value=1, max_value=48, step=3)\n",
    "    c_strides_3 = hp.Int('c_strides_3', min_value=1, max_value=10, step=1)\n",
    "    c_filters_4 = hp.Int('c_filters_4', min_value=16, max_value=256, step=16)\n",
    "    c_kernel_4 = hp.Int('c_kernel_4', min_value=1, max_value=48, step=3)\n",
    "    c_strides_4 = hp.Int('c_strides_4', min_value=1, max_value=10, step=1)\n",
    "    r_unit_1 = hp.Int('r_unit_1', min_value=6, max_value=120, step=6)\n",
    "    r_unit_2 = hp.Int('r_unit_2', min_value=6, max_value=120, step=6)\n",
    "    d_unit_1 = hp.Int('d_unit_1', min_value=12, max_value=120, step=12)\n",
    "    lr = hp.Float('lr', min_value=0.0000005, max_value=0.0001, sampling='log')\n",
    "    \n",
    "    input_shape = tf.keras.Input(shape=(120,60))\n",
    "    X = tf.keras.layers.Conv1D(filters=c_filters_1, kernel_size=1, strides=1, padding='causal')(input_shape)\n",
    "    X = tf.keras.layers.Conv1D(filters=c_filters_2, kernel_size=c_kernel_2, strides=c_strides_2, padding='causal')(X)\n",
    "    X = tf.keras.layers.Conv1D(filters=c_filters_3, kernel_size=c_kernel_3, strides=c_strides_3, padding='causal')(X)\n",
    "    X = tf.keras.layers.Conv1D(filters=c_filters_4, kernel_size=c_kernel_4, strides=c_strides_4, padding='causal')(X)\n",
    "    X = tf.keras.layers.GRU(units=r_unit_1, return_sequences=True)(X)\n",
    "    X = tf.keras.layers.GRU(units=r_unit_2, return_sequences=False)(X)\n",
    "    X = tf.keras.layers.Dense(units=d_unit_1, activation='relu')(X)\n",
    "    outputs = tf.keras.layers.Dense(units=2, activation='softmax')(X)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=input_shape, outputs=outputs)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def model_301_kt(hp):\n",
    "    '''developed from model_300: \n",
    "    - add batch normalization and relu activation between convolutional layer\n",
    "    \n",
    "    - 120 recurrent, 60 filtered features input\n",
    "    - 2 outputs (1 true / 1 false)'''\n",
    "    c_filters_1 = hp.Int('c_filters_1', min_value=8, max_value=256, step=8)\n",
    "    c_filters_2 = hp.Int('c_filters_2', min_value=16, max_value=256, step=16)\n",
    "    c_kernel_2 = hp.Int('c_kernel_2', min_value=1, max_value=48, step=2)\n",
    "    c_strides_2 = hp.Int('c_strides_2', min_value=1, max_value=10, step=1)\n",
    "    c_filters_3 = hp.Int('c_filters_3', min_value=16, max_value=256, step=16)\n",
    "    c_kernel_3 = hp.Int('c_kernel_3', min_value=1, max_value=48, step=3)\n",
    "    c_strides_3 = hp.Int('c_strides_3', min_value=1, max_value=10, step=1)\n",
    "    c_filters_4 = hp.Int('c_filters_4', min_value=16, max_value=256, step=16)\n",
    "    c_kernel_4 = hp.Int('c_kernel_4', min_value=1, max_value=48, step=3)\n",
    "    c_strides_4 = hp.Int('c_strides_4', min_value=1, max_value=10, step=1)\n",
    "    r_unit_1 = hp.Int('r_unit_1', min_value=6, max_value=120, step=6)\n",
    "    r_unit_2 = hp.Int('r_unit_2', min_value=6, max_value=120, step=6)\n",
    "    d_unit_1 = hp.Int('d_unit_1', min_value=12, max_value=120, step=12)\n",
    "    lr = hp.Float('lr', min_value=0.0000005, max_value=0.0001, sampling='log')\n",
    "    \n",
    "    input_shape = tf.keras.Input(shape=(120,60))\n",
    "    X = tf.keras.layers.Conv1D(filters=c_filters_1, kernel_size=1, strides=1, padding='causal')(input_shape)\n",
    "    X = tf.keras.layers.BatchNormalization()(X)\n",
    "    X = tf.keras.layers.Activation('relu')(X)\n",
    "    X = tf.keras.layers.Conv1D(filters=c_filters_2, kernel_size=c_kernel_2, strides=c_strides_2, padding='causal')(X)\n",
    "    X = tf.keras.layers.BatchNormalization()(X)\n",
    "    X = tf.keras.layers.Activation('relu')(X)\n",
    "    X = tf.keras.layers.Conv1D(filters=c_filters_3, kernel_size=c_kernel_3, strides=c_strides_3, padding='causal')(X)\n",
    "    X = tf.keras.layers.BatchNormalization()(X)\n",
    "    X = tf.keras.layers.Activation('relu')(X)\n",
    "    X = tf.keras.layers.Conv1D(filters=c_filters_4, kernel_size=c_kernel_4, strides=c_strides_4, padding='causal')(X)\n",
    "    X = tf.keras.layers.BatchNormalization()(X)\n",
    "    X = tf.keras.layers.Activation('relu')(X)\n",
    "    X = tf.keras.layers.GRU(units=r_unit_1, return_sequences=True)(X)\n",
    "    X = tf.keras.layers.GRU(units=r_unit_2, return_sequences=False)(X)\n",
    "    X = tf.keras.layers.Dense(units=d_unit_1, activation='relu')(X)\n",
    "    outputs = tf.keras.layers.Dense(units=2, activation='softmax')(X)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=input_shape, outputs=outputs)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def model_302_kt(hp):\n",
    "    '''developed from model_301: \n",
    "    - add bypass and residual layer to increase generalization\n",
    "        between layers\n",
    "    \n",
    "    - 120 recurrent, 60 filtered features input\n",
    "    - 2 outputs (1 true / 1 false)'''\n",
    "    def bypass(X, X_bypass, operations):\n",
    "        '''A conditional layer that let the optimizer\n",
    "        choose to bypass, add, or concatenate'''\n",
    "        # assert X.shape == X_bypass.shape\n",
    "        if operations == 0:\n",
    "            return X\n",
    "        elif operations == 1:\n",
    "            return tf.keras.layers.Add()([X, X_bypass])\n",
    "        elif operations == 2:\n",
    "            return tf.keras.layers.concatenate([X, X_bypass])\n",
    "        \n",
    "    bp_1 = hp.Choice('bp_1', [0,1])\n",
    "    bp_2 = hp.Choice('bp_2', [0,1])\n",
    "    bp_3 = hp.Choice('bp_3', [0,1])\n",
    "    \n",
    "    \n",
    "    c_filters_1 = hp.Int('c_filters_1', min_value=8, max_value=256, step=8)\n",
    "    c_kernel_1 = hp.Int('c_kernel_1', min_value=1, max_value=48, step=2)\n",
    "    c_strides_1 = hp.Int('c_strides_1', min_value=1, max_value=4, step=1)\n",
    "    c_kernel_2 = hp.Int('c_kernel_2', min_value=1, max_value=48, step=2)\n",
    "    c_kernel_3 = hp.Int('c_kernel_3', min_value=1, max_value=48, step=3)\n",
    "    c_kernel_4 = hp.Int('c_kernel_4', min_value=1, max_value=48, step=4)\n",
    "    r_unit_1 = hp.Int('r_unit_1', min_value=6, max_value=120, step=6)\n",
    "    r_unit_2 = hp.Int('r_unit_2', min_value=6, max_value=120, step=6)\n",
    "    d_unit_1 = hp.Int('d_unit_1', min_value=12, max_value=120, step=12)\n",
    "    lr = hp.Float('lr', min_value=0.0000005, max_value=0.0001, sampling='log')\n",
    "    \n",
    "    input_shape = tf.keras.Input(shape=(120,60))\n",
    "    \n",
    "    X = tf.keras.layers.Conv1D(filters=c_filters_1, kernel_size=c_kernel_1, strides=c_strides_1, padding='causal')(input_shape)\n",
    "    X1 = tf.keras.layers.BatchNormalization()(X)\n",
    "    X = tf.keras.layers.Activation('relu')(X1)\n",
    "    \n",
    "    X = tf.keras.layers.Conv1D(filters=c_filters_1, kernel_size=c_kernel_2, strides=1, padding='causal')(X)\n",
    "    X2 = tf.keras.layers.BatchNormalization()(X)\n",
    "    X = bypass(X2, X1, operations=bp_1)\n",
    "    X = tf.keras.layers.Activation('relu')(X)\n",
    "    \n",
    "    X = tf.keras.layers.Conv1D(filters=c_filters_1, kernel_size=c_kernel_3, strides=1, padding='causal')(X)\n",
    "    X3 = tf.keras.layers.BatchNormalization()(X)\n",
    "    X = bypass(X3, X2, operations=bp_2)\n",
    "    X = tf.keras.layers.Activation('relu')(X)\n",
    "    \n",
    "    X = tf.keras.layers.Conv1D(filters=c_filters_1, kernel_size=c_kernel_4, strides=1, padding='causal')(X)\n",
    "    X4 = tf.keras.layers.BatchNormalization()(X)\n",
    "    X = bypass(X4, X3, operations=bp_3)\n",
    "    X = tf.keras.layers.Activation('relu')(X)\n",
    "    \n",
    "    X = tf.keras.layers.GRU(units=r_unit_1, return_sequences=True)(X)\n",
    "    X = tf.keras.layers.GRU(units=r_unit_2, return_sequences=False)(X)\n",
    "    X = tf.keras.layers.Dense(units=d_unit_1, activation='relu')(X)\n",
    "    outputs = tf.keras.layers.Dense(units=2, activation='softmax')(X)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=input_shape, outputs=outputs)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def model_303_kt(hp):\n",
    "    '''developed from model_301: \n",
    "    - add bypass and residual layer to increase generalization\n",
    "        between layers\n",
    "    \n",
    "    - 120 recurrent, 60 filtered features input\n",
    "    - 2 outputs (1 true / 1 false)'''\n",
    "    def bypass(X, X_bypass, operations):\n",
    "        '''A conditional layer that let the optimizer\n",
    "        choose to bypass, add, or concatenate'''\n",
    "        # assert X.shape == X_bypass.shape\n",
    "        if operations == 0:\n",
    "            return X\n",
    "        elif operations == 1:\n",
    "            return tf.keras.layers.Add()([X, X_bypass])\n",
    "        elif operations == 2:\n",
    "            print(X.shape, X_bypass.shape)\n",
    "            return tf.keras.layers.Concatenate(axis=1)([X, X_bypass])\n",
    "        \n",
    "    bp_1 = hp.Choice('bp_1', [0,2])\n",
    "    bp_2 = hp.Choice('bp_2', [0,2])\n",
    "    bp_3 = hp.Choice('bp_3', [0,2])\n",
    "    \n",
    "    \n",
    "    c_filters_1 = hp.Int('c_filters_1', min_value=8, max_value=256, step=8)\n",
    "    c_kernel_2 = hp.Int('c_kernel_2', min_value=1, max_value=48, step=2)\n",
    "    c_strides_2 = hp.Int('c_strides_2', min_value=1, max_value=4, step=1)\n",
    "    c_kernel_3 = hp.Int('c_kernel_3', min_value=1, max_value=48, step=3)\n",
    "    c_strides_3 = hp.Int('c_strides_3', min_value=1, max_value=4, step=1)\n",
    "    c_kernel_4 = hp.Int('c_kernel_4', min_value=1, max_value=48, step=4)\n",
    "    c_strides_4 = hp.Int('c_strides_4', min_value=1, max_value=4, step=1)\n",
    "    r_unit_1 = hp.Int('r_unit_1', min_value=6, max_value=120, step=6)\n",
    "    r_unit_2 = hp.Int('r_unit_2', min_value=6, max_value=120, step=6)\n",
    "    d_unit_1 = hp.Int('d_unit_1', min_value=12, max_value=120, step=12)\n",
    "    lr = hp.Float('lr', min_value=0.0000005, max_value=0.0001, sampling='log')\n",
    "    \n",
    "    input_shape = tf.keras.Input(shape=(120,60))\n",
    "    \n",
    "    X = tf.keras.layers.Conv1D(filters=c_filters_1, kernel_size=1, strides=1, padding='causal')(input_shape)\n",
    "    X1 = tf.keras.layers.BatchNormalization()(X)\n",
    "    X = tf.keras.layers.Activation('relu')(X1)\n",
    "    \n",
    "    X = tf.keras.layers.Conv1D(filters=c_filters_1, kernel_size=c_kernel_2, strides=c_strides_2, padding='causal')(X)\n",
    "    X2 = tf.keras.layers.BatchNormalization()(X)\n",
    "    X = bypass(X2, X1, operations=bp_1)\n",
    "    X = tf.keras.layers.Activation('relu')(X)\n",
    "    \n",
    "    X = tf.keras.layers.Conv1D(filters=c_filters_1, kernel_size=c_kernel_3, strides=c_strides_3, padding='causal')(X)\n",
    "    X3 = tf.keras.layers.BatchNormalization()(X)\n",
    "    X = bypass(X3, X2, operations=bp_2)\n",
    "    X = tf.keras.layers.Activation('relu')(X)\n",
    "    \n",
    "    X = tf.keras.layers.Conv1D(filters=c_filters_1, kernel_size=c_kernel_4, strides=c_strides_4, padding='causal')(X)\n",
    "    X4 = tf.keras.layers.BatchNormalization()(X)\n",
    "    X = bypass(X4, X3, operations=bp_3)\n",
    "    X = tf.keras.layers.Activation('relu')(X)\n",
    "    \n",
    "    X = tf.keras.layers.GRU(units=r_unit_1, return_sequences=True)(X)\n",
    "    X = tf.keras.layers.GRU(units=r_unit_2, return_sequences=False)(X)\n",
    "    X = tf.keras.layers.Dense(units=d_unit_1, activation='relu')(X)\n",
    "    outputs = tf.keras.layers.Dense(units=2, activation='softmax')(X)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=input_shape, outputs=outputs)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def model_304_kt(hp):\n",
    "    '''developed from model_301: \n",
    "    - add bypass and residual layer to increase generalization\n",
    "        between layers\n",
    "    \n",
    "    - 120 recurrent, 60 filtered features input\n",
    "    - 2 outputs (1 true / 1 false)'''       \n",
    "    def block(X, filters, recunits):\n",
    "        # Conv1\n",
    "        X = tf.keras.layers.Conv1D(filters=filters, kernel_size=1, strides=1, padding='causal')(X)\n",
    "        X = tf.keras.layers.BatchNormalization()(X)\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        # Conv2\n",
    "        X = tf.keras.layers.Conv1D(filters=filters, kernel_size=1, strides=1, padding='causal')(X)\n",
    "        X = tf.keras.layers.BatchNormalization()(X)\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        # Conv3\n",
    "        X = tf.keras.layers.Conv1D(filters=filters, kernel_size=1, strides=1, padding='causal')(X)\n",
    "        X = tf.keras.layers.BatchNormalization()(X)\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        # Recurrent unit\n",
    "        X = tf.keras.layers.GRU(units=recunits, return_sequences=True)(X)\n",
    "        X_bn = tf.keras.layers.BatchNormalization()(X)\n",
    "        X_act = tf.keras.layers.Activation('relu')(X_bn)\n",
    "        return X_bn, X_act\n",
    "    \n",
    "    b1_filters = hp.Int('b1_filters', min_value=4, max_value=16, step=4)\n",
    "    b2_filters = hp.Int('b2_filters', min_value=16, max_value=64, step=16)\n",
    "    b3_filters = hp.Int('b3_filters', min_value=4, max_value=32, step=8)\n",
    "    b1b2_recunits = hp.Int('b1b2_recunits', min_value=8, max_value=64, step=8)\n",
    "    b3_recunits = hp.Int('b3_recunits', min_value=4, max_value=32, step=8)\n",
    "    final_recunits = hp.Int('final_recunits', min_value=4, max_value=16, step=4)\n",
    "    final_dunits = hp.Int('final_dunits', min_value=4, max_value=16, step=4)\n",
    "    lr = hp.Float('lr', min_value=0.000001, max_value=0.0001, sampling='log')\n",
    "    \n",
    "    input_shape = tf.keras.Input(shape=(120,60))\n",
    "    # Block 1\n",
    "    X_bn_1, X_act_1 = block(input_shape, b1_filters, b1b2_recunits)\n",
    "    # Block 2\n",
    "    X_bn_2, X_act_2 = block(X_act_1, b2_filters, b1b2_recunits)\n",
    "    # Addition block\n",
    "    X = tf.keras.layers.Add()([X_bn_1, X_bn_2])\n",
    "    X = tf.keras.layers.Activation('relu')(X)\n",
    "    # Block 3\n",
    "    X_bn_3, X_act_3 = block(X, b3_filters, b3_recunits)\n",
    "    # Final layer\n",
    "    X = tf.keras.layers.GRU(units=final_recunits, return_sequences=False)(X_act_3)\n",
    "    X = tf.keras.layers.Dense(units=final_dunits, activation='relu')(X)\n",
    "    outputs = tf.keras.layers.Dense(units=2, activation='softmax')(X)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=input_shape, outputs=outputs)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def model_305_kt(hp):\n",
    "    '''developed from model_304: \n",
    "    - split data sequence into sequence using prime number strides\n",
    "    \n",
    "    - 120 recurrent, 60 filtered features input\n",
    "    - 2 outputs (1 true / 1 false)'''       \n",
    "    def block(X, filters, recunits, first_block=False, strides_val=1):\n",
    "        # Conv1\n",
    "        X = tf.keras.layers.Conv1D(filters=filters, kernel_size=1, strides=1 if not first_block else strides_val, padding='causal')(X)\n",
    "        X = tf.keras.layers.BatchNormalization()(X)\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        # Conv2\n",
    "        X = tf.keras.layers.Conv1D(filters=filters, kernel_size=1, strides=1, padding='causal')(X)\n",
    "        X = tf.keras.layers.BatchNormalization()(X)\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        # Conv3\n",
    "        X = tf.keras.layers.Conv1D(filters=filters, kernel_size=1, strides=1, padding='causal')(X)\n",
    "        X = tf.keras.layers.BatchNormalization()(X)\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        # Recurrent unit\n",
    "        X = tf.keras.layers.GRU(units=recunits, return_sequences=True)(X)\n",
    "        X_bn = tf.keras.layers.BatchNormalization()(X)\n",
    "        X_act = tf.keras.layers.Activation('relu')(X_bn)\n",
    "        return X_bn, X_act\n",
    "    \n",
    "    def superblock(input_shape, component_num, final_dunits):\n",
    "        b1_filters = hp.Int(f'comp{component_num}_b1_filters', min_value=4, max_value=16, step=4)\n",
    "        b2_filters = hp.Int(f'comp{component_num}_b2_filters', min_value=16, max_value=64, step=16)\n",
    "        b3_filters = hp.Int(f'comp{component_num}_b3_filters', min_value=4, max_value=32, step=8)\n",
    "        b1b2_recunits = hp.Int(f'comp{component_num}_1b2_recunits', min_value=8, max_value=64, step=8)\n",
    "        b3_recunits = hp.Int(f'comp{component_num}_b3_recunits', min_value=4, max_value=32, step=8)\n",
    "        final_recunits = hp.Int(f'comp{component_num}_final_recunits', min_value=4, max_value=16, step=4)\n",
    "        # final_dunits = hp.Int(f'comp{component_num}_final_dunits', min_value=4, max_value=16, step=4)\n",
    "        # Block 1\n",
    "        X_bn_1, X_act_1 = block(input_shape, b1_filters, b1b2_recunits, first_block=True, strides_val=int(component_num))\n",
    "        # Block 2\n",
    "        X_bn_2, X_act_2 = block(X_act_1, b2_filters, b1b2_recunits)\n",
    "        # Addition block\n",
    "        X = tf.keras.layers.Add()([X_bn_1, X_bn_2])\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        # Block 3\n",
    "        X_bn_3, X_act_3 = block(X, b3_filters, b3_recunits)\n",
    "        # Final layer\n",
    "        X = tf.keras.layers.GRU(units=final_recunits, return_sequences=False)(X_act_3)\n",
    "        X = tf.keras.layers.Dense(units=final_dunits, activation='relu')(X)\n",
    "        return tf.expand_dims(X, axis=1)\n",
    "    \n",
    "    recurrents = 120\n",
    "    superblock_final_dunits = hp.Int('superblock_final_dunits', min_value=4, max_value=16, step=4)\n",
    "    lr = hp.Float('lr', min_value=0.000001, max_value=0.0001, sampling='log')\n",
    "    input_shape = tf.keras.Input(shape=(recurrents,60))\n",
    "    \n",
    "    n_components = generate_prime(recurrents)\n",
    "    component_results = []\n",
    "    for component_num in n_components:\n",
    "        superblock_result = superblock(input_shape, component_num, superblock_final_dunits)\n",
    "        component_results.append(superblock_result)\n",
    "        \n",
    "    # reverse component_result so the highest stride come first\n",
    "    component_results = component_results[::-1]\n",
    "    \n",
    "    X = tf.keras.layers.Concatenate(axis=1)(component_results)\n",
    "    X = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(units=8))(X)\n",
    "    X = tf.keras.layers.Dense(units=8, activation='relu')(X)\n",
    "    outputs = tf.keras.layers.Dense(units=2, activation='softmax')(X)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=input_shape, outputs=outputs)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def model_306_kt(hp):\n",
    "    '''developed from model_305: \n",
    "    - add component_strides to reduce number of components,\n",
    "        thus reducing model complexity (the 305 ver take 5 minutes\n",
    "        to compile)\n",
    "    - add batch normalization in the end of superblock\n",
    "    \n",
    "    - 120 recurrent, 60 filtered features input\n",
    "    - 2 outputs (1 true / 1 false)'''       \n",
    "    def block(X, filters, recunits, first_block=False, strides_val=1):\n",
    "        # Conv1\n",
    "        X = tf.keras.layers.Conv1D(filters=filters, kernel_size=1, strides=1 if not first_block else strides_val, padding='causal')(X)\n",
    "        X = tf.keras.layers.BatchNormalization()(X)\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        # Conv2\n",
    "        X = tf.keras.layers.Conv1D(filters=filters, kernel_size=1, strides=1, padding='causal')(X)\n",
    "        X = tf.keras.layers.BatchNormalization()(X)\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        # Conv3\n",
    "        X = tf.keras.layers.Conv1D(filters=filters, kernel_size=1, strides=1, padding='causal')(X)\n",
    "        X = tf.keras.layers.BatchNormalization()(X)\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        # Recurrent unit\n",
    "        X = tf.keras.layers.GRU(units=recunits, return_sequences=True)(X)\n",
    "        X_bn = tf.keras.layers.BatchNormalization()(X)\n",
    "        X_act = tf.keras.layers.Activation('relu')(X_bn)\n",
    "        return X_bn, X_act\n",
    "    \n",
    "    def superblock(input_shape, component_num, final_dunits):\n",
    "        b1_filters = hp.Int(f'comp{component_num}_b1_filters', min_value=4, max_value=16, step=4)\n",
    "        b2_filters = hp.Int(f'comp{component_num}_b2_filters', min_value=16, max_value=64, step=16)\n",
    "        b3_filters = hp.Int(f'comp{component_num}_b3_filters', min_value=4, max_value=32, step=8)\n",
    "        b1b2_recunits = hp.Int(f'comp{component_num}_1b2_recunits', min_value=8, max_value=64, step=8)\n",
    "        b3_recunits = hp.Int(f'comp{component_num}_b3_recunits', min_value=4, max_value=32, step=8)\n",
    "        final_recunits = hp.Int(f'comp{component_num}_final_recunits', min_value=4, max_value=16, step=4)\n",
    "        # final_dunits = hp.Int(f'comp{component_num}_final_dunits', min_value=4, max_value=16, step=4)\n",
    "        # Block 1\n",
    "        X_bn_1, X_act_1 = block(input_shape, b1_filters, b1b2_recunits, first_block=True, strides_val=int(component_num))\n",
    "        # Block 2\n",
    "        X_bn_2, X_act_2 = block(X_act_1, b2_filters, b1b2_recunits)\n",
    "        # Addition block\n",
    "        X = tf.keras.layers.Add()([X_bn_1, X_bn_2])\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        # Block 3\n",
    "        X_bn_3, X_act_3 = block(X, b3_filters, b3_recunits)\n",
    "        # Final layer\n",
    "        X = tf.keras.layers.GRU(units=final_recunits, return_sequences=False)(X_act_3)\n",
    "        X = tf.keras.layers.Dense(units=final_dunits)(X)\n",
    "        X = tf.keras.layers.BatchNormalization()(X)\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        return tf.expand_dims(X, axis=1)\n",
    "    \n",
    "    recurrents = 120\n",
    "    component_strides = 3\n",
    "    superblock_final_dunits = hp.Int('superblock_final_dunits', min_value=4, max_value=16, step=4)\n",
    "    lr = hp.Float('lr', min_value=0.000001, max_value=0.0001, sampling='log')\n",
    "    input_shape = tf.keras.Input(shape=(recurrents,60))\n",
    "    \n",
    "    n_components = generate_prime(int(recurrents // component_strides))\n",
    "    component_results = []\n",
    "    for component_num in n_components:\n",
    "        superblock_result = superblock(input_shape, component_num, superblock_final_dunits)\n",
    "        component_results.append(superblock_result)\n",
    "        \n",
    "    # reverse component_result so the highest stride come first\n",
    "    component_results = component_results[::-1]\n",
    "    \n",
    "    X = tf.keras.layers.Concatenate(axis=1)(component_results)\n",
    "    X = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(units=8))(X)\n",
    "    X = tf.keras.layers.Dense(units=8, activation='relu')(X)\n",
    "    outputs = tf.keras.layers.Dense(units=2, activation='softmax')(X)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=input_shape, outputs=outputs)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def model_307_kt(hp):\n",
    "    '''developed from model_306: \n",
    "    - Keep component_strides=1, but maintaining complexity of\n",
    "        each superblock by considering their total number of\n",
    "        constituent.\n",
    "    \n",
    "    - 120 recurrent, 60 filtered features input\n",
    "    - 2 outputs (1 true / 1 false)'''  \n",
    "    def min_max_step(component_num, recurrents, fraction, floor_fraction, num_steps, minimum_value):\n",
    "        recommended_value = int((recurrents // component_num) * fraction)\n",
    "        recommended_value = recommended_value if recommended_value >= minimum_value else minimum_value\n",
    "        floor_value = int(floor_fraction * recommended_value)\n",
    "        floor_value = floor_value if floor_value >= minimum_value else minimum_value\n",
    "        step_value = int((recommended_value - floor_value) // num_steps)\n",
    "        step_value = step_value if step_value >= 1 else 1\n",
    "        return floor_value, recommended_value, step_value\n",
    "    \n",
    "    def block(X, filters, recunits, first_block=False, strides_val=1):\n",
    "        # Conv1\n",
    "        X = tf.keras.layers.Conv1D(filters=filters, kernel_size=1, strides=1 if not first_block else strides_val, padding='causal')(X)\n",
    "        X = tf.keras.layers.BatchNormalization()(X)\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        # Conv2\n",
    "        X = tf.keras.layers.Conv1D(filters=filters, kernel_size=1, strides=1, padding='causal')(X)\n",
    "        X = tf.keras.layers.BatchNormalization()(X)\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        # Conv3\n",
    "        X = tf.keras.layers.Conv1D(filters=filters, kernel_size=1, strides=1, padding='causal')(X)\n",
    "        X = tf.keras.layers.BatchNormalization()(X)\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        # Recurrent unit\n",
    "        X = tf.keras.layers.GRU(units=recunits, return_sequences=True)(X)\n",
    "        X_bn = tf.keras.layers.BatchNormalization()(X)\n",
    "        X_act = tf.keras.layers.Activation('relu')(X_bn)\n",
    "        return X_bn, X_act\n",
    "    \n",
    "    def superblock(input_shape, component_num, final_dunits, recurrents, fraction=1, floor_fraction=0.25, num_steps=4, minimum_value=4):\n",
    "        recommended_value = int((recurrents // component_num) * fraction)\n",
    "        minv, maxv, step = min_max_step(component_num, recurrents, fraction, floor_fraction, num_steps, minimum_value)\n",
    "        superblock_hyp = hp.Int(f'comp{component_num}', min_value=minv, max_value=maxv, step=step)\n",
    "        X_bn_1, X_act_1 = block(input_shape, superblock_hyp, superblock_hyp, first_block=True, strides_val=int(component_num))\n",
    "        # Block 2\n",
    "        X_bn_2, X_act_2 = block(X_act_1, superblock_hyp, superblock_hyp)\n",
    "        # Addition block\n",
    "        X = tf.keras.layers.Add()([X_bn_1, X_bn_2])\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        # Block 3\n",
    "        X_bn_3, X_act_3 = block(X, superblock_hyp, superblock_hyp)\n",
    "        # Final layer\n",
    "        X = tf.keras.layers.GRU(units=superblock_hyp, return_sequences=False)(X_act_3)\n",
    "        X = tf.keras.layers.Dense(units=final_dunits)(X)\n",
    "        X = tf.keras.layers.BatchNormalization()(X)\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        return tf.expand_dims(X, axis=1)\n",
    "    \n",
    "    recurrents = 120\n",
    "    component_strides = 1\n",
    "    superblock_final_dunits = hp.Int('superblock_final_dunits', min_value=2, max_value=6, step=2)\n",
    "    lr = hp.Float('lr', min_value=0.000001, max_value=0.0001, sampling='log')\n",
    "    input_shape = tf.keras.Input(shape=(recurrents,60))\n",
    "    \n",
    "    n_components = generate_prime(int(recurrents // component_strides))\n",
    "    component_results = []\n",
    "    for component_num in n_components:\n",
    "        superblock_result = superblock(input_shape, component_num, superblock_final_dunits, recurrents)\n",
    "        component_results.append(superblock_result)\n",
    "        \n",
    "    # reverse component_result so the highest stride come first\n",
    "    component_results = component_results[::-1]\n",
    "    \n",
    "    X = tf.keras.layers.Concatenate(axis=1)(component_results)\n",
    "    X = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(units=8))(X)\n",
    "    X = tf.keras.layers.Dense(units=8, activation='relu')(X)\n",
    "    outputs = tf.keras.layers.Dense(units=2, activation='softmax')(X)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=input_shape, outputs=outputs)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def model_308_kt(hp):\n",
    "    '''developed from model_307: \n",
    "    - Stacking superblock until the number of recurrent\n",
    "        sequence before the final node reach threshold.\n",
    "    \n",
    "    - 120 recurrent, 60 filtered features input\n",
    "    - 2 outputs (1 true / 1 false)'''  \n",
    "    def min_max_step(component_num, recurrents, fraction, floor_fraction, num_steps, minimum_value):\n",
    "        recommended_value = int((recurrents // component_num) * fraction)\n",
    "        recommended_value = recommended_value if recommended_value >= minimum_value else minimum_value\n",
    "        floor_value = int(floor_fraction * recommended_value)\n",
    "        floor_value = floor_value if floor_value >= minimum_value else minimum_value\n",
    "        step_value = int((recommended_value - floor_value) // num_steps)\n",
    "        step_value = step_value if step_value >= 1 else 1\n",
    "        return floor_value, recommended_value, step_value\n",
    "    \n",
    "    def block(X, filters, recunits, first_block=False, strides_val=1):\n",
    "        # Conv1\n",
    "        X = tf.keras.layers.Conv1D(filters=filters, kernel_size=1, strides=1 if not first_block else strides_val, padding='causal')(X)\n",
    "        X = tf.keras.layers.BatchNormalization()(X)\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        # Conv2\n",
    "        X = tf.keras.layers.Conv1D(filters=filters, kernel_size=1, strides=1, padding='causal')(X)\n",
    "        X = tf.keras.layers.BatchNormalization()(X)\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        # Conv3\n",
    "        X = tf.keras.layers.Conv1D(filters=filters, kernel_size=1, strides=1, padding='causal')(X)\n",
    "        X = tf.keras.layers.BatchNormalization()(X)\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        # Recurrent unit\n",
    "        X = tf.keras.layers.GRU(units=recunits, return_sequences=True)(X)\n",
    "        X_bn = tf.keras.layers.BatchNormalization()(X)\n",
    "        X_act = tf.keras.layers.Activation('relu')(X_bn)\n",
    "        return X_bn, X_act\n",
    "    \n",
    "    def superblock(super_component_num, input_shape, component_num, final_dunits, recurrents, fraction=0.75, floor_fraction=0.25, num_steps=4, minimum_value=4):\n",
    "        recommended_value = int((recurrents // component_num) * fraction)\n",
    "        minv, maxv, step = min_max_step(component_num, recurrents, fraction, floor_fraction, num_steps, minimum_value)\n",
    "        superblock_hyp = hp.Int(f'{super_component_num}_comp{component_num}', min_value=minv, max_value=maxv, step=step)\n",
    "        X_bn_1, X_act_1 = block(input_shape, superblock_hyp, superblock_hyp, first_block=True, strides_val=int(component_num))\n",
    "        # Block 2\n",
    "        X_bn_2, X_act_2 = block(X_act_1, superblock_hyp, superblock_hyp)\n",
    "        # Addition block\n",
    "        X = tf.keras.layers.Add()([X_bn_1, X_bn_2])\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        # Block 3\n",
    "        X_bn_3, X_act_3 = block(X, superblock_hyp, superblock_hyp)\n",
    "        # Final layer\n",
    "        X = tf.keras.layers.GRU(units=superblock_hyp, return_sequences=False)(X_act_3)\n",
    "        X = tf.keras.layers.Dense(units=final_dunits)(X)\n",
    "        X = tf.keras.layers.BatchNormalization()(X)\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        return tf.expand_dims(X, axis=1)\n",
    "    \n",
    "    def superblock_component(input_shape, super_component_num):\n",
    "        component_strides = 1\n",
    "        superblock_final_dunits = hp.Int(f'{super_component_num}_superblock_final_dunits', min_value=2, max_value=6, step=2)\n",
    "        \n",
    "        n_components = generate_prime(int(input_shape.shape[1] // component_strides))\n",
    "        component_results = []\n",
    "        for component_num in n_components:\n",
    "            superblock_result = superblock(super_component_num, input_shape, component_num, superblock_final_dunits, input_shape.shape[1])\n",
    "            component_results.append(superblock_result)\n",
    "\n",
    "        # reverse component_result so the highest stride come first\n",
    "        component_results = component_results[::-1]\n",
    "        X = tf.keras.layers.Concatenate(axis=1)(component_results)\n",
    "        return X\n",
    "    \n",
    "    def stack_superblock(input_shape, threshold=10):\n",
    "        # Define initial state\n",
    "        # This initial state would be re-weitten in each\n",
    "        # loop\n",
    "        current_recurrent = input_shape.shape[1]\n",
    "        X = input_shape\n",
    "        count_component = 0\n",
    "        while current_recurrent > threshold:\n",
    "            X = superblock_component(X, count_component)\n",
    "            current_recurrent = X.shape[1]\n",
    "            count_component+=1\n",
    "        return X\n",
    "            \n",
    "        \n",
    "    recurrents = 120\n",
    "    lr = hp.Float('lr', min_value=0.000001, max_value=0.0001, sampling='log')\n",
    "    \n",
    "    input_shape = tf.keras.Input(shape=(recurrents,60))\n",
    "    X = stack_superblock(input_shape, threshold=10)\n",
    "    X = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(units=8))(X)\n",
    "    X = tf.keras.layers.Dense(units=8, activation='relu')(X)\n",
    "    outputs = tf.keras.layers.Dense(units=2, activation='softmax')(X)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=input_shape, outputs=outputs)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def model_309_kt(hp):\n",
    "    '''developed from model_307, optimized from model_308: \n",
    "    - Stacking superblock until the number of recurrent\n",
    "        sequence before the final node reach threshold.\n",
    "    - using threshold=2 and fraction=1. Compared to\n",
    "        threshold=10 and fraction=0.75 in model_308.\n",
    "    \n",
    "    - 120 recurrent, 60 filtered features input\n",
    "    - 2 outputs (1 true / 1 false)'''  \n",
    "    def min_max_step(component_num, recurrents, fraction, floor_fraction, num_steps, minimum_value):\n",
    "        recommended_value = int((recurrents // component_num) * fraction)\n",
    "        recommended_value = recommended_value if recommended_value >= minimum_value else minimum_value\n",
    "        floor_value = int(floor_fraction * recommended_value)\n",
    "        floor_value = floor_value if floor_value >= minimum_value else minimum_value\n",
    "        step_value = int((recommended_value - floor_value) // num_steps)\n",
    "        step_value = step_value if step_value >= 1 else 1\n",
    "        return floor_value, recommended_value, step_value\n",
    "    \n",
    "    def block(X, filters, recunits, first_block=False, strides_val=1):\n",
    "        # Conv1\n",
    "        X = tf.keras.layers.Conv1D(filters=filters, kernel_size=1, strides=1 if not first_block else strides_val, padding='causal')(X)\n",
    "        X = tf.keras.layers.BatchNormalization()(X)\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        # Conv2\n",
    "        X = tf.keras.layers.Conv1D(filters=filters, kernel_size=1, strides=1, padding='causal')(X)\n",
    "        X = tf.keras.layers.BatchNormalization()(X)\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        # Conv3\n",
    "        X = tf.keras.layers.Conv1D(filters=filters, kernel_size=1, strides=1, padding='causal')(X)\n",
    "        X = tf.keras.layers.BatchNormalization()(X)\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        # Recurrent unit\n",
    "        X = tf.keras.layers.GRU(units=recunits, return_sequences=True)(X)\n",
    "        X_bn = tf.keras.layers.BatchNormalization()(X)\n",
    "        X_act = tf.keras.layers.Activation('relu')(X_bn)\n",
    "        return X_bn, X_act\n",
    "    \n",
    "    def superblock(super_component_num, input_shape, component_num, final_dunits, recurrents, fraction=1, floor_fraction=0.25, num_steps=4, minimum_value=4):\n",
    "        recommended_value = int((recurrents // component_num) * fraction)\n",
    "        minv, maxv, step = min_max_step(component_num, recurrents, fraction, floor_fraction, num_steps, minimum_value)\n",
    "        superblock_hyp = hp.Int(f'{super_component_num}_comp{component_num}', min_value=minv, max_value=maxv, step=step)\n",
    "        X_bn_1, X_act_1 = block(input_shape, superblock_hyp, superblock_hyp, first_block=True, strides_val=int(component_num))\n",
    "        # Block 2\n",
    "        X_bn_2, X_act_2 = block(X_act_1, superblock_hyp, superblock_hyp)\n",
    "        # Addition block\n",
    "        X = tf.keras.layers.Add()([X_bn_1, X_bn_2])\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        # Block 3\n",
    "        X_bn_3, X_act_3 = block(X, superblock_hyp, superblock_hyp)\n",
    "        # Final layer\n",
    "        X = tf.keras.layers.GRU(units=superblock_hyp, return_sequences=False)(X_act_3)\n",
    "        X = tf.keras.layers.Dense(units=final_dunits)(X)\n",
    "        X = tf.keras.layers.BatchNormalization()(X)\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        return tf.expand_dims(X, axis=1)\n",
    "    \n",
    "    def superblock_component(input_shape, super_component_num):\n",
    "        component_strides = 1\n",
    "        superblock_final_dunits = hp.Int(f'{super_component_num}_superblock_final_dunits', min_value=2, max_value=6, step=2)\n",
    "        \n",
    "        n_components = generate_prime(int(input_shape.shape[1] // component_strides))\n",
    "        component_results = []\n",
    "        for component_num in n_components:\n",
    "            superblock_result = superblock(super_component_num, input_shape, component_num, superblock_final_dunits, input_shape.shape[1])\n",
    "            component_results.append(superblock_result)\n",
    "\n",
    "        # reverse component_result so the highest stride come first\n",
    "        component_results = component_results[::-1]\n",
    "        X = tf.keras.layers.Concatenate(axis=1)(component_results)\n",
    "        return X\n",
    "    \n",
    "    def stack_superblock(input_shape, threshold=2):\n",
    "        # Define initial state\n",
    "        # This initial state would be re-weitten in each\n",
    "        # loop\n",
    "        current_recurrent = input_shape.shape[1]\n",
    "        X = input_shape\n",
    "        count_component = 0\n",
    "        while current_recurrent > threshold:\n",
    "            X = superblock_component(X, count_component)\n",
    "            current_recurrent = X.shape[1]\n",
    "            count_component+=1\n",
    "        return X\n",
    "        \n",
    "    recurrents = 120\n",
    "    lr = hp.Float('lr', min_value=0.000001, max_value=0.0001, sampling='log')\n",
    "    \n",
    "    input_shape = tf.keras.Input(shape=(recurrents,60))\n",
    "    X = stack_superblock(input_shape, threshold=2)\n",
    "    X = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(units=8))(X)\n",
    "    X = tf.keras.layers.Dense(units=8, activation='relu')(X)\n",
    "    outputs = tf.keras.layers.Dense(units=2, activation='softmax')(X)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=input_shape, outputs=outputs)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def model_310_kt(hp):\n",
    "    '''developed from model_307: \n",
    "    - 2-3x more layer units (with hope to increase performance)\n",
    "        fraction 1 -> 3\n",
    "        minimum_value 4 -> 8\n",
    "        superblock_final_dunits 2/6/2 -> 8/24/4\n",
    "        bdRNN 8 -> 32\n",
    "        dense 8 -> 32\n",
    "    \n",
    "    - 120 recurrent, 60 filtered features input\n",
    "    - 2 outputs (1 true / 1 false)'''  \n",
    "    def min_max_step(component_num, recurrents, fraction, floor_fraction, num_steps, minimum_value):\n",
    "        recommended_value = int((recurrents // component_num) * fraction)\n",
    "        recommended_value = recommended_value if recommended_value >= minimum_value else minimum_value\n",
    "        floor_value = int(floor_fraction * recommended_value)\n",
    "        floor_value = floor_value if floor_value >= minimum_value else minimum_value\n",
    "        step_value = int((recommended_value - floor_value) // num_steps)\n",
    "        step_value = step_value if step_value >= 1 else 1\n",
    "        return floor_value, recommended_value, step_value\n",
    "    \n",
    "    def block(X, filters, recunits, first_block=False, strides_val=1):\n",
    "        # Conv1\n",
    "        X = tf.keras.layers.Conv1D(filters=filters, kernel_size=1, strides=1 if not first_block else strides_val, padding='causal')(X)\n",
    "        X = tf.keras.layers.BatchNormalization()(X)\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        # Conv2\n",
    "        X = tf.keras.layers.Conv1D(filters=filters, kernel_size=1, strides=1, padding='causal')(X)\n",
    "        X = tf.keras.layers.BatchNormalization()(X)\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        # Conv3\n",
    "        X = tf.keras.layers.Conv1D(filters=filters, kernel_size=1, strides=1, padding='causal')(X)\n",
    "        X = tf.keras.layers.BatchNormalization()(X)\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        # Recurrent unit\n",
    "        X = tf.keras.layers.GRU(units=recunits, return_sequences=True)(X)\n",
    "        X_bn = tf.keras.layers.BatchNormalization()(X)\n",
    "        X_act = tf.keras.layers.Activation('relu')(X_bn)\n",
    "        return X_bn, X_act\n",
    "    \n",
    "    def superblock(input_shape, component_num, final_dunits, recurrents, fraction=3, floor_fraction=0.25, num_steps=4, minimum_value=8):\n",
    "        recommended_value = int((recurrents // component_num) * fraction)\n",
    "        minv, maxv, step = min_max_step(component_num, recurrents, fraction, floor_fraction, num_steps, minimum_value)\n",
    "        superblock_hyp = hp.Int(f'comp{component_num}', min_value=minv, max_value=maxv, step=step)\n",
    "        X_bn_1, X_act_1 = block(input_shape, superblock_hyp, superblock_hyp, first_block=True, strides_val=int(component_num))\n",
    "        # Block 2\n",
    "        X_bn_2, X_act_2 = block(X_act_1, superblock_hyp, superblock_hyp)\n",
    "        # Addition block\n",
    "        X = tf.keras.layers.Add()([X_bn_1, X_bn_2])\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        # Block 3\n",
    "        X_bn_3, X_act_3 = block(X, superblock_hyp, superblock_hyp)\n",
    "        # Final layer\n",
    "        X = tf.keras.layers.GRU(units=superblock_hyp, return_sequences=False)(X_act_3)\n",
    "        X = tf.keras.layers.Dense(units=final_dunits)(X)\n",
    "        X = tf.keras.layers.BatchNormalization()(X)\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        return tf.expand_dims(X, axis=1)\n",
    "    \n",
    "    recurrents = 120\n",
    "    component_strides = 1\n",
    "    superblock_final_dunits = hp.Int('superblock_final_dunits', min_value=8, max_value=24, step=4)\n",
    "    lr = hp.Float('lr', min_value=0.000001, max_value=0.0001, sampling='log')\n",
    "    input_shape = tf.keras.Input(shape=(recurrents,60))\n",
    "    \n",
    "    n_components = generate_prime(int(recurrents // component_strides))\n",
    "    component_results = []\n",
    "    for component_num in n_components:\n",
    "        superblock_result = superblock(input_shape, component_num, superblock_final_dunits, recurrents)\n",
    "        component_results.append(superblock_result)\n",
    "        \n",
    "    # reverse component_result so the highest stride come first\n",
    "    component_results = component_results[::-1]\n",
    "    \n",
    "    X = tf.keras.layers.Concatenate(axis=1)(component_results)\n",
    "    X = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(units=32))(X)\n",
    "    X = tf.keras.layers.Dense(units=32, activation='relu')(X)\n",
    "    outputs = tf.keras.layers.Dense(units=2, activation='softmax')(X)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=input_shape, outputs=outputs)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def model_311_kt(hp):\n",
    "    '''Recurrent preprocessor model.\n",
    "    - Every features from index 0 -> len(features) - 1\n",
    "        processed individually, and then concatenated with\n",
    "        other proprocessed features.\n",
    "    \n",
    "    - 120 recurrent, 60 filtered features input\n",
    "    - 2 outputs (1 true / 1 false)'''\n",
    "    def recurrent_preprocessor_component(X, feature_no, rnnu_comp):\n",
    "        '''Version 1: using single LSTM layer to preprocess'''\n",
    "        X = tf.keras.layers.LSTM(rnnu_comp, return_sequences=False)(X[:,:,feature_no:feature_no+1])\n",
    "        X = tf.keras.layers.BatchNormalization()(X)\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        return tf.expand_dims(X, axis=1)\n",
    "    def recurrent_preprocessor(input_shape, features, min_value=4, max_value=10, step=2):\n",
    "        rnnu_comp = hp.Int(f'rnnu_comp', min_value=min_value, max_value=max_value, step=step)\n",
    "        component_results = []\n",
    "        for feature_no in range(features):\n",
    "            X = recurrent_preprocessor_component(input_shape, feature_no, rnnu_comp)\n",
    "            component_results.append(X)\n",
    "        X = tf.keras.layers.Concatenate(axis=1)(component_results)\n",
    "        return X\n",
    "    \n",
    "    lr = hp.Float('lr', min_value=0.000001, max_value=0.0001, sampling='log')\n",
    "    final_rnnu = hp.Int('final_rnnu', min_value=8, max_value=48, step=8)\n",
    "    final_denseu = hp.Int('final_denseu', min_value=8, max_value=48, step=8)\n",
    "    \n",
    "    recurrents = 120\n",
    "    features = 60\n",
    "    input_shape = tf.keras.Input(shape=(recurrents, features))\n",
    "    X = recurrent_preprocessor(input_shape, features)\n",
    "    X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=final_rnnu))(X)\n",
    "    X = tf.keras.layers.Dense(units=final_denseu, activation='relu')(X)\n",
    "    outputs = tf.keras.layers.Dense(units=2, activation='softmax')(X)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=input_shape, outputs=outputs)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "    return model \n",
    "\n",
    "def model_312_kt(hp):\n",
    "    '''Recurrent preprocessor model.\n",
    "    - Every features from index 0 -> len(features) - 1\n",
    "        processed individually, and then concatenated with\n",
    "        other proprocessed features.\n",
    "    \n",
    "    Rev from model_311:\n",
    "    - Remove final rnn layers. Early training show shortcoming in acc\n",
    "    \n",
    "    - 120 recurrent, 60 filtered features input\n",
    "    - 2 outputs (1 true / 1 false)'''\n",
    "    def recurrent_preprocessor_component(X, feature_no, rnnu_comp):\n",
    "        '''Version 1: using single LSTM layer to preprocess'''\n",
    "        X = tf.keras.layers.LSTM(rnnu_comp, return_sequences=False)(X[:,:,feature_no:feature_no+1])\n",
    "        X = tf.keras.layers.BatchNormalization()(X)\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        return tf.expand_dims(X, axis=1)\n",
    "    def recurrent_preprocessor(input_shape, features, min_value=4, max_value=10, step=2):\n",
    "        rnnu_comp = hp.Int(f'rnnu_comp', min_value=min_value, max_value=max_value, step=step)\n",
    "        component_results = []\n",
    "        for feature_no in range(features):\n",
    "            X = recurrent_preprocessor_component(input_shape, feature_no, rnnu_comp)\n",
    "            component_results.append(X)\n",
    "        X = tf.keras.layers.Concatenate(axis=1)(component_results)\n",
    "        return X\n",
    "    \n",
    "    lr = hp.Float('lr', min_value=0.000001, max_value=0.0001, sampling='log')\n",
    "    final_denseu = hp.Int('final_denseu', min_value=8, max_value=48, step=8)\n",
    "    \n",
    "    recurrents = 120\n",
    "    features = 60\n",
    "    input_shape = tf.keras.Input(shape=(recurrents, features))\n",
    "    X = recurrent_preprocessor(input_shape, features)\n",
    "    X = tf.keras.layers.Flatten()(X)\n",
    "    X = tf.keras.layers.Dense(units=final_denseu, activation='relu')(X)\n",
    "    outputs = tf.keras.layers.Dense(units=2, activation='softmax')(X)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=input_shape, outputs=outputs)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def model_313_kt(hp):\n",
    "    '''Recurrent preprocessor model.\n",
    "    - Every features from index 0 -> len(features) - 1\n",
    "        processed individually, and then concatenated with\n",
    "        other proprocessed features.\n",
    "    \n",
    "    Rev from model_311:\n",
    "    - Remove final rnn layers. Early training show shortcoming in acc\n",
    "    \n",
    "    Rev from model_312:\n",
    "    - Early training show even worse loss: 0.9.\n",
    "    - Try to stack conv1d 1 kernel\n",
    "    - add model.summary()\n",
    "    \n",
    "    - 120 recurrent, 60 filtered features input\n",
    "    - 2 outputs (1 true / 1 false)'''\n",
    "    def recurrent_preprocessor_component(X, feature_no, rnnu_comp):\n",
    "        '''Version 1: using single LSTM layer to preprocess'''\n",
    "        X = tf.keras.layers.LSTM(rnnu_comp, return_sequences=False)(X[:,:,feature_no:feature_no+1])\n",
    "        X = tf.keras.layers.BatchNormalization()(X)\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        return tf.expand_dims(X, axis=1)\n",
    "    def recurrent_preprocessor(input_shape, features, min_value=4, max_value=10, step=2):\n",
    "        rnnu_comp = hp.Int(f'rnnu_comp', min_value=min_value, max_value=max_value, step=step)\n",
    "        component_results = []\n",
    "        for feature_no in range(features):\n",
    "            X = recurrent_preprocessor_component(input_shape, feature_no, rnnu_comp)\n",
    "            component_results.append(X)\n",
    "        X = tf.keras.layers.Concatenate(axis=1)(component_results)\n",
    "        return X\n",
    "    \n",
    "    lr = hp.Float('lr', min_value=0.000001, max_value=0.0001, sampling='log')\n",
    "    final_denseu = hp.Int('final_denseu', min_value=8, max_value=48, step=8)\n",
    "    final_c1d_1_filters = hp.Int('final_c1d_1_filters', min_value=4, max_value=8, step=2)\n",
    "    final_c1d_2_filters = hp.Int('final_c1d_2_filters', min_value=4, max_value=16, step=4)\n",
    "    final_c1d_2_kernels = hp.Int('final_c1d_2_kernels', min_value=1, max_value=7, step=1)\n",
    "    final_c1d_2_strides = hp.Int('final_c1d_2_strides', min_value=1, max_value=3, step=1)\n",
    "    final_c1d_3_filters = hp.Int('final_c1d_3_filters', min_value=4, max_value=16, step=4)\n",
    "    final_c1d_3_kernels = hp.Int('final_c1d_3_kernels', min_value=1, max_value=7, step=1)\n",
    "    final_c1d_3_strides = hp.Int('final_c1d_3_strides', min_value=1, max_value=3, step=1)\n",
    "    \n",
    "    recurrents = 120\n",
    "    features = 60\n",
    "    input_shape = tf.keras.Input(shape=(recurrents, features))\n",
    "    X = recurrent_preprocessor(input_shape, features)\n",
    "    # Stack multiple conv1d\n",
    "    X = tf.keras.layers.Conv1D(filters=final_c1d_1_filters, kernel_size=1, strides=1)(X)\n",
    "    X = tf.keras.layers.BatchNormalization()(X)\n",
    "    X = tf.keras.layers.Activation('relu')(X)\n",
    "    X = tf.keras.layers.Conv1D(filters=final_c1d_2_filters, kernel_size=final_c1d_2_kernels, strides=final_c1d_2_strides)(X)\n",
    "    X = tf.keras.layers.BatchNormalization()(X)\n",
    "    X = tf.keras.layers.Activation('relu')(X)\n",
    "    X = tf.keras.layers.Conv1D(filters=final_c1d_3_filters, kernel_size=final_c1d_3_kernels, strides=final_c1d_3_strides)(X)\n",
    "    X = tf.keras.layers.BatchNormalization()(X)\n",
    "    X = tf.keras.layers.Activation('relu')(X)\n",
    "    \n",
    "    X = tf.keras.layers.Flatten()(X)\n",
    "    X = tf.keras.layers.Dense(units=final_denseu, activation='relu')(X)\n",
    "    outputs = tf.keras.layers.Dense(units=2, activation='softmax')(X)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=input_shape, outputs=outputs)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def model_314_kt(hp):\n",
    "    '''developed from model_307: \n",
    "    - Stacking superblock component together like block that stacked together.\n",
    "    - Stacking difference with 308 & 309:\n",
    "        - This 314 stack component before concatenation\n",
    "            of superblock\n",
    "        - The final concatenation is the same as 307, but\n",
    "            with added component before that concatenation.\n",
    "        - Desired effect:\n",
    "            - Its possible that concatenation in 307 already\n",
    "                abstract enough that additional abstraction\n",
    "                just made the performance and learning curve\n",
    "                going down.\n",
    "            - Abstraction in prime component may give boost in\n",
    "                performance by outputting more distinguishable\n",
    "                value to later layer without shuffling the\n",
    "                prime components even more.\n",
    "            - This model also verify if stacking conv+rnn\n",
    "                is possible. If not, try to stack more\n",
    "                conv layer only to the component (next model).\n",
    "    \n",
    "    - 120 recurrent, 60 filtered features input\n",
    "    - 2 outputs (1 true / 1 false)'''  \n",
    "    def min_max_step(component_num, recurrents, fraction, floor_fraction, num_steps, minimum_value):\n",
    "        recommended_value = int((recurrents // component_num) * fraction)\n",
    "        recommended_value = recommended_value if recommended_value >= minimum_value else minimum_value\n",
    "        floor_value = int(floor_fraction * recommended_value)\n",
    "        floor_value = floor_value if floor_value >= minimum_value else minimum_value\n",
    "        step_value = int((recommended_value - floor_value) // num_steps)\n",
    "        step_value = step_value if step_value >= 1 else 1\n",
    "        return floor_value, recommended_value, step_value\n",
    "    \n",
    "    def block(X, filters, recunits, first_block=False, strides_val=1):\n",
    "        # Conv1\n",
    "        X = tf.keras.layers.Conv1D(filters=filters, kernel_size=1, strides=1 if not first_block else strides_val, padding='causal')(X)\n",
    "        X = tf.keras.layers.BatchNormalization()(X)\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        # Conv2\n",
    "        X = tf.keras.layers.Conv1D(filters=filters, kernel_size=1, strides=1, padding='causal')(X)\n",
    "        X = tf.keras.layers.BatchNormalization()(X)\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        # Conv3\n",
    "        X = tf.keras.layers.Conv1D(filters=filters, kernel_size=1, strides=1, padding='causal')(X)\n",
    "        X = tf.keras.layers.BatchNormalization()(X)\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        # Recurrent unit\n",
    "        X = tf.keras.layers.GRU(units=recunits, return_sequences=True)(X)\n",
    "        X_bn = tf.keras.layers.BatchNormalization()(X)\n",
    "        X_act = tf.keras.layers.Activation('relu')(X_bn)\n",
    "        return X_bn, X_act\n",
    "    \n",
    "    def superblock_component(X, superblock_hyp, component_num, first_node=True, final_node=False):\n",
    "        X_bn_1, X_act_1 = block(X, superblock_hyp, superblock_hyp, first_block=True, strides_val=int(component_num) if first_node else 1)\n",
    "        # Block 2\n",
    "        X_bn_2, X_act_2 = block(X_act_1, superblock_hyp, superblock_hyp)\n",
    "        # Addition block\n",
    "        X = tf.keras.layers.Add()([X_bn_1, X_bn_2])\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        # Block 3\n",
    "        X_bn_3, X_act_3 = block(X, superblock_hyp, superblock_hyp)\n",
    "        # Final layer\n",
    "        X = tf.keras.layers.GRU(units=superblock_hyp, return_sequences=not final_node)(X_act_3)\n",
    "        X_bn = tf.keras.layers.BatchNormalization()(X)\n",
    "        X_act = tf.keras.layers.Activation('relu')(X_bn)        \n",
    "        return X_bn, X_act\n",
    "    \n",
    "    def superblock(input_shape, component_num, final_dunits, recurrents, fraction=1, floor_fraction=0.25, num_steps=4, minimum_value=4):\n",
    "        recommended_value = int((recurrents // component_num) * fraction)\n",
    "        minv, maxv, step = min_max_step(component_num, recurrents, fraction, floor_fraction, num_steps, minimum_value)\n",
    "        superblock_hyp = hp.Int(f'comp{component_num}', min_value=minv, max_value=maxv, step=step)\n",
    "        \n",
    "        X_bn_1, X_act_1 = superblock_component(input_shape, superblock_hyp, component_num, first_node=True, final_node=False)\n",
    "        X_bn_2, X_act_2 = superblock_component(X_act_1, superblock_hyp, component_num, first_node=False, final_node=False)\n",
    "        # Addition block\n",
    "        X = tf.keras.layers.Add()([X_bn_1, X_bn_2])\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        X_bn_3, X_act_3 = superblock_component(X, superblock_hyp, component_num, first_node=False, final_node=True)\n",
    "        \n",
    "        X = tf.keras.layers.Dense(units=final_dunits)(X_act_3)\n",
    "        X = tf.keras.layers.BatchNormalization()(X)\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        return tf.expand_dims(X, axis=1)\n",
    "    \n",
    "    recurrents = 120\n",
    "    component_strides = 1\n",
    "    superblock_final_dunits = hp.Int('superblock_final_dunits', min_value=2, max_value=6, step=2)\n",
    "    lr = hp.Float('lr', min_value=0.000001, max_value=0.0001, sampling='log')\n",
    "    input_shape = tf.keras.Input(shape=(recurrents,60))\n",
    "    \n",
    "    n_components = generate_prime(int(recurrents // component_strides))\n",
    "    component_results = []\n",
    "    for component_num in n_components:\n",
    "        superblock_result = superblock(input_shape, component_num, superblock_final_dunits, recurrents)\n",
    "        component_results.append(superblock_result)\n",
    "        \n",
    "    # reverse component_result so the highest stride come first\n",
    "    component_results = component_results[::-1]\n",
    "    \n",
    "    X = tf.keras.layers.Concatenate(axis=1)(component_results)\n",
    "    X = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(units=8))(X)\n",
    "    X = tf.keras.layers.Dense(units=8, activation='relu')(X)\n",
    "    outputs = tf.keras.layers.Dense(units=2, activation='softmax')(X)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=input_shape, outputs=outputs)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "    print(f'Total params: {model.count_params()}')\n",
    "    return model\n",
    "\n",
    "def model_315_kt(hp):\n",
    "    '''developed from model_307: \n",
    "    - Difference with 307:\n",
    "        - Add conv stack and additive layer in the superblock\n",
    "        - Why?\n",
    "            - Early kt search with 314 show very long model\n",
    "                compilation although the network not complex enough\n",
    "                in terms of parameters.\n",
    "            - Need faster iteration between model\n",
    "            - even if the model 314 is better compared to 307 & 310,\n",
    "                comparable model with faster training time is needed.\n",
    "            - It's still unknown wheter 314 complexity is enough to\n",
    "                drive lower val_loss. Using this model variation as\n",
    "                comparison can lower iteration time.\n",
    "    \n",
    "    - 120 recurrent, 60 filtered features input\n",
    "    - 2 outputs (1 true / 1 false)'''  \n",
    "    def min_max_step(component_num, recurrents, fraction, floor_fraction, num_steps, minimum_value):\n",
    "        recommended_value = int((recurrents // component_num) * fraction)\n",
    "        recommended_value = recommended_value if recommended_value >= minimum_value else minimum_value\n",
    "        floor_value = int(floor_fraction * recommended_value)\n",
    "        floor_value = floor_value if floor_value >= minimum_value else minimum_value\n",
    "        step_value = int((recommended_value - floor_value) // num_steps)\n",
    "        step_value = step_value if step_value >= 1 else 1\n",
    "        return floor_value, recommended_value, step_value\n",
    "    \n",
    "    def block(X, filters, recunits, first_block=False, strides_val=1):\n",
    "        # Conv1\n",
    "        X = tf.keras.layers.Conv1D(filters=filters, kernel_size=1, strides=1 if not first_block else strides_val, padding='causal')(X)\n",
    "        X = tf.keras.layers.BatchNormalization()(X)\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        # Conv2\n",
    "        X = tf.keras.layers.Conv1D(filters=filters, kernel_size=1, strides=1, padding='causal')(X)\n",
    "        X = tf.keras.layers.BatchNormalization()(X)\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        # Conv3\n",
    "        X = tf.keras.layers.Conv1D(filters=filters, kernel_size=1, strides=1, padding='causal')(X)\n",
    "        X = tf.keras.layers.BatchNormalization()(X)\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        # Recurrent unit\n",
    "        X = tf.keras.layers.GRU(units=recunits, return_sequences=True)(X)\n",
    "        X_bn = tf.keras.layers.BatchNormalization()(X)\n",
    "        X_act = tf.keras.layers.Activation('relu')(X_bn)\n",
    "        return X_bn, X_act\n",
    "    \n",
    "    def superblock(input_shape, component_num, final_dunits, recurrents, fraction=1, floor_fraction=0.25, num_steps=4, minimum_value=4):\n",
    "        recommended_value = int((recurrents // component_num) * fraction)\n",
    "        minv, maxv, step = min_max_step(component_num, recurrents, fraction, floor_fraction, num_steps, minimum_value)\n",
    "        superblock_hyp = hp.Int(f'comp{component_num}', min_value=minv, max_value=maxv, step=step)\n",
    "        \n",
    "        # Block 1\n",
    "        X_bn_1, X_act_1 = block(input_shape, superblock_hyp, superblock_hyp, first_block=True, strides_val=int(component_num))\n",
    "        # Block 2\n",
    "        X_bn_2, X_act_2 = block(X_act_1, superblock_hyp, superblock_hyp)\n",
    "        # Block 1 + 2: Addition block\n",
    "        X = tf.keras.layers.Add()([X_bn_1, X_bn_2])\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        \n",
    "        # Block 3\n",
    "        X_bn_3, X_act_3 = block(X, superblock_hyp, superblock_hyp)\n",
    "        # Block 4\n",
    "        X_bn_4, X_act_4 = block(X_act_3, superblock_hyp, superblock_hyp)\n",
    "        # Block 3 + 4: Addition block\n",
    "        X = tf.keras.layers.Add()([X_bn_3, X_bn_4])\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        \n",
    "        # Block 5\n",
    "        X_bn_5, X_act_5 = block(X, superblock_hyp, superblock_hyp)\n",
    "        \n",
    "        # Final layer\n",
    "        X = tf.keras.layers.GRU(units=superblock_hyp, return_sequences=False)(X_act_3)\n",
    "        X = tf.keras.layers.Dense(units=final_dunits)(X)\n",
    "        X = tf.keras.layers.BatchNormalization()(X)\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        return tf.expand_dims(X, axis=1)\n",
    "    \n",
    "    recurrents = 120\n",
    "    component_strides = 1\n",
    "    superblock_final_dunits = hp.Int('superblock_final_dunits', min_value=2, max_value=6, step=2)\n",
    "    lr = hp.Float('lr', min_value=0.000001, max_value=0.0001, sampling='log')\n",
    "    input_shape = tf.keras.Input(shape=(recurrents,60))\n",
    "    \n",
    "    n_components = generate_prime(int(recurrents // component_strides))\n",
    "    component_results = []\n",
    "    for component_num in n_components:\n",
    "        superblock_result = superblock(input_shape, component_num, superblock_final_dunits, recurrents)\n",
    "        component_results.append(superblock_result)\n",
    "        \n",
    "    # reverse component_result so the highest stride come first\n",
    "    component_results = component_results[::-1]\n",
    "    \n",
    "    X = tf.keras.layers.Concatenate(axis=1)(component_results)\n",
    "    X = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(units=8))(X)\n",
    "    X = tf.keras.layers.Dense(units=8, activation='relu')(X)\n",
    "    outputs = tf.keras.layers.Dense(units=2, activation='softmax')(X)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=input_shape, outputs=outputs)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def model_263_kt_build(hp, train_inputs, train_labels):\n",
    "    '''model_263 kt version: \n",
    "    - 120 recurrent, 60 filtered features input\n",
    "    - 2 outputs (1 true / 1 false)'''\n",
    "    \n",
    "    keys = ['first_conv_filters', 'second_conv_filters', 'second_conv_kernel_size', 'second_conv_strides', 'third_conv_filters', 'third_conv_kernel_size', 'third_conv_strides', 'recurrent_unit', 'dense_unit', 'lr']\n",
    "    values = hp\n",
    "    hyperparameters = {keys[i]:values[i] for i in range(len(keys))}\n",
    "    \n",
    "    first_conv_filters = hyperparameters['first_conv_filters']\n",
    "    second_conv_filters = hyperparameters['second_conv_filters']\n",
    "    second_conv_kernel_size = hyperparameters['second_conv_kernel_size']\n",
    "    second_conv_strides = hyperparameters['second_conv_strides']\n",
    "    third_conv_filters = hyperparameters['third_conv_filters']\n",
    "    third_conv_kernel_size = hyperparameters['third_conv_kernel_size']\n",
    "    third_conv_strides = hyperparameters['third_conv_strides']\n",
    "    recurrent_unit = hyperparameters['recurrent_unit']\n",
    "    dense_unit = hyperparameters['dense_unit']\n",
    "    lr = hyperparameters['lr']\n",
    "    \n",
    "    input_shape = tf.keras.Input(shape=(120,60))\n",
    "    X = tf.keras.layers.Conv1D(filters=first_conv_filters, kernel_size=1, strides=1, padding='causal')(input_shape)\n",
    "    X = tf.keras.layers.Conv1D(filters=second_conv_filters, kernel_size=second_conv_kernel_size, strides=second_conv_strides, padding='causal')(X)\n",
    "    X = tf.keras.layers.Conv1D(filters=third_conv_filters, kernel_size=third_conv_kernel_size, strides=third_conv_strides, padding='causal')(X)\n",
    "    X = tf.keras.layers.GRU(units=recurrent_unit, return_sequences=False)(X)\n",
    "    X = tf.keras.layers.Dense(units=dense_unit, activation='relu')(X)\n",
    "    outputs = tf.keras.layers.Dense(units=2, activation='softmax')(X)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=input_shape, outputs=outputs)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def model_300_kt_build(hp, train_inputs, train_labels):\n",
    "    '''developed from model_263: \n",
    "    - additional C1D and recurrent layer\n",
    "    - 2x more variable search range\n",
    "    - widening lr range\n",
    "    \n",
    "    - 120 recurrent, 60 filtered features input\n",
    "    - 2 outputs (1 true / 1 false)'''\n",
    "    \n",
    "    keys = ['c_filters_1', 'c_filters_2', 'c_kernel_2', 'c_strides_2', 'c_filters_3', 'c_kernel_3', 'c_strides_3', 'c_filters_4', 'c_kernel_4', 'c_strides_4', 'r_unit_1', 'r_unit_2', 'd_unit_1', 'lr']\n",
    "    values = hp\n",
    "    hyperparameters = {keys[i]:values[i] for i in range(len(keys))}\n",
    "    \n",
    "    c_filters_1 = hyperparameters['c_filters_1']\n",
    "    c_filters_2 = hyperparameters['c_filters_2']\n",
    "    c_kernel_2 = hyperparameters['c_kernel_2']\n",
    "    c_strides_2 = hyperparameters['c_strides_2']\n",
    "    c_filters_3 = hyperparameters['c_filters_3']\n",
    "    c_kernel_3 = hyperparameters['c_kernel_3']\n",
    "    c_strides_3 = hyperparameters['c_strides_3']\n",
    "    c_filters_4 = hyperparameters['c_filters_4']\n",
    "    c_kernel_4 = hyperparameters['c_kernel_4']\n",
    "    c_strides_4 = hyperparameters['c_strides_4']\n",
    "    r_unit_1 = hyperparameters['r_unit_1']\n",
    "    r_unit_2 = hyperparameters['r_unit_2']\n",
    "    d_unit_1 = hyperparameters['d_unit_1']\n",
    "    lr = hyperparameters['lr']\n",
    "    \n",
    "    input_shape = tf.keras.Input(shape=(120,60))\n",
    "    X = tf.keras.layers.Conv1D(filters=c_filters_1, kernel_size=1, strides=1, padding='causal')(input_shape)\n",
    "    X = tf.keras.layers.Conv1D(filters=c_filters_2, kernel_size=c_kernel_2, strides=c_strides_2, padding='causal')(X)\n",
    "    X = tf.keras.layers.Conv1D(filters=c_filters_3, kernel_size=c_kernel_3, strides=c_strides_3, padding='causal')(X)\n",
    "    X = tf.keras.layers.Conv1D(filters=c_filters_4, kernel_size=c_kernel_4, strides=c_strides_4, padding='causal')(X)\n",
    "    X = tf.keras.layers.GRU(units=r_unit_1, return_sequences=True)(X)\n",
    "    X = tf.keras.layers.GRU(units=r_unit_2, return_sequences=False)(X)\n",
    "    X = tf.keras.layers.Dense(units=d_unit_1, activation='relu')(X)\n",
    "    outputs = tf.keras.layers.Dense(units=2, activation='softmax')(X)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=input_shape, outputs=outputs)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def model_301_kt_build(hp, train_inputs, train_labels):\n",
    "    '''developed from model_300: \n",
    "    - add batch normalization and relu activation between convolutional layer\n",
    "    \n",
    "    - 120 recurrent, 60 filtered features input\n",
    "    - 2 outputs (1 true / 1 false)'''\n",
    "    \n",
    "    keys = ['c_filters_1', 'c_filters_2', 'c_kernel_2', 'c_strides_2', 'c_filters_3', 'c_kernel_3', 'c_strides_3', 'c_filters_4', 'c_kernel_4', 'c_strides_4', 'r_unit_1', 'r_unit_2', 'd_unit_1', 'lr']\n",
    "    values = hp\n",
    "    hyperparameters = {keys[i]:values[i] for i in range(len(keys))}\n",
    "    \n",
    "    c_filters_1 = hyperparameters['c_filters_1']\n",
    "    c_filters_2 = hyperparameters['c_filters_2']\n",
    "    c_kernel_2 = hyperparameters['c_kernel_2']\n",
    "    c_strides_2 = hyperparameters['c_strides_2']\n",
    "    c_filters_3 = hyperparameters['c_filters_3']\n",
    "    c_kernel_3 = hyperparameters['c_kernel_3']\n",
    "    c_strides_3 = hyperparameters['c_strides_3']\n",
    "    c_filters_4 = hyperparameters['c_filters_4']\n",
    "    c_kernel_4 = hyperparameters['c_kernel_4']\n",
    "    c_strides_4 = hyperparameters['c_strides_4']\n",
    "    r_unit_1 = hyperparameters['r_unit_1']\n",
    "    r_unit_2 = hyperparameters['r_unit_2']\n",
    "    d_unit_1 = hyperparameters['d_unit_1']\n",
    "    lr = hyperparameters['lr']\n",
    "    \n",
    "    input_shape = tf.keras.Input(shape=(120,60))\n",
    "    X = tf.keras.layers.Conv1D(filters=c_filters_1, kernel_size=1, strides=1, padding='causal')(input_shape)\n",
    "    X = tf.keras.layers.BatchNormalization()(X)\n",
    "    X = tf.keras.layers.Activation('relu')(X)\n",
    "    X = tf.keras.layers.Conv1D(filters=c_filters_2, kernel_size=c_kernel_2, strides=c_strides_2, padding='causal')(X)\n",
    "    X = tf.keras.layers.BatchNormalization()(X)\n",
    "    X = tf.keras.layers.Activation('relu')(X)\n",
    "    X = tf.keras.layers.Conv1D(filters=c_filters_3, kernel_size=c_kernel_3, strides=c_strides_3, padding='causal')(X)\n",
    "    X = tf.keras.layers.BatchNormalization()(X)\n",
    "    X = tf.keras.layers.Activation('relu')(X)\n",
    "    X = tf.keras.layers.Conv1D(filters=c_filters_4, kernel_size=c_kernel_4, strides=c_strides_4, padding='causal')(X)\n",
    "    X = tf.keras.layers.BatchNormalization()(X)\n",
    "    X = tf.keras.layers.Activation('relu')(X)\n",
    "    X = tf.keras.layers.GRU(units=r_unit_1, return_sequences=True)(X)\n",
    "    X = tf.keras.layers.GRU(units=r_unit_2, return_sequences=False)(X)\n",
    "    X = tf.keras.layers.Dense(units=d_unit_1, activation='relu')(X)\n",
    "    outputs = tf.keras.layers.Dense(units=2, activation='softmax')(X)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=input_shape, outputs=outputs)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def model_307_kt_build(hp, train_inputs, train_labels):\n",
    "    '''developed from model_306: \n",
    "    - Keep component_strides=1, but maintaining complexity of\n",
    "        each superblock by considering their total number of\n",
    "        constituent.\n",
    "    \n",
    "    - 120 recurrent, 60 filtered features input\n",
    "    - 2 outputs (1 true / 1 false)'''\n",
    "    \n",
    "    keys = ['superblock_final_dunits', 'lr', 'comp2', 'comp3', 'comp5', 'comp7', 'comp11', 'comp13', 'comp17', 'comp19', 'comp23', 'comp29', 'comp31', 'comp37', 'comp41', 'comp43', 'comp47', 'comp53', 'comp59', 'comp61', 'comp67', 'comp71', 'comp73', 'comp79', 'comp83', 'comp89', 'comp97', 'comp101', 'comp103', 'comp107', 'comp109', 'comp113']\n",
    "    values = hp\n",
    "    hyperparameters = {keys[i]:values[i] for i in range(len(keys))}\n",
    "    \n",
    "    def min_max_step(component_num, recurrents, fraction, floor_fraction, num_steps, minimum_value):\n",
    "        recommended_value = int((recurrents // component_num) * fraction)\n",
    "        recommended_value = recommended_value if recommended_value >= minimum_value else minimum_value\n",
    "        floor_value = int(floor_fraction * recommended_value)\n",
    "        floor_value = floor_value if floor_value >= minimum_value else minimum_value\n",
    "        step_value = int((recommended_value - floor_value) // num_steps)\n",
    "        step_value = step_value if step_value >= 1 else 1\n",
    "        return floor_value, recommended_value, step_value\n",
    "    \n",
    "    def block(X, filters, recunits, first_block=False, strides_val=1):\n",
    "        # Conv1\n",
    "        X = tf.keras.layers.Conv1D(filters=filters, kernel_size=1, strides=1 if not first_block else strides_val, padding='causal')(X)\n",
    "        X = tf.keras.layers.BatchNormalization()(X)\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        # Conv2\n",
    "        X = tf.keras.layers.Conv1D(filters=filters, kernel_size=1, strides=1, padding='causal')(X)\n",
    "        X = tf.keras.layers.BatchNormalization()(X)\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        # Conv3\n",
    "        X = tf.keras.layers.Conv1D(filters=filters, kernel_size=1, strides=1, padding='causal')(X)\n",
    "        X = tf.keras.layers.BatchNormalization()(X)\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        # Recurrent unit\n",
    "        X = tf.keras.layers.GRU(units=recunits, return_sequences=True)(X)\n",
    "        X_bn = tf.keras.layers.BatchNormalization()(X)\n",
    "        X_act = tf.keras.layers.Activation('relu')(X_bn)\n",
    "        return X_bn, X_act\n",
    "    \n",
    "    def superblock(input_shape, component_num, final_dunits, recurrents, fraction=1, floor_fraction=0.25, num_steps=4, minimum_value=4):\n",
    "        recommended_value = int((recurrents // component_num) * fraction)\n",
    "        minv, maxv, step = min_max_step(component_num, recurrents, fraction, floor_fraction, num_steps, minimum_value)\n",
    "        superblock_hyp = hyperparameters[f'comp{component_num}']\n",
    "        X_bn_1, X_act_1 = block(input_shape, superblock_hyp, superblock_hyp, first_block=True, strides_val=int(component_num))\n",
    "        # Block 2\n",
    "        X_bn_2, X_act_2 = block(X_act_1, superblock_hyp, superblock_hyp)\n",
    "        # Addition block\n",
    "        X = tf.keras.layers.Add()([X_bn_1, X_bn_2])\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        # Block 3\n",
    "        X_bn_3, X_act_3 = block(X, superblock_hyp, superblock_hyp)\n",
    "        # Final layer\n",
    "        X = tf.keras.layers.GRU(units=superblock_hyp, return_sequences=False)(X_act_3)\n",
    "        X = tf.keras.layers.Dense(units=final_dunits)(X)\n",
    "        X = tf.keras.layers.BatchNormalization()(X)\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        return tf.expand_dims(X, axis=1)\n",
    "    \n",
    "    recurrents = 120\n",
    "    component_strides = 1\n",
    "    superblock_final_dunits = hyperparameters['superblock_final_dunits']\n",
    "    lr = hyperparameters['lr']\n",
    "    input_shape = tf.keras.Input(shape=(train_inputs.shape[1], train_inputs.shape[2]))\n",
    "    \n",
    "    n_components = generate_prime(int(recurrents // component_strides))\n",
    "    component_results = []\n",
    "    for component_num in n_components:\n",
    "        superblock_result = superblock(input_shape, component_num, superblock_final_dunits, recurrents)\n",
    "        component_results.append(superblock_result)\n",
    "        \n",
    "    # reverse component_result so the highest stride come first\n",
    "    component_results = component_results[::-1]\n",
    "    \n",
    "    X = tf.keras.layers.Concatenate(axis=1)(component_results)\n",
    "    X = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(units=8))(X)\n",
    "    X = tf.keras.layers.Dense(units=8, activation='relu')(X)\n",
    "    outputs = tf.keras.layers.Dense(units=train_labels.shape[1], activation='softmax')(X)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=input_shape, outputs=outputs)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def model_308_kt_build(hp, train_inputs, train_labels):\n",
    "    '''developed from model_307: \n",
    "    - Stacking superblock until the number of recurrent\n",
    "        sequence before the final node reach threshold.\n",
    "    \n",
    "    - 120 recurrent, 60 filtered features input\n",
    "    - 2 outputs (1 true / 1 false)'''  \n",
    "    \n",
    "    keys = ['lr', '0_superblock_final_dunits', '0_comp2', '0_comp3', '0_comp5', '0_comp7', '0_comp11', '0_comp13', '0_comp17', '0_comp19', '0_comp23', '0_comp29', '0_comp31', '0_comp37', '0_comp41', '0_comp43', '0_comp47', '0_comp53', '0_comp59', '0_comp61', '0_comp67', '0_comp71', '0_comp73', '0_comp79', '0_comp83', '0_comp89', '0_comp97', '0_comp101', '0_comp103', '0_comp107', '0_comp109', '0_comp113', '1_superblock_final_dunits', '1_comp2', '1_comp3', '1_comp5', '1_comp7', '1_comp11', '1_comp13', '1_comp17', '1_comp19', '1_comp23', '1_comp29']\n",
    "    values = hp\n",
    "    hyperparameters = {keys[i]:values[i] for i in range(len(keys))}\n",
    "    \n",
    "    def min_max_step(component_num, recurrents, fraction, floor_fraction, num_steps, minimum_value):\n",
    "        recommended_value = int((recurrents // component_num) * fraction)\n",
    "        recommended_value = recommended_value if recommended_value >= minimum_value else minimum_value\n",
    "        floor_value = int(floor_fraction * recommended_value)\n",
    "        floor_value = floor_value if floor_value >= minimum_value else minimum_value\n",
    "        step_value = int((recommended_value - floor_value) // num_steps)\n",
    "        step_value = step_value if step_value >= 1 else 1\n",
    "        return floor_value, recommended_value, step_value\n",
    "    \n",
    "    def block(X, filters, recunits, first_block=False, strides_val=1):\n",
    "        # Conv1\n",
    "        X = tf.keras.layers.Conv1D(filters=filters, kernel_size=1, strides=1 if not first_block else strides_val, padding='causal')(X)\n",
    "        X = tf.keras.layers.BatchNormalization()(X)\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        # Conv2\n",
    "        X = tf.keras.layers.Conv1D(filters=filters, kernel_size=1, strides=1, padding='causal')(X)\n",
    "        X = tf.keras.layers.BatchNormalization()(X)\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        # Conv3\n",
    "        X = tf.keras.layers.Conv1D(filters=filters, kernel_size=1, strides=1, padding='causal')(X)\n",
    "        X = tf.keras.layers.BatchNormalization()(X)\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        # Recurrent unit\n",
    "        X = tf.keras.layers.GRU(units=recunits, return_sequences=True)(X)\n",
    "        X_bn = tf.keras.layers.BatchNormalization()(X)\n",
    "        X_act = tf.keras.layers.Activation('relu')(X_bn)\n",
    "        return X_bn, X_act\n",
    "    \n",
    "    def superblock(super_component_num, input_shape, component_num, final_dunits, recurrents, fraction=0.75, floor_fraction=0.25, num_steps=4, minimum_value=4):\n",
    "        recommended_value = int((recurrents // component_num) * fraction)\n",
    "        minv, maxv, step = min_max_step(component_num, recurrents, fraction, floor_fraction, num_steps, minimum_value)\n",
    "        superblock_hyp = hyperparameters[f'{super_component_num}_comp{component_num}']\n",
    "        X_bn_1, X_act_1 = block(input_shape, superblock_hyp, superblock_hyp, first_block=True, strides_val=int(component_num))\n",
    "        # Block 2\n",
    "        X_bn_2, X_act_2 = block(X_act_1, superblock_hyp, superblock_hyp)\n",
    "        # Addition block\n",
    "        X = tf.keras.layers.Add()([X_bn_1, X_bn_2])\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        # Block 3\n",
    "        X_bn_3, X_act_3 = block(X, superblock_hyp, superblock_hyp)\n",
    "        # Final layer\n",
    "        X = tf.keras.layers.GRU(units=superblock_hyp, return_sequences=False)(X_act_3)\n",
    "        X = tf.keras.layers.Dense(units=final_dunits)(X)\n",
    "        X = tf.keras.layers.BatchNormalization()(X)\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        return tf.expand_dims(X, axis=1)\n",
    "    \n",
    "    def superblock_component(input_shape, super_component_num):\n",
    "        component_strides = 1\n",
    "        superblock_final_dunits = hyperparameters[f'{super_component_num}_superblock_final_dunits']\n",
    "        \n",
    "        n_components = generate_prime(int(input_shape.shape[1] // component_strides))\n",
    "        component_results = []\n",
    "        for component_num in n_components:\n",
    "            superblock_result = superblock(super_component_num, input_shape, component_num, superblock_final_dunits, input_shape.shape[1])\n",
    "            component_results.append(superblock_result)\n",
    "\n",
    "        # reverse component_result so the highest stride come first\n",
    "        component_results = component_results[::-1]\n",
    "        X = tf.keras.layers.Concatenate(axis=1)(component_results)\n",
    "        return X\n",
    "    \n",
    "    def stack_superblock(input_shape, threshold=10):\n",
    "        # Define initial state\n",
    "        # This initial state would be re-weitten in each\n",
    "        # loop\n",
    "        current_recurrent = input_shape.shape[1]\n",
    "        X = input_shape\n",
    "        count_component = 0\n",
    "        while current_recurrent > threshold:\n",
    "            X = superblock_component(X, count_component)\n",
    "            current_recurrent = X.shape[1]\n",
    "            count_component+=1\n",
    "        return X\n",
    "        \n",
    "    recurrents = 120\n",
    "    lr = hyperparameters['lr']\n",
    "    \n",
    "    input_shape = tf.keras.Input(shape=(train_inputs.shape[1], train_inputs.shape[2]))\n",
    "    X = stack_superblock(input_shape, threshold=10)\n",
    "    X = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(units=8))(X)\n",
    "    X = tf.keras.layers.Dense(units=8, activation='relu')(X)\n",
    "    outputs = tf.keras.layers.Dense(units=train_labels.shape[1], activation='softmax')(X)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=input_shape, outputs=outputs)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def model_309_kt_build(hp, train_inputs, train_labels):\n",
    "    '''developed from model_307, optimized from model_308: \n",
    "    - Stacking superblock until the number of recurrent\n",
    "        sequence before the final node reach threshold.\n",
    "    - using threshold=2 and fraction=1. Compared to\n",
    "        threshold=10 and fraction=0.75 in model_308.\n",
    "    \n",
    "    - 120 recurrent, 60 filtered features input\n",
    "    - 2 outputs (1 true / 1 false)''' \n",
    "    \n",
    "    keys = ['lr', '0_superblock_final_dunits', '0_comp2', '0_comp3', '0_comp5', '0_comp7', '0_comp11', '0_comp13', '0_comp17', '0_comp19', '0_comp23', '0_comp29', '0_comp31', '0_comp37', '0_comp41', '0_comp43', '0_comp47', '0_comp53', '0_comp59', '0_comp61', '0_comp67', '0_comp71', '0_comp73', '0_comp79', '0_comp83', '0_comp89', '0_comp97', '0_comp101', '0_comp103', '0_comp107', '0_comp109', '0_comp113', '1_superblock_final_dunits', '1_comp2', '1_comp3', '1_comp5', '1_comp7', '1_comp11', '1_comp13', '1_comp17', '1_comp19', '1_comp23', '1_comp29', '2_superblock_final_dunits', '2_comp2', '2_comp3', '2_comp5', '2_comp7', '3_superblock_final_dunits', '3_comp2', '3_comp3']\n",
    "    values = hp\n",
    "    hyperparameters = {keys[i]:values[i] for i in range(len(keys))}\n",
    "    \n",
    "    def min_max_step(component_num, recurrents, fraction, floor_fraction, num_steps, minimum_value):\n",
    "        recommended_value = int((recurrents // component_num) * fraction)\n",
    "        recommended_value = recommended_value if recommended_value >= minimum_value else minimum_value\n",
    "        floor_value = int(floor_fraction * recommended_value)\n",
    "        floor_value = floor_value if floor_value >= minimum_value else minimum_value\n",
    "        step_value = int((recommended_value - floor_value) // num_steps)\n",
    "        step_value = step_value if step_value >= 1 else 1\n",
    "        return floor_value, recommended_value, step_value\n",
    "    \n",
    "    def block(X, filters, recunits, first_block=False, strides_val=1):\n",
    "        # Conv1\n",
    "        X = tf.keras.layers.Conv1D(filters=filters, kernel_size=1, strides=1 if not first_block else strides_val, padding='causal')(X)\n",
    "        X = tf.keras.layers.BatchNormalization()(X)\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        # Conv2\n",
    "        X = tf.keras.layers.Conv1D(filters=filters, kernel_size=1, strides=1, padding='causal')(X)\n",
    "        X = tf.keras.layers.BatchNormalization()(X)\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        # Conv3\n",
    "        X = tf.keras.layers.Conv1D(filters=filters, kernel_size=1, strides=1, padding='causal')(X)\n",
    "        X = tf.keras.layers.BatchNormalization()(X)\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        # Recurrent unit\n",
    "        X = tf.keras.layers.GRU(units=recunits, return_sequences=True)(X)\n",
    "        X_bn = tf.keras.layers.BatchNormalization()(X)\n",
    "        X_act = tf.keras.layers.Activation('relu')(X_bn)\n",
    "        return X_bn, X_act\n",
    "    \n",
    "    def superblock(super_component_num, input_shape, component_num, final_dunits, recurrents, fraction=1, floor_fraction=0.25, num_steps=4, minimum_value=4):\n",
    "        recommended_value = int((recurrents // component_num) * fraction)\n",
    "        minv, maxv, step = min_max_step(component_num, recurrents, fraction, floor_fraction, num_steps, minimum_value)\n",
    "        superblock_hyp = hyperparameters[f'{super_component_num}_comp{component_num}']\n",
    "        X_bn_1, X_act_1 = block(input_shape, superblock_hyp, superblock_hyp, first_block=True, strides_val=int(component_num))\n",
    "        # Block 2\n",
    "        X_bn_2, X_act_2 = block(X_act_1, superblock_hyp, superblock_hyp)\n",
    "        # Addition block\n",
    "        X = tf.keras.layers.Add()([X_bn_1, X_bn_2])\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        # Block 3\n",
    "        X_bn_3, X_act_3 = block(X, superblock_hyp, superblock_hyp)\n",
    "        # Final layer\n",
    "        X = tf.keras.layers.GRU(units=superblock_hyp, return_sequences=False)(X_act_3)\n",
    "        X = tf.keras.layers.Dense(units=final_dunits)(X)\n",
    "        X = tf.keras.layers.BatchNormalization()(X)\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        return tf.expand_dims(X, axis=1)\n",
    "    \n",
    "    def superblock_component(input_shape, super_component_num):\n",
    "        component_strides = 1\n",
    "        superblock_final_dunits = hyperparameters[f'{super_component_num}_superblock_final_dunits']\n",
    "        \n",
    "        n_components = generate_prime(int(input_shape.shape[1] // component_strides))\n",
    "        component_results = []\n",
    "        for component_num in n_components:\n",
    "            superblock_result = superblock(super_component_num, input_shape, component_num, superblock_final_dunits, input_shape.shape[1])\n",
    "            component_results.append(superblock_result)\n",
    "\n",
    "        # reverse component_result so the highest stride come first\n",
    "        component_results = component_results[::-1]\n",
    "        X = tf.keras.layers.Concatenate(axis=1)(component_results)\n",
    "        return X\n",
    "    \n",
    "    def stack_superblock(input_shape, threshold=2):\n",
    "        # Define initial state\n",
    "        # This initial state would be re-weitten in each\n",
    "        # loop\n",
    "        current_recurrent = input_shape.shape[1]\n",
    "        X = input_shape\n",
    "        count_component = 0\n",
    "        while current_recurrent > threshold:\n",
    "            X = superblock_component(X, count_component)\n",
    "            current_recurrent = X.shape[1]\n",
    "            count_component+=1\n",
    "        return X\n",
    "        \n",
    "    recurrents = 120\n",
    "    lr = hyperparameters['lr']\n",
    "    \n",
    "    input_shape = tf.keras.Input(shape=(train_inputs.shape[1], train_inputs.shape[2]))\n",
    "    X = stack_superblock(input_shape, threshold=2)\n",
    "    X = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(units=8))(X)\n",
    "    X = tf.keras.layers.Dense(units=8, activation='relu')(X)\n",
    "    outputs = tf.keras.layers.Dense(units=train_labels.shape[1], activation='softmax')(X)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=input_shape, outputs=outputs)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def model_310_kt_build(hp, train_inputs, train_labels):\n",
    "    '''developed from model_307: \n",
    "    - 2-3x more layer units (with hope to increase performance)\n",
    "        fraction 1 -> 3\n",
    "        minimum_value 4 -> 8\n",
    "        superblock_final_dunits 2/6/2 -> 8/24/4\n",
    "        bdRNN 8 -> 32\n",
    "        dense 8 -> 32\n",
    "    \n",
    "    - 120 recurrent, 60 filtered features input\n",
    "    - 2 outputs (1 true / 1 false)'''\n",
    "    \n",
    "    keys = ['superblock_final_dunits', 'lr', 'comp2', 'comp3', 'comp5', 'comp7', 'comp11', 'comp13', 'comp17', 'comp19', 'comp23', 'comp29', 'comp31', 'comp37', 'comp41', 'comp43', 'comp47', 'comp53', 'comp59', 'comp61', 'comp67', 'comp71', 'comp73', 'comp79', 'comp83', 'comp89', 'comp97', 'comp101', 'comp103', 'comp107', 'comp109', 'comp113']\n",
    "    values = hp\n",
    "    hyperparameters = {keys[i]:values[i] for i in range(len(keys))}\n",
    "    \n",
    "    def min_max_step(component_num, recurrents, fraction, floor_fraction, num_steps, minimum_value):\n",
    "        recommended_value = int((recurrents // component_num) * fraction)\n",
    "        recommended_value = recommended_value if recommended_value >= minimum_value else minimum_value\n",
    "        floor_value = int(floor_fraction * recommended_value)\n",
    "        floor_value = floor_value if floor_value >= minimum_value else minimum_value\n",
    "        step_value = int((recommended_value - floor_value) // num_steps)\n",
    "        step_value = step_value if step_value >= 1 else 1\n",
    "        return floor_value, recommended_value, step_value\n",
    "    \n",
    "    def block(X, filters, recunits, first_block=False, strides_val=1):\n",
    "        # Conv1\n",
    "        X = tf.keras.layers.Conv1D(filters=filters, kernel_size=1, strides=1 if not first_block else strides_val, padding='causal')(X)\n",
    "        X = tf.keras.layers.BatchNormalization()(X)\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        # Conv2\n",
    "        X = tf.keras.layers.Conv1D(filters=filters, kernel_size=1, strides=1, padding='causal')(X)\n",
    "        X = tf.keras.layers.BatchNormalization()(X)\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        # Conv3\n",
    "        X = tf.keras.layers.Conv1D(filters=filters, kernel_size=1, strides=1, padding='causal')(X)\n",
    "        X = tf.keras.layers.BatchNormalization()(X)\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        # Recurrent unit\n",
    "        X = tf.keras.layers.GRU(units=recunits, return_sequences=True)(X)\n",
    "        X_bn = tf.keras.layers.BatchNormalization()(X)\n",
    "        X_act = tf.keras.layers.Activation('relu')(X_bn)\n",
    "        return X_bn, X_act\n",
    "    \n",
    "    def superblock(input_shape, component_num, final_dunits, recurrents, fraction=1, floor_fraction=0.25, num_steps=4, minimum_value=4):\n",
    "        recommended_value = int((recurrents // component_num) * fraction)\n",
    "        minv, maxv, step = min_max_step(component_num, recurrents, fraction, floor_fraction, num_steps, minimum_value)\n",
    "        superblock_hyp = hyperparameters[f'comp{component_num}']\n",
    "        X_bn_1, X_act_1 = block(input_shape, superblock_hyp, superblock_hyp, first_block=True, strides_val=int(component_num))\n",
    "        # Block 2\n",
    "        X_bn_2, X_act_2 = block(X_act_1, superblock_hyp, superblock_hyp)\n",
    "        # Addition block\n",
    "        X = tf.keras.layers.Add()([X_bn_1, X_bn_2])\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        # Block 3\n",
    "        X_bn_3, X_act_3 = block(X, superblock_hyp, superblock_hyp)\n",
    "        # Final layer\n",
    "        X = tf.keras.layers.GRU(units=superblock_hyp, return_sequences=False)(X_act_3)\n",
    "        X = tf.keras.layers.Dense(units=final_dunits)(X)\n",
    "        X = tf.keras.layers.BatchNormalization()(X)\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        return tf.expand_dims(X, axis=1)\n",
    "    \n",
    "    recurrents = 120\n",
    "    component_strides = 1\n",
    "    superblock_final_dunits = hyperparameters['superblock_final_dunits']\n",
    "    lr = hyperparameters['lr']\n",
    "    input_shape = tf.keras.Input(shape=(train_inputs.shape[1], train_inputs.shape[2]))\n",
    "    \n",
    "    n_components = generate_prime(int(recurrents // component_strides))\n",
    "    component_results = []\n",
    "    for component_num in n_components:\n",
    "        superblock_result = superblock(input_shape, component_num, superblock_final_dunits, recurrents)\n",
    "        component_results.append(superblock_result)\n",
    "        \n",
    "    # reverse component_result so the highest stride come first\n",
    "    component_results = component_results[::-1]\n",
    "    \n",
    "    X = tf.keras.layers.Concatenate(axis=1)(component_results)\n",
    "    X = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(units=32))(X)\n",
    "    X = tf.keras.layers.Dense(units=32, activation='relu')(X)\n",
    "    outputs = tf.keras.layers.Dense(units=train_labels.shape[1], activation='softmax')(X)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=input_shape, outputs=outputs)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def model_311_kt_build(hp, train_inputs, train_labels):\n",
    "    '''Recurrent preprocessor model.\n",
    "    - Every features from index 0 -> len(features) - 1\n",
    "        processed individually, and then concatenated with\n",
    "        other proprocessed features.\n",
    "    \n",
    "    - 120 recurrent, 60 filtered features input\n",
    "    - 2 outputs (1 true / 1 false)'''\n",
    "    \n",
    "    keys = ['lr', 'final_rnnu', 'final_denseu', 'rnnu_comp']\n",
    "    values = hp\n",
    "    hyperparameters = {keys[i]:values[i] for i in range(len(keys))}\n",
    "    \n",
    "    def recurrent_preprocessor_component(X, feature_no, rnnu_comp):\n",
    "        '''Version 1: using single LSTM layer to preprocess'''\n",
    "        X = tf.keras.layers.LSTM(rnnu_comp, return_sequences=False)(X[:,:,feature_no:feature_no+1])\n",
    "        X = tf.keras.layers.BatchNormalization()(X)\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        return tf.expand_dims(X, axis=1)\n",
    "    def recurrent_preprocessor(input_shape, features, min_value=4, max_value=10, step=2):\n",
    "        rnnu_comp = hyperparameters['rnnu_comp']\n",
    "        component_results = []\n",
    "        for feature_no in range(features):\n",
    "            X = recurrent_preprocessor_component(input_shape, feature_no, rnnu_comp)\n",
    "            component_results.append(X)\n",
    "        X = tf.keras.layers.Concatenate(axis=1)(component_results)\n",
    "        return X\n",
    "    \n",
    "    lr = hyperparameters['lr']\n",
    "    final_rnnu = hyperparameters['final_rnnu']\n",
    "    final_denseu = hyperparameters['final_denseu']\n",
    "    \n",
    "    recurrents = 120\n",
    "    features = 60\n",
    "    input_shape = tf.keras.Input(shape=(train_inputs.shape[1], train_inputs.shape[2]))\n",
    "    X = recurrent_preprocessor(input_shape, features)\n",
    "    X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=final_rnnu))(X)\n",
    "    X = tf.keras.layers.Dense(units=final_denseu, activation='relu')(X)\n",
    "    outputs = tf.keras.layers.Dense(units=train_labels.shape[1], activation='softmax')(X)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=input_shape, outputs=outputs)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def model_312_kt_build(hp, train_inputs, train_labels):\n",
    "    '''Recurrent preprocessor model.\n",
    "    - Every features from index 0 -> len(features) - 1\n",
    "        processed individually, and then concatenated with\n",
    "        other proprocessed features.\n",
    "    \n",
    "    Rev from model_311:\n",
    "    - Remove final rnn layers. Early training show shortcoming in acc\n",
    "    \n",
    "    - 120 recurrent, 60 filtered features input\n",
    "    - 2 outputs (1 true / 1 false)'''\n",
    "    \n",
    "    keys = ['lr', 'final_denseu', 'rnnu_comp']\n",
    "    values = hp\n",
    "    hyperparameters = {keys[i]:values[i] for i in range(len(keys))}\n",
    "    \n",
    "    def recurrent_preprocessor_component(X, feature_no, rnnu_comp):\n",
    "        '''Version 1: using single LSTM layer to preprocess'''\n",
    "        X = tf.keras.layers.LSTM(rnnu_comp, return_sequences=False)(X[:,:,feature_no:feature_no+1])\n",
    "        X = tf.keras.layers.BatchNormalization()(X)\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        return tf.expand_dims(X, axis=1)\n",
    "    def recurrent_preprocessor(input_shape, features, min_value=4, max_value=10, step=2):\n",
    "        rnnu_comp = hyperparameters['rnnu_comp']\n",
    "        component_results = []\n",
    "        for feature_no in range(features):\n",
    "            X = recurrent_preprocessor_component(input_shape, feature_no, rnnu_comp)\n",
    "            component_results.append(X)\n",
    "        X = tf.keras.layers.Concatenate(axis=1)(component_results)\n",
    "        return X\n",
    "    \n",
    "    lr = hyperparameters['lr']\n",
    "    final_denseu = hyperparameters['final_denseu']\n",
    "    \n",
    "    recurrents = 120\n",
    "    features = 60\n",
    "    input_shape = tf.keras.Input(shape=(train_inputs.shape[1], train_inputs.shape[2]))\n",
    "    X = recurrent_preprocessor(input_shape, features)\n",
    "    X = tf.keras.layers.Flatten()(X)\n",
    "    X = tf.keras.layers.Dense(units=final_denseu, activation='relu')(X)\n",
    "    outputs = tf.keras.layers.Dense(units=train_labels.shape[1], activation='softmax')(X)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=input_shape, outputs=outputs)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def model_313_kt_build(hp, train_inputs, train_labels):\n",
    "    '''Recurrent preprocessor model.\n",
    "    - Every features from index 0 -> len(features) - 1\n",
    "        processed individually, and then concatenated with\n",
    "        other proprocessed features.\n",
    "    \n",
    "    Rev from model_311:\n",
    "    - Remove final rnn layers. Early training show shortcoming in acc\n",
    "    \n",
    "    Rev from model_312:\n",
    "    - Early training show even worse loss: 0.9.\n",
    "    - Try to stack conv1d 1 kernel\n",
    "    - add model.summary()\n",
    "    \n",
    "    - 120 recurrent, 60 filtered features input\n",
    "    - 2 outputs (1 true / 1 false)'''\n",
    "    \n",
    "    keys = ['lr', 'final_denseu', 'final_c1d_1_filters', 'final_c1d_2_filters', 'final_c1d_2_kernels', 'final_c1d_2_strides', 'final_c1d_3_filters', 'final_c1d_3_kernels', 'final_c1d_3_strides', 'rnnu_comp']\n",
    "    values = hp\n",
    "    hyperparameters = {keys[i]:values[i] for i in range(len(keys))}\n",
    "    \n",
    "    def recurrent_preprocessor_component(X, feature_no, rnnu_comp):\n",
    "        '''Version 1: using single LSTM layer to preprocess'''\n",
    "        X = tf.keras.layers.LSTM(rnnu_comp, return_sequences=False)(X[:,:,feature_no:feature_no+1])\n",
    "        X = tf.keras.layers.BatchNormalization()(X)\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        return tf.expand_dims(X, axis=1)\n",
    "    def recurrent_preprocessor(input_shape, features, min_value=4, max_value=10, step=2):\n",
    "        rnnu_comp = hyperparameters['rnnu_comp']\n",
    "        component_results = []\n",
    "        for feature_no in range(features):\n",
    "            X = recurrent_preprocessor_component(input_shape, feature_no, rnnu_comp)\n",
    "            component_results.append(X)\n",
    "        X = tf.keras.layers.Concatenate(axis=1)(component_results)\n",
    "        return X\n",
    "    \n",
    "    lr = hyperparameters['lr']\n",
    "    final_denseu = hyperparameters['final_denseu']\n",
    "    final_c1d_1_filters = hyperparameters['final_c1d_1_filters']\n",
    "    final_c1d_2_filters = hyperparameters['final_c1d_2_filters']\n",
    "    final_c1d_2_kernels = hyperparameters['final_c1d_2_kernels']\n",
    "    final_c1d_2_strides = hyperparameters['final_c1d_2_strides']\n",
    "    final_c1d_3_filters = hyperparameters['final_c1d_3_filters']\n",
    "    final_c1d_3_kernels = hyperparameters['final_c1d_3_kernels']\n",
    "    final_c1d_3_strides = hyperparameters['final_c1d_3_strides']\n",
    "    \n",
    "    recurrents = 120\n",
    "    features = 60\n",
    "    input_shape = tf.keras.Input(shape=(train_inputs.shape[1], train_inputs.shape[2]))\n",
    "    X = recurrent_preprocessor(input_shape, features)\n",
    "    # Stack multiple conv1d\n",
    "    X = tf.keras.layers.Conv1D(filters=final_c1d_1_filters, kernel_size=1, strides=1)(X)\n",
    "    X = tf.keras.layers.BatchNormalization()(X)\n",
    "    X = tf.keras.layers.Activation('relu')(X)\n",
    "    X = tf.keras.layers.Conv1D(filters=final_c1d_2_filters, kernel_size=final_c1d_2_kernels, strides=final_c1d_2_strides)(X)\n",
    "    X = tf.keras.layers.BatchNormalization()(X)\n",
    "    X = tf.keras.layers.Activation('relu')(X)\n",
    "    X = tf.keras.layers.Conv1D(filters=final_c1d_3_filters, kernel_size=final_c1d_3_kernels, strides=final_c1d_3_strides)(X)\n",
    "    X = tf.keras.layers.BatchNormalization()(X)\n",
    "    X = tf.keras.layers.Activation('relu')(X)\n",
    "    \n",
    "    X = tf.keras.layers.Flatten()(X)\n",
    "    X = tf.keras.layers.Dense(units=final_denseu, activation='relu')(X)\n",
    "    outputs = tf.keras.layers.Dense(units=train_labels.shape[1], activation='softmax')(X)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=input_shape, outputs=outputs)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def model_314_kt_build(hp, train_inputs, train_labels):\n",
    "    '''developed from model_307: \n",
    "    - Stacking superblock component together like block that stacked together.\n",
    "    - Stacking difference with 308 & 309:\n",
    "        - This 314 stack component before concatenation\n",
    "            of superblock\n",
    "        - The final concatenation is the same as 307, but\n",
    "            with added component before that concatenation.\n",
    "        - Desired effect:\n",
    "            - Its possible that concatenation in 307 already\n",
    "                abstract enough that additional abstraction\n",
    "                just made the performance and learning curve\n",
    "                going down.\n",
    "            - Abstraction in prime component may give boost in\n",
    "                performance by outputting more distinguishable\n",
    "                value to later layer without shuffling the\n",
    "                prime components even more.\n",
    "            - This model also verify if stacking conv+rnn\n",
    "                is possible. If not, try to stack more\n",
    "                conv layer only to the component (next model).\n",
    "    \n",
    "    - 120 recurrent, 60 filtered features input\n",
    "    - 2 outputs (1 true / 1 false)'''  \n",
    "    \n",
    "    keys = ['superblock_final_dunits', 'lr', 'comp2', 'comp3', 'comp5', 'comp7', 'comp11', 'comp13', 'comp17', 'comp19', 'comp23', 'comp29', 'comp31', 'comp37', 'comp41', 'comp43', 'comp47', 'comp53', 'comp59', 'comp61', 'comp67', 'comp71', 'comp73', 'comp79', 'comp83', 'comp89', 'comp97', 'comp101', 'comp103', 'comp107', 'comp109', 'comp113']\n",
    "    values = hp\n",
    "    hyperparameters = {keys[i]:values[i] for i in range(len(keys))}\n",
    "    \n",
    "    def min_max_step(component_num, recurrents, fraction, floor_fraction, num_steps, minimum_value):\n",
    "        recommended_value = int((recurrents // component_num) * fraction)\n",
    "        recommended_value = recommended_value if recommended_value >= minimum_value else minimum_value\n",
    "        floor_value = int(floor_fraction * recommended_value)\n",
    "        floor_value = floor_value if floor_value >= minimum_value else minimum_value\n",
    "        step_value = int((recommended_value - floor_value) // num_steps)\n",
    "        step_value = step_value if step_value >= 1 else 1\n",
    "        return floor_value, recommended_value, step_value\n",
    "    \n",
    "    def block(X, filters, recunits, first_block=False, strides_val=1):\n",
    "        # Conv1\n",
    "        X = tf.keras.layers.Conv1D(filters=filters, kernel_size=1, strides=1 if not first_block else strides_val, padding='causal')(X)\n",
    "        X = tf.keras.layers.BatchNormalization()(X)\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        # Conv2\n",
    "        X = tf.keras.layers.Conv1D(filters=filters, kernel_size=1, strides=1, padding='causal')(X)\n",
    "        X = tf.keras.layers.BatchNormalization()(X)\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        # Conv3\n",
    "        X = tf.keras.layers.Conv1D(filters=filters, kernel_size=1, strides=1, padding='causal')(X)\n",
    "        X = tf.keras.layers.BatchNormalization()(X)\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        # Recurrent unit\n",
    "        X = tf.keras.layers.GRU(units=recunits, return_sequences=True)(X)\n",
    "        X_bn = tf.keras.layers.BatchNormalization()(X)\n",
    "        X_act = tf.keras.layers.Activation('relu')(X_bn)\n",
    "        return X_bn, X_act\n",
    "    \n",
    "    def superblock_component(X, superblock_hyp, component_num, first_node=True, final_node=False):\n",
    "        X_bn_1, X_act_1 = block(X, superblock_hyp, superblock_hyp, first_block=True, strides_val=int(component_num) if first_node else 1)\n",
    "        # Block 2\n",
    "        X_bn_2, X_act_2 = block(X_act_1, superblock_hyp, superblock_hyp)\n",
    "        # Addition block\n",
    "        X = tf.keras.layers.Add()([X_bn_1, X_bn_2])\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        # Block 3\n",
    "        X_bn_3, X_act_3 = block(X, superblock_hyp, superblock_hyp)\n",
    "        # Final layer\n",
    "        X = tf.keras.layers.GRU(units=superblock_hyp, return_sequences=not final_node)(X_act_3)\n",
    "        X_bn = tf.keras.layers.BatchNormalization()(X)\n",
    "        X_act = tf.keras.layers.Activation('relu')(X_bn)        \n",
    "        return X_bn, X_act\n",
    "    \n",
    "    def superblock(input_shape, component_num, final_dunits, recurrents, fraction=1, floor_fraction=0.25, num_steps=4, minimum_value=4):\n",
    "        recommended_value = int((recurrents // component_num) * fraction)\n",
    "        minv, maxv, step = min_max_step(component_num, recurrents, fraction, floor_fraction, num_steps, minimum_value)\n",
    "        superblock_hyp = hyperparameters[f'comp{component_num}']\n",
    "        \n",
    "        X_bn_1, X_act_1 = superblock_component(input_shape, superblock_hyp, component_num, first_node=True, final_node=False)\n",
    "        X_bn_2, X_act_2 = superblock_component(X_act_1, superblock_hyp, component_num, first_node=False, final_node=False)\n",
    "        # Addition block\n",
    "        X = tf.keras.layers.Add()([X_bn_1, X_bn_2])\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        X_bn_3, X_act_3 = superblock_component(X, superblock_hyp, component_num, first_node=False, final_node=True)\n",
    "        \n",
    "        X = tf.keras.layers.Dense(units=final_dunits)(X_act_3)\n",
    "        X = tf.keras.layers.BatchNormalization()(X)\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        return tf.expand_dims(X, axis=1)\n",
    "    \n",
    "    recurrents = 120\n",
    "    component_strides = 1\n",
    "    superblock_final_dunits = hyperparameters['superblock_final_dunits']\n",
    "    lr = hyperparameters['lr']\n",
    "    input_shape = tf.keras.Input(shape=(train_inputs.shape[1], train_inputs.shape[2]))\n",
    "    \n",
    "    n_components = generate_prime(int(recurrents // component_strides))\n",
    "    component_results = []\n",
    "    for component_num in n_components:\n",
    "        superblock_result = superblock(input_shape, component_num, superblock_final_dunits, recurrents)\n",
    "        component_results.append(superblock_result)\n",
    "        \n",
    "    # reverse component_result so the highest stride come first\n",
    "    component_results = component_results[::-1]\n",
    "    \n",
    "    X = tf.keras.layers.Concatenate(axis=1)(component_results)\n",
    "    X = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(units=8))(X)\n",
    "    X = tf.keras.layers.Dense(units=8, activation='relu')(X)\n",
    "    outputs = tf.keras.layers.Dense(units=train_labels.shape[1], activation='softmax')(X)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=input_shape, outputs=outputs)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "    print(f'Total params: {model.count_params()}')\n",
    "    return model\n",
    "\n",
    "def model_315_kt_build(hp, train_inputs, train_labels):\n",
    "    '''developed from model_307: \n",
    "    - Difference with 307:\n",
    "        - Add conv stack and additive layer in the superblock\n",
    "        - Why?\n",
    "            - Early kt search with 314 show very long model\n",
    "                compilation although the network not complex enough\n",
    "                in terms of parameters.\n",
    "            - Need faster iteration between model\n",
    "            - even if the model 314 is better compared to 307 & 310,\n",
    "                comparable model with faster training time is needed.\n",
    "            - It's still unknown wheter 314 complexity is enough to\n",
    "                drive lower val_loss. Using this model variation as\n",
    "                comparison can lower iteration time.\n",
    "    \n",
    "    - 120 recurrent, 60 filtered features input\n",
    "    - 2 outputs (1 true / 1 false)'''  \n",
    "    \n",
    "    keys = ['superblock_final_dunits', 'lr', 'comp2', 'comp3', 'comp5', 'comp7', 'comp11', 'comp13', 'comp17', 'comp19', 'comp23', 'comp29', 'comp31', 'comp37', 'comp41', 'comp43', 'comp47', 'comp53', 'comp59', 'comp61', 'comp67', 'comp71', 'comp73', 'comp79', 'comp83', 'comp89', 'comp97', 'comp101', 'comp103', 'comp107', 'comp109', 'comp113']\n",
    "    values = hp\n",
    "    hyperparameters = {keys[i]:values[i] for i in range(len(keys))}\n",
    "    \n",
    "    def min_max_step(component_num, recurrents, fraction, floor_fraction, num_steps, minimum_value):\n",
    "        recommended_value = int((recurrents // component_num) * fraction)\n",
    "        recommended_value = recommended_value if recommended_value >= minimum_value else minimum_value\n",
    "        floor_value = int(floor_fraction * recommended_value)\n",
    "        floor_value = floor_value if floor_value >= minimum_value else minimum_value\n",
    "        step_value = int((recommended_value - floor_value) // num_steps)\n",
    "        step_value = step_value if step_value >= 1 else 1\n",
    "        return floor_value, recommended_value, step_value\n",
    "    \n",
    "    def block(X, filters, recunits, first_block=False, strides_val=1):\n",
    "        # Conv1\n",
    "        X = tf.keras.layers.Conv1D(filters=filters, kernel_size=1, strides=1 if not first_block else strides_val, padding='causal')(X)\n",
    "        X = tf.keras.layers.BatchNormalization()(X)\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        # Conv2\n",
    "        X = tf.keras.layers.Conv1D(filters=filters, kernel_size=1, strides=1, padding='causal')(X)\n",
    "        X = tf.keras.layers.BatchNormalization()(X)\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        # Conv3\n",
    "        X = tf.keras.layers.Conv1D(filters=filters, kernel_size=1, strides=1, padding='causal')(X)\n",
    "        X = tf.keras.layers.BatchNormalization()(X)\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        # Recurrent unit\n",
    "        X = tf.keras.layers.GRU(units=recunits, return_sequences=True)(X)\n",
    "        X_bn = tf.keras.layers.BatchNormalization()(X)\n",
    "        X_act = tf.keras.layers.Activation('relu')(X_bn)\n",
    "        return X_bn, X_act\n",
    "    \n",
    "    def superblock(input_shape, component_num, final_dunits, recurrents, fraction=1, floor_fraction=0.25, num_steps=4, minimum_value=4):\n",
    "        recommended_value = int((recurrents // component_num) * fraction)\n",
    "        minv, maxv, step = min_max_step(component_num, recurrents, fraction, floor_fraction, num_steps, minimum_value)\n",
    "        superblock_hyp = hyperparameters[f'comp{component_num}']\n",
    "        \n",
    "        # Block 1\n",
    "        X_bn_1, X_act_1 = block(input_shape, superblock_hyp, superblock_hyp, first_block=True, strides_val=int(component_num))\n",
    "        # Block 2\n",
    "        X_bn_2, X_act_2 = block(X_act_1, superblock_hyp, superblock_hyp)\n",
    "        # Block 1 + 2: Addition block\n",
    "        X = tf.keras.layers.Add()([X_bn_1, X_bn_2])\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        \n",
    "        # Block 3\n",
    "        X_bn_3, X_act_3 = block(X, superblock_hyp, superblock_hyp)\n",
    "        # Block 4\n",
    "        X_bn_4, X_act_4 = block(X_act_3, superblock_hyp, superblock_hyp)\n",
    "        # Block 3 + 4: Addition block\n",
    "        X = tf.keras.layers.Add()([X_bn_3, X_bn_4])\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        \n",
    "        # Block 5\n",
    "        X_bn_5, X_act_5 = block(X, superblock_hyp, superblock_hyp)\n",
    "        \n",
    "        # Final layer\n",
    "        X = tf.keras.layers.GRU(units=superblock_hyp, return_sequences=False)(X_act_3)\n",
    "        X = tf.keras.layers.Dense(units=final_dunits)(X)\n",
    "        X = tf.keras.layers.BatchNormalization()(X)\n",
    "        X = tf.keras.layers.Activation('relu')(X)\n",
    "        return tf.expand_dims(X, axis=1)\n",
    "    \n",
    "    recurrents = 120\n",
    "    component_strides = 1\n",
    "    superblock_final_dunits = hyperparameters['superblock_final_dunits']\n",
    "    lr = hyperparameters['lr']\n",
    "    input_shape = tf.keras.Input(shape=(train_inputs.shape[1], train_inputs.shape[2]))\n",
    "    \n",
    "    n_components = generate_prime(int(recurrents // component_strides))\n",
    "    component_results = []\n",
    "    for component_num in n_components:\n",
    "        superblock_result = superblock(input_shape, component_num, superblock_final_dunits, recurrents)\n",
    "        component_results.append(superblock_result)\n",
    "        \n",
    "    # reverse component_result so the highest stride come first\n",
    "    component_results = component_results[::-1]\n",
    "    \n",
    "    X = tf.keras.layers.Concatenate(axis=1)(component_results)\n",
    "    X = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(units=8))(X)\n",
    "    X = tf.keras.layers.Dense(units=8, activation='relu')(X)\n",
    "    outputs = tf.keras.layers.Dense(units=train_labels.shape[1], activation='softmax')(X)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=input_shape, outputs=outputs)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
